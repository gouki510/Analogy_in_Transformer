{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gDzOY0uAnMcs"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RueoPKArbvB7"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4zH0LkjDnMct"
   },
   "outputs": [],
   "source": [
    "def build_dicts(entities):\n",
    "    entity2ind = dict()\n",
    "    ind2entity = []\n",
    "    for i in range(len(entities)):\n",
    "        entity = entities[i]\n",
    "        if not (entity in ind2entity):\n",
    "            ind2entity.append(entity)\n",
    "            entity2ind[entity] = len(ind2entity) - 1\n",
    "    return ind2entity, entity2ind\n",
    "\n",
    "def choose(arr, ratio_or_count):\n",
    "    if type(ratio_or_count) == float:\n",
    "        num = round(ratio_or_count*len(arr))\n",
    "    elif type(ratio_or_count) == int:\n",
    "        num = ratio_or_count\n",
    "    else:\n",
    "         assert False\n",
    "    if num >= len(arr):\n",
    "        return arr\n",
    "    rand_inds = np.random.choice(len(arr), num, replace=False).tolist()\n",
    "    return [arr[i] for i in rand_inds]\n",
    "\n",
    "def split(arr, ratio_or_count):\n",
    "    if type(ratio_or_count) == float:\n",
    "        num = round(ratio_or_count*len(arr))\n",
    "    elif type(ratio_or_count) == int:\n",
    "        num = ratio_or_count\n",
    "    else:\n",
    "         assert False\n",
    "    train, test = [], []\n",
    "    rand_inds = np.random.choice(len(arr), num, replace=False).tolist()\n",
    "    for i in tqdm(range(len(arr))):\n",
    "        if i in rand_inds:\n",
    "            test.append(arr[i])\n",
    "        else:\n",
    "            train.append(arr[i])\n",
    "    return [train, test]\n",
    "\n",
    "def form_items(c, t):\n",
    "    input_text = \"\".join(c)\n",
    "    target_text = input_text + \"\".join([t])\n",
    "    item = {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "    return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TSAgHxiAyrZ"
   },
   "source": [
    "## Compositional Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "31ec093ff2f84de7aa996349962a941b",
      "6a175f3c87954291855e768a759d5488",
      "41edb705cfee4fc7b504dbe765505311",
      "7518919438f74527afc6e8b9ed5504f8",
      "00218674721c4f2796b8a00964c5786c",
      "ed6f9e5104ea46b7b5a566d3cc830d29",
      "fbd33915d45f4eb38f07462c5c17eb18",
      "5afe77f841ca44afaefdac67da0d9f3a",
      "323afa4c7a6a4aeeaffe8384ece47370",
      "c2825c6af3484a8fbdc5abdc24ca4939",
      "1e41a0d760264eec9ad28568d70905eb",
      "c471edc43a9d42b2af43792c67a56bb7",
      "f8cce61e763e4c74944ce19748e1ff15",
      "f203832014364752ae089f58c2711249",
      "dfca1c2552e447619274a3b4f02882d4",
      "1fe50f0d192c41a78cae6235b1481028",
      "4c8710a736864be8821318d1b844a5e0",
      "f6c603ac98e240adbb92a37dff9cb38c",
      "c53dfaab98334c0a9f2022ca2fd3bf16",
      "487544ee16c6427a81be73f50690f9a1",
      "d0b286ab73274c57ba2096a31e778d49",
      "ab234546ea2a46a0b352d11c353d8fd7",
      "6d42fdb9ffd2406484e39223b9cc6a7e",
      "7c48de5f28cf41b383a57213158122a2",
      "605cf6073e304b36bb0c4ea1df747fd3",
      "84271db35276416097f9f7ed70b0d758",
      "56a7c956b3c8436a83f0119df5dfdd0e",
      "5c4b337d19d348d2998be7c68ece723e",
      "64385b72913d4cefb67ced4296b6a948",
      "6095ae7d99ef42a3985b28fe57edac77",
      "24d18c2452aa4fb5bd08d8f15209e559",
      "b08aa8a305dd4c5d9536302945b8f0a2",
      "f59782f065d442158d1925495ae32b0d"
     ]
    },
    "id": "ef7b8nwxnMct",
    "outputId": "ef8afb42-6f60-4d0b-f53b-e8bd8c1fd8d6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ec093ff2f84de7aa996349962a941b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c471edc43a9d42b2af43792c67a56bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d42fdb9ffd2406484e39223b9cc6a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_dataset(num_entities, num_relations, out_degree=20, atomic_ood_ratio=0.05, inferred_ood_ratio=0.005):\n",
    "\n",
    "    entities = [\"<e_{}>\".format(i) for i in range(num_entities)]\n",
    "    ind2entity, entity2ind = build_dicts(entities)\n",
    "\n",
    "    relations = [\"<r_{}>\".format(i) for i in range(num_relations)]\n",
    "    ind2relation, relation2ind = build_dicts(relations)\n",
    "\n",
    "    atomic_dict = dict()   # maps a head entity to a list of (r, t) pairs\n",
    "    atomic_facts = []\n",
    "    atomics = []\n",
    "\n",
    "    for i in tqdm(range(num_entities)):\n",
    "        # for each subject entity, randomly select some outgoing relations to some random object entity\n",
    "        num_rows = out_degree\n",
    "        selected_rows = np.random.choice(num_relations, size=num_rows, replace=False).tolist()\n",
    "        for row_idx in selected_rows:\n",
    "            col_idx = np.random.randint(num_entities)  # pick some random tail entity for each selected (h,r)\n",
    "            h,r,t = ind2entity[i], ind2relation[row_idx], ind2entity[col_idx]\n",
    "            atomic_facts.append(form_items([h, r], t))\n",
    "            atomics.append((h,r,t))\n",
    "            if h not in atomic_dict:\n",
    "                atomic_dict[h] = []\n",
    "            atomic_dict[h].append((r, t))\n",
    "\n",
    "    # split ID/OOD\n",
    "    OOD_facts, ID_facts = split(atomics, round(len(atomics)*atomic_ood_ratio))\n",
    "    OOD_facts, ID_facts = set(OOD_facts), set(ID_facts)\n",
    "\n",
    "    id_atomic_facts = [form_items([h, r], t) for (h,r,t) in ID_facts]\n",
    "    ood_atomic_facts = [form_items([h, r], t) for (h,r,t) in OOD_facts]\n",
    "\n",
    "    iid_inferred_facts, near_ood_inferred_facts, far_ood_inferred_facts = [], [], []\n",
    "    for ent in tqdm(entities):\n",
    "        for (r1, b) in atomic_dict[ent]:\n",
    "            for (r2, t) in atomic_dict[b]:\n",
    "                if (ent, r1, b) in OOD_facts or (b, r2, t) in OOD_facts:\n",
    "                    # if (ent, r1, b) in OOD_facts and (b, r2, t) in OOD_facts:\n",
    "                    far_ood_inferred_facts.append(form_items([ent, r1, r2], t))\n",
    "                    continue\n",
    "                if np.random.uniform() > inferred_ood_ratio:\n",
    "                    iid_inferred_facts.append(form_items([ent, r1, r2], t))\n",
    "                else:\n",
    "                    near_ood_inferred_facts.append(form_items([ent, r1, r2], t))\n",
    "\n",
    "    return entities, relations, id_atomic_facts, ood_atomic_facts, iid_inferred_facts, near_ood_inferred_facts, far_ood_inferred_facts\n",
    "\n",
    "NUM_ENTITY_IN = 2000\n",
    "NUM_RELATION = 200\n",
    "\n",
    "train_entities, train_relations, id_atomic_facts, ood_atomic_facts, iid_inferred_facts, near_ood_inferred_facts, far_ood_inferred_facts = build_dataset(NUM_ENTITY_IN, NUM_RELATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHdjSq2eA1KH"
   },
   "source": [
    "## Functor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09z2Bqx5ApTx",
    "outputId": "4696f6df-7494-4dbb-8afa-8e309613d95b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#entities: 10\n",
      "#relations: 10001\n",
      "ID atomics: 40\n",
      "OOD atomics: 0\n",
      "ID compositional: 106\n",
      "near OOD compositional: 14\n",
      "far OOD compositional: 0\n",
      "ID analogical: 3\n",
      "OOD analogical: 2\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# helpers\n",
    "# =========================\n",
    "def build_dicts(items):\n",
    "    ind2 = {i: it for i, it in enumerate(items)}\n",
    "    toind = {it: i for i, it in ind2.items()}\n",
    "    return ind2, toind\n",
    "\n",
    "def split(items, n_ood, rng=None):\n",
    "    \"\"\"\n",
    "    items Split into OOD/ID and return (list -> (ood_list, id_list)).\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    items_list = list(items)\n",
    "    rng.shuffle(items_list)\n",
    "    n_ood = max(0, min(n_ood, len(items_list)))\n",
    "    ood = items_list[:n_ood]\n",
    "    ide = items_list[n_ood:]\n",
    "    return ood, ide\n",
    "\n",
    "# =========================\n",
    "# main builder\n",
    "# =========================\n",
    "def build_dataset_with_functor(\n",
    "    num_entities,\n",
    "    num_relations,\n",
    "    sub_size,                       # |E1| = |E2|\n",
    "    atomic_ood_ratio=0.05,          # OOD ratio for atomic facts (h,r,t) (across E1/E2/OTHER)\n",
    "    compositional_ood_ratio=0.005,       # 2-hop Near-OOD ratio for reasoning\n",
    "    analogical_ood_ratio=0.10,        # [e,<f>]→f(e)  OOD ratio\n",
    "    seed=None,\n",
    "    include_f_inverse=False,         # Inverse functor <f_inv>\n",
    "    duplicate_relation=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      entities, relations,\n",
    "      id_atomic_facts, ood_atomic_facts,\n",
    "      iid_inferred_facts, near_ood_inferred_facts, far_ood_inferred_facts,\n",
    "      E1, E2, f_id_atomic_facts, f_ood_atomic_facts,\n",
    "      f_map_dict\n",
    "    \"\"\"\n",
    "    # ====== Notation ======= #\n",
    "    # atomic_fact: (h, r, t) 1-hop knowledge. h and t are from the same category.\n",
    "    # compositional_fact: (h, r1, r2, t) 2-hop knowledge. h and t are from the same category.\n",
    "    # analogical_fact: (h, f, t) 1-hop knowledge. h and t are from the different categories.\n",
    "    # ======================= #\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # ----------------------------\n",
    "    # vocabulary\n",
    "    # ----------------------------\n",
    "    entities = [f\"<e_{i}>\" for i in range(num_entities)]\n",
    "    ind2entity, entity2ind = build_dicts(entities)\n",
    "\n",
    "    base_relations = [f\"<r_{i}>\" for i in range(num_relations)]\n",
    "    extra_relations = [\"<f>\"] + ([\"<f_inv>\"] if include_f_inverse else [])\n",
    "    relations = base_relations + extra_relations\n",
    "    ind2relation, relation2ind = build_dicts(relations)\n",
    "    F = \"<f>\"\n",
    "    F_INV = \"<f_inv>\" if include_f_inverse else None\n",
    "\n",
    "    assert 2 * sub_size <= num_entities, \"sub_size is too large (E1 and E2 must be disjoint).\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # E1 / E2 / OTHER partition & functor f: E1→E2\n",
    "    # ----------------------------\n",
    "    perm = rng.permutation(num_entities)\n",
    "    E1_idx = perm[:sub_size].tolist()\n",
    "    E2_idx = perm[sub_size:2*sub_size].tolist()\n",
    "    OTHER_idx = perm[2*sub_size:].tolist() # not typically used\n",
    "\n",
    "    E1 = [ind2entity[i] for i in E1_idx]\n",
    "    E2 = [ind2entity[i] for i in E2_idx]\n",
    "\n",
    "    # f is a bijection E1→E2\n",
    "    E2_perm = rng.permutation(E2_idx).tolist()\n",
    "    f_map = {ind2entity[i]: ind2entity[j] for i, j in zip(E1_idx, E2_perm)}\n",
    "    if include_f_inverse:\n",
    "        f_inv_map = {v: k for k, v in f_map.items()}\n",
    "\n",
    "    # E1/E2 relation subset R_sub used internally\n",
    "    rsubset_size = max(1, num_relations)\n",
    "    # without duplicates\n",
    "    R_sub_idx = rng.choice(num_relations, size=rsubset_size, replace=False).tolist()\n",
    "\n",
    "    # ----------------------------\n",
    "    # Build atomic triples (h,r,t) (E1 first, then OTHER)\n",
    "    # ※ E2 copy not done here (done after ID determination)\n",
    "    # ----------------------------\n",
    "    atomic_dict = defaultdict(list)   # h -> [(r,t)]\n",
    "    atomics = []                      # [(h,r,t)]  ※ Do not include <f>/<f_inv> here\n",
    "\n",
    "    # E1: Internal edges (select from R_sub, connect to t in E1)\n",
    "    for hi in E1_idx:     # source node\n",
    "        used_r = set()\n",
    "        for ti in E1_idx: # target node\n",
    "            if ti == hi:\n",
    "                continue\n",
    "            if duplicate_relation:\n",
    "                r_idx = rng.choice(R_sub_idx)\n",
    "            else:\n",
    "                available_r = [r for r in R_sub_idx if r not in used_r] #hir used once does not appear again\n",
    "                if not available_r:\n",
    "                    break\n",
    "                r_idx = rng.choice(available_r)\n",
    "                used_r.add(r_idx)\n",
    "            h, r, t = ind2entity[hi], ind2relation[r_idx], ind2entity[ti]\n",
    "            atomics.append((h, r, t))\n",
    "            atomic_dict[h].append((r, t))\n",
    "\n",
    "    # OTHER: not typically used（categorynoise unrelated to category)\n",
    "    for hi in OTHER_idx:\n",
    "        for ti in OTHER_idx:\n",
    "            if ti == hi:\n",
    "                continue\n",
    "            r_idx = rng.choice(R_sub_idx)\n",
    "            h, r, t = ind2entity[hi], ind2relation[r_idx], ind2entity[ti]\n",
    "            atomics.append((h, r, t))\n",
    "            atomic_dict[h].append((r, t))\n",
    "\n",
    "    # ----------------------------\n",
    "    # OOD split for atomic facts\n",
    "    # ----------------------------\n",
    "    n_ood_atomic = round(len(atomics) * (1-atomic_ood_ratio))\n",
    "    ID_atomic_list, OOD_atomic_list = split(atomics, n_ood_atomic, rng=rng)\n",
    "    ID_atomic_facts, OOD_atomic_facts = set(ID_atomic_list), set(OOD_atomic_list)\n",
    "\n",
    "    # ----------------------------\n",
    "    #  E2 Create atomic_facts, same relation structure as E1.\n",
    "    # ----------------------------\n",
    "    for (h1, r, t1) in ID_atomic_list:\n",
    "        h2, t2 = f_map[h1], f_map[t1]\n",
    "        edge2 = (h2, r, t2)\n",
    "        ID_atomic_facts.add(edge2)\n",
    "        atomic_dict[h2].append((r, t2))\n",
    "\n",
    "    # atomic_fact  to JSON record\n",
    "    id_atomic_facts  = [form_items([h, r], t) for (h, r, t) in sorted(ID_atomic_facts)]\n",
    "    ood_atomic_facts = [form_items([h, r], t) for (h, r, t) in sorted(OOD_atomic_facts)]\n",
    "\n",
    "    # ---- Compositional Facts -------------\n",
    "    # 2-hop Reasoning: (h->r1->b->r2->t, (h, r1, r2, t)), h and t are in same category\n",
    "    # id_compositional_fact: (h, r1, b), (b, r2, t) both are in ID_atomic_facts and (h, r1, r2, t) is also ID\n",
    "    # far_ood_compositonal_fact: (h, r1, b), (b, r2, t) either is in OOD_atomic_facts and (h, r1, r2, t) is also OOD\n",
    "    # near_ood_compositional_fact: (h, r1, b), (b, r2, t) both in ID_atomic_facts but (h, r1, r2, t) is OOD\n",
    "    # Main goal is to test if near_ood_compositional_fact can be learned\n",
    "    # ----------------------------\n",
    "    id_compositional_facts, near_ood_compositional_facts, far_ood_compositional_facts = [], [], []\n",
    "    for ent in entities:\n",
    "        for (r1, b) in atomic_dict[ent]:\n",
    "            for (r2, t) in atomic_dict[b]:\n",
    "                s1 = (ent, r1, b)\n",
    "                s2 = (b, r2, t)\n",
    "                if ent == t:\n",
    "                    continue\n",
    "                if (s1 in OOD_atomic_facts) or (s2 in OOD_atomic_facts):\n",
    "                    far_ood_compositional_facts.append(form_items([ent, r1, r2], t))\n",
    "                else:\n",
    "                    if rng.uniform() > compositional_ood_ratio:\n",
    "                        id_compositional_facts.append(form_items([ent, r1, r2], t))\n",
    "                    else:\n",
    "                        near_ood_compositional_facts.append(form_items([ent, r1, r2], t))\n",
    "\n",
    "    # ---- Analogical Facts -------------\n",
    "    # 1-hop Reasoning: (h->f->t, (h, f, t)), h and t are in different categories\n",
    "    # id_analogical_fact:\n",
    "    # ood_analogical_fact: (h, r1, s), (s, r2, t) both in ID_atomic_facts but (h, r1, r2, t) is OOD\n",
    "    # Main goal is to test if near_ood_compositional_fact can be learned\n",
    "    # inv reverses the category direction\n",
    "    # ----------------------------\n",
    "    analogical_facts = [(e1, F, f_map[e1]) for e1 in E1]\n",
    "    if include_f_inverse:\n",
    "        inv_analogical_facts = [(f_map[e1], F_INV, e1) for e1 in E1]\n",
    "    else:\n",
    "        inv_analogical_facts = []\n",
    "    all_analogical_facts = analogical_facts+ inv_analogical_facts\n",
    "\n",
    "    n_analogical_ood = int(round(len(all_analogical_facts) * (1-analogical_ood_ratio)))\n",
    "    analogical_ID_list, analogical_OOD_list = split(all_analogical_facts, n_analogical_ood, rng=rng)\n",
    "    analogical_ID, analogical_OOD = set(analogical_ID_list), set(analogical_OOD_list)\n",
    "\n",
    "    id_analogical_facts  = [form_items([h, f], t) for (h, f, t) in sorted(analogical_ID)]\n",
    "    ood_analogical_facts = [form_items([h, f], t) for (h, f, t) in sorted(analogical_OOD)]\n",
    "\n",
    "    # return f correspondence dictionary\n",
    "    f_map_dict = dict(f_map)\n",
    "\n",
    "    return (entities, relations,\n",
    "            id_atomic_facts, ood_atomic_facts,\n",
    "            id_compositional_facts, near_ood_compositional_facts, far_ood_compositional_facts,\n",
    "            id_analogical_facts, ood_analogical_facts,\n",
    "            )\n",
    "\n",
    "# =========================\n",
    "# Example usage\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    NUM_ENTITY_IN = 10\n",
    "    NUM_RELATION  = 10000 # min (NUM_ENTITY_IN//2)-1\n",
    "    SUB_SIZE = NUM_ENTITY_IN//2 # |E1|=|E2|\n",
    "    ATOMIC_OOD_RATIO = 0.0\n",
    "    COMPOSITIONAL_OOD_RATIO = 0.1\n",
    "    ANALOGICAL_OOD_RATIO = 0.4\n",
    "\n",
    "\n",
    "    res = build_dataset_with_functor(\n",
    "        NUM_ENTITY_IN, NUM_RELATION,\n",
    "        sub_size=SUB_SIZE,\n",
    "        atomic_ood_ratio=ATOMIC_OOD_RATIO,\n",
    "        compositional_ood_ratio=COMPOSITIONAL_OOD_RATIO,\n",
    "        analogical_ood_ratio=ANALOGICAL_OOD_RATIO,\n",
    "        seed=42,\n",
    "        include_f_inverse=False, # whether to include inverse functor\n",
    "        duplicate_relation=False  # whether to include duplicate relations\n",
    "    )\n",
    "\n",
    "    (entities, relations,\n",
    "     id_atomic_facts, ood_atomic_facts,\n",
    "     id_compositional_facts, near_ood_compositional_facts, far_ood_compositional_facts,\n",
    "     id_analogical_facts, ood_analogical_facts,\n",
    "     ) = res\n",
    "\n",
    "    # simple summary\n",
    "    print(\"#entities:\", len(entities))\n",
    "    print(\"#relations:\", len(relations)) # R + <f>\n",
    "    print(\"ID atomics:\", len(id_atomic_facts))\n",
    "    print(\"OOD atomics:\", len(ood_atomic_facts))\n",
    "    print(\"ID compositional:\", len(id_compositional_facts))\n",
    "    print(\"near OOD compositional:\", len(near_ood_compositional_facts))\n",
    "    print(\"far OOD compositional:\", len(far_ood_compositional_facts))\n",
    "    print(\"ID analogical:\", len(id_analogical_facts))\n",
    "    print(\"OOD analogical:\", len(ood_analogical_facts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I8fDefir6A9A",
    "outputId": "92ce4cda-18fe-46ea-c0b5-c9a7bf9ec1d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID atomics: 40.0\n",
      "OOD atomics: 0.0\n"
     ]
    }
   ],
   "source": [
    "# theoretical value\n",
    "print(f\"ID atomics: {(SUB_SIZE)*(SUB_SIZE-1)*2*(1-ATOMIC_OOD_RATIO)}\")\n",
    "print(f\"OOD atomics: {(SUB_SIZE)*(SUB_SIZE-1)*2*ATOMIC_OOD_RATIO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6uBKr_gCnMcu",
    "outputId": "1e0942cc-f993-4bb6-fa9c-ab9180614051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<e_0>', '<e_1>', '<e_2>', '<e_3>', '<e_4>', '<e_5>', '<e_6>', '<e_7>', '<e_8>', '<e_9>', '<r_0>', '<r_1>', '<r_2>', '<r_3>', '<r_4>', '<r_5>', '<r_6>', '<r_7>', '<r_8>', '<r_9>', '<r_10>', '<r_11>', '<r_12>', '<r_13>', '<r_14>', '<r_15>', '<r_16>', '<r_17>', '<r_18>', '<r_19>', '<r_20>', '<r_21>', '<r_22>', '<r_23>', '<r_24>', '<r_25>', '<r_26>', '<r_27>', '<r_28>', '<r_29>', '<r_30>', '<r_31>', '<r_32>', '<r_33>', '<r_34>', '<r_35>', '<r_36>', '<r_37>', '<r_38>', '<r_39>', '<r_40>', '<r_41>', '<r_42>', '<r_43>', '<r_44>', '<r_45>', '<r_46>', '<r_47>', '<r_48>', '<r_49>', '<r_50>', '<r_51>', '<r_52>', '<r_53>', '<r_54>', '<r_55>', '<r_56>', '<r_57>', '<r_58>', '<r_59>', '<r_60>', '<r_61>', '<r_62>', '<r_63>', '<r_64>', '<r_65>', '<r_66>', '<r_67>', '<r_68>', '<r_69>', '<r_70>', '<r_71>', '<r_72>', '<r_73>', '<r_74>', '<r_75>', '<r_76>', '<r_77>', '<r_78>', '<r_79>', '<r_80>', '<r_81>', '<r_82>', '<r_83>', '<r_84>', '<r_85>', '<r_86>', '<r_87>', '<r_88>', '<r_89>', '<r_90>', '<r_91>', '<r_92>', '<r_93>', '<r_94>', '<r_95>', '<r_96>', '<r_97>', '<r_98>', '<r_99>', '<r_100>', '<r_101>', '<r_102>', '<r_103>', '<r_104>', '<r_105>', '<r_106>', '<r_107>', '<r_108>', '<r_109>', '<r_110>', '<r_111>', '<r_112>', '<r_113>', '<r_114>', '<r_115>', '<r_116>', '<r_117>', '<r_118>', '<r_119>', '<r_120>', '<r_121>', '<r_122>', '<r_123>', '<r_124>', '<r_125>', '<r_126>', '<r_127>', '<r_128>', '<r_129>', '<r_130>', '<r_131>', '<r_132>', '<r_133>', '<r_134>', '<r_135>', '<r_136>', '<r_137>', '<r_138>', '<r_139>', '<r_140>', '<r_141>', '<r_142>', '<r_143>', '<r_144>', '<r_145>', '<r_146>', '<r_147>', '<r_148>', '<r_149>', '<r_150>', '<r_151>', '<r_152>', '<r_153>', '<r_154>', '<r_155>', '<r_156>', '<r_157>', '<r_158>', '<r_159>', '<r_160>', '<r_161>', '<r_162>', '<r_163>', '<r_164>', '<r_165>', '<r_166>', '<r_167>', '<r_168>', '<r_169>', '<r_170>', '<r_171>', '<r_172>', '<r_173>', '<r_174>', '<r_175>', '<r_176>', '<r_177>', '<r_178>', '<r_179>', '<r_180>', '<r_181>', '<r_182>', '<r_183>', '<r_184>', '<r_185>', '<r_186>', '<r_187>', '<r_188>', '<r_189>', '<r_190>', '<r_191>', '<r_192>', '<r_193>', '<r_194>', '<r_195>', '<r_196>', '<r_197>', '<r_198>', '<r_199>', '<r_200>', '<r_201>', '<r_202>', '<r_203>', '<r_204>', '<r_205>', '<r_206>', '<r_207>', '<r_208>', '<r_209>', '<r_210>', '<r_211>', '<r_212>', '<r_213>', '<r_214>', '<r_215>', '<r_216>', '<r_217>', '<r_218>', '<r_219>', '<r_220>', '<r_221>', '<r_222>', '<r_223>', '<r_224>', '<r_225>', '<r_226>', '<r_227>', '<r_228>', '<r_229>', '<r_230>', '<r_231>', '<r_232>', '<r_233>', '<r_234>', '<r_235>', '<r_236>', '<r_237>', '<r_238>', '<r_239>', '<r_240>', '<r_241>', '<r_242>', '<r_243>', '<r_244>', '<r_245>', '<r_246>', '<r_247>', '<r_248>', '<r_249>', '<r_250>', '<r_251>', '<r_252>', '<r_253>', '<r_254>', '<r_255>', '<r_256>', '<r_257>', '<r_258>', '<r_259>', '<r_260>', '<r_261>', '<r_262>', '<r_263>', '<r_264>', '<r_265>', '<r_266>', '<r_267>', '<r_268>', '<r_269>', '<r_270>', '<r_271>', '<r_272>', '<r_273>', '<r_274>', '<r_275>', '<r_276>', '<r_277>', '<r_278>', '<r_279>', '<r_280>', '<r_281>', '<r_282>', '<r_283>', '<r_284>', '<r_285>', '<r_286>', '<r_287>', '<r_288>', '<r_289>', '<r_290>', '<r_291>', '<r_292>', '<r_293>', '<r_294>', '<r_295>', '<r_296>', '<r_297>', '<r_298>', '<r_299>', '<r_300>', '<r_301>', '<r_302>', '<r_303>', '<r_304>', '<r_305>', '<r_306>', '<r_307>', '<r_308>', '<r_309>', '<r_310>', '<r_311>', '<r_312>', '<r_313>', '<r_314>', '<r_315>', '<r_316>', '<r_317>', '<r_318>', '<r_319>', '<r_320>', '<r_321>', '<r_322>', '<r_323>', '<r_324>', '<r_325>', '<r_326>', '<r_327>', '<r_328>', '<r_329>', '<r_330>', '<r_331>', '<r_332>', '<r_333>', '<r_334>', '<r_335>', '<r_336>', '<r_337>', '<r_338>', '<r_339>', '<r_340>', '<r_341>', '<r_342>', '<r_343>', '<r_344>', '<r_345>', '<r_346>', '<r_347>', '<r_348>', '<r_349>', '<r_350>', '<r_351>', '<r_352>', '<r_353>', '<r_354>', '<r_355>', '<r_356>', '<r_357>', '<r_358>', '<r_359>', '<r_360>', '<r_361>', '<r_362>', '<r_363>', '<r_364>', '<r_365>', '<r_366>', '<r_367>', '<r_368>', '<r_369>', '<r_370>', '<r_371>', '<r_372>', '<r_373>', '<r_374>', '<r_375>', '<r_376>', '<r_377>', '<r_378>', '<r_379>', '<r_380>', '<r_381>', '<r_382>', '<r_383>', '<r_384>', '<r_385>', '<r_386>', '<r_387>', '<r_388>', '<r_389>', '<r_390>', '<r_391>', '<r_392>', '<r_393>', '<r_394>', '<r_395>', '<r_396>', '<r_397>', '<r_398>', '<r_399>', '<r_400>', '<r_401>', '<r_402>', '<r_403>', '<r_404>', '<r_405>', '<r_406>', '<r_407>', '<r_408>', '<r_409>', '<r_410>', '<r_411>', '<r_412>', '<r_413>', '<r_414>', '<r_415>', '<r_416>', '<r_417>', '<r_418>', '<r_419>', '<r_420>', '<r_421>', '<r_422>', '<r_423>', '<r_424>', '<r_425>', '<r_426>', '<r_427>', '<r_428>', '<r_429>', '<r_430>', '<r_431>', '<r_432>', '<r_433>', '<r_434>', '<r_435>', '<r_436>', '<r_437>', '<r_438>', '<r_439>', '<r_440>', '<r_441>', '<r_442>', '<r_443>', '<r_444>', '<r_445>', '<r_446>', '<r_447>', '<r_448>', '<r_449>', '<r_450>', '<r_451>', '<r_452>', '<r_453>', '<r_454>', '<r_455>', '<r_456>', '<r_457>', '<r_458>', '<r_459>', '<r_460>', '<r_461>', '<r_462>', '<r_463>', '<r_464>', '<r_465>', '<r_466>', '<r_467>', '<r_468>', '<r_469>', '<r_470>', '<r_471>', '<r_472>', '<r_473>', '<r_474>', '<r_475>', '<r_476>', '<r_477>', '<r_478>', '<r_479>', '<r_480>', '<r_481>', '<r_482>', '<r_483>', '<r_484>', '<r_485>', '<r_486>', '<r_487>', '<r_488>', '<r_489>', '<r_490>', '<r_491>', '<r_492>', '<r_493>', '<r_494>', '<r_495>', '<r_496>', '<r_497>', '<r_498>', '<r_499>', '<r_500>', '<r_501>', '<r_502>', '<r_503>', '<r_504>', '<r_505>', '<r_506>', '<r_507>', '<r_508>', '<r_509>', '<r_510>', '<r_511>', '<r_512>', '<r_513>', '<r_514>', '<r_515>', '<r_516>', '<r_517>', '<r_518>', '<r_519>', '<r_520>', '<r_521>', '<r_522>', '<r_523>', '<r_524>', '<r_525>', '<r_526>', '<r_527>', '<r_528>', '<r_529>', '<r_530>', '<r_531>', '<r_532>', '<r_533>', '<r_534>', '<r_535>', '<r_536>', '<r_537>', '<r_538>', '<r_539>', '<r_540>', '<r_541>', '<r_542>', '<r_543>', '<r_544>', '<r_545>', '<r_546>', '<r_547>', '<r_548>', '<r_549>', '<r_550>', '<r_551>', '<r_552>', '<r_553>', '<r_554>', '<r_555>', '<r_556>', '<r_557>', '<r_558>', '<r_559>', '<r_560>', '<r_561>', '<r_562>', '<r_563>', '<r_564>', '<r_565>', '<r_566>', '<r_567>', '<r_568>', '<r_569>', '<r_570>', '<r_571>', '<r_572>', '<r_573>', '<r_574>', '<r_575>', '<r_576>', '<r_577>', '<r_578>', '<r_579>', '<r_580>', '<r_581>', '<r_582>', '<r_583>', '<r_584>', '<r_585>', '<r_586>', '<r_587>', '<r_588>', '<r_589>', '<r_590>', '<r_591>', '<r_592>', '<r_593>', '<r_594>', '<r_595>', '<r_596>', '<r_597>', '<r_598>', '<r_599>', '<r_600>', '<r_601>', '<r_602>', '<r_603>', '<r_604>', '<r_605>', '<r_606>', '<r_607>', '<r_608>', '<r_609>', '<r_610>', '<r_611>', '<r_612>', '<r_613>', '<r_614>', '<r_615>', '<r_616>', '<r_617>', '<r_618>', '<r_619>', '<r_620>', '<r_621>', '<r_622>', '<r_623>', '<r_624>', '<r_625>', '<r_626>', '<r_627>', '<r_628>', '<r_629>', '<r_630>', '<r_631>', '<r_632>', '<r_633>', '<r_634>', '<r_635>', '<r_636>', '<r_637>', '<r_638>', '<r_639>', '<r_640>', '<r_641>', '<r_642>', '<r_643>', '<r_644>', '<r_645>', '<r_646>', '<r_647>', '<r_648>', '<r_649>', '<r_650>', '<r_651>', '<r_652>', '<r_653>', '<r_654>', '<r_655>', '<r_656>', '<r_657>', '<r_658>', '<r_659>', '<r_660>', '<r_661>', '<r_662>', '<r_663>', '<r_664>', '<r_665>', '<r_666>', '<r_667>', '<r_668>', '<r_669>', '<r_670>', '<r_671>', '<r_672>', '<r_673>', '<r_674>', '<r_675>', '<r_676>', '<r_677>', '<r_678>', '<r_679>', '<r_680>', '<r_681>', '<r_682>', '<r_683>', '<r_684>', '<r_685>', '<r_686>', '<r_687>', '<r_688>', '<r_689>', '<r_690>', '<r_691>', '<r_692>', '<r_693>', '<r_694>', '<r_695>', '<r_696>', '<r_697>', '<r_698>', '<r_699>', '<r_700>', '<r_701>', '<r_702>', '<r_703>', '<r_704>', '<r_705>', '<r_706>', '<r_707>', '<r_708>', '<r_709>', '<r_710>', '<r_711>', '<r_712>', '<r_713>', '<r_714>', '<r_715>', '<r_716>', '<r_717>', '<r_718>', '<r_719>', '<r_720>', '<r_721>', '<r_722>', '<r_723>', '<r_724>', '<r_725>', '<r_726>', '<r_727>', '<r_728>', '<r_729>', '<r_730>', '<r_731>', '<r_732>', '<r_733>', '<r_734>', '<r_735>', '<r_736>', '<r_737>', '<r_738>', '<r_739>', '<r_740>', '<r_741>', '<r_742>', '<r_743>', '<r_744>', '<r_745>', '<r_746>', '<r_747>', '<r_748>', '<r_749>', '<r_750>', '<r_751>', '<r_752>', '<r_753>', '<r_754>', '<r_755>', '<r_756>', '<r_757>', '<r_758>', '<r_759>', '<r_760>', '<r_761>', '<r_762>', '<r_763>', '<r_764>', '<r_765>', '<r_766>', '<r_767>', '<r_768>', '<r_769>', '<r_770>', '<r_771>', '<r_772>', '<r_773>', '<r_774>', '<r_775>', '<r_776>', '<r_777>', '<r_778>', '<r_779>', '<r_780>', '<r_781>', '<r_782>', '<r_783>', '<r_784>', '<r_785>', '<r_786>', '<r_787>', '<r_788>', '<r_789>', '<r_790>', '<r_791>', '<r_792>', '<r_793>', '<r_794>', '<r_795>', '<r_796>', '<r_797>', '<r_798>', '<r_799>', '<r_800>', '<r_801>', '<r_802>', '<r_803>', '<r_804>', '<r_805>', '<r_806>', '<r_807>', '<r_808>', '<r_809>', '<r_810>', '<r_811>', '<r_812>', '<r_813>', '<r_814>', '<r_815>', '<r_816>', '<r_817>', '<r_818>', '<r_819>', '<r_820>', '<r_821>', '<r_822>', '<r_823>', '<r_824>', '<r_825>', '<r_826>', '<r_827>', '<r_828>', '<r_829>', '<r_830>', '<r_831>', '<r_832>', '<r_833>', '<r_834>', '<r_835>', '<r_836>', '<r_837>', '<r_838>', '<r_839>', '<r_840>', '<r_841>', '<r_842>', '<r_843>', '<r_844>', '<r_845>', '<r_846>', '<r_847>', '<r_848>', '<r_849>', '<r_850>', '<r_851>', '<r_852>', '<r_853>', '<r_854>', '<r_855>', '<r_856>', '<r_857>', '<r_858>', '<r_859>', '<r_860>', '<r_861>', '<r_862>', '<r_863>', '<r_864>', '<r_865>', '<r_866>', '<r_867>', '<r_868>', '<r_869>', '<r_870>', '<r_871>', '<r_872>', '<r_873>', '<r_874>', '<r_875>', '<r_876>', '<r_877>', '<r_878>', '<r_879>', '<r_880>', '<r_881>', '<r_882>', '<r_883>', '<r_884>', '<r_885>', '<r_886>', '<r_887>', '<r_888>', '<r_889>', '<r_890>', '<r_891>', '<r_892>', '<r_893>', '<r_894>', '<r_895>', '<r_896>', '<r_897>', '<r_898>', '<r_899>', '<r_900>', '<r_901>', '<r_902>', '<r_903>', '<r_904>', '<r_905>', '<r_906>', '<r_907>', '<r_908>', '<r_909>', '<r_910>', '<r_911>', '<r_912>', '<r_913>', '<r_914>', '<r_915>', '<r_916>', '<r_917>', '<r_918>', '<r_919>', '<r_920>', '<r_921>', '<r_922>', '<r_923>', '<r_924>', '<r_925>', '<r_926>', '<r_927>', '<r_928>', '<r_929>', '<r_930>', '<r_931>', '<r_932>', '<r_933>', '<r_934>', '<r_935>', '<r_936>', '<r_937>', '<r_938>', '<r_939>', '<r_940>', '<r_941>', '<r_942>', '<r_943>', '<r_944>', '<r_945>', '<r_946>', '<r_947>', '<r_948>', '<r_949>', '<r_950>', '<r_951>', '<r_952>', '<r_953>', '<r_954>', '<r_955>', '<r_956>', '<r_957>', '<r_958>', '<r_959>', '<r_960>', '<r_961>', '<r_962>', '<r_963>', '<r_964>', '<r_965>', '<r_966>', '<r_967>', '<r_968>', '<r_969>', '<r_970>', '<r_971>', '<r_972>', '<r_973>', '<r_974>', '<r_975>', '<r_976>', '<r_977>', '<r_978>', '<r_979>', '<r_980>', '<r_981>', '<r_982>', '<r_983>', '<r_984>', '<r_985>', '<r_986>', '<r_987>', '<r_988>', '<r_989>', '<r_990>', '<r_991>', '<r_992>', '<r_993>', '<r_994>', '<r_995>', '<r_996>', '<r_997>', '<r_998>', '<r_999>', '<r_1000>', '<r_1001>', '<r_1002>', '<r_1003>', '<r_1004>', '<r_1005>', '<r_1006>', '<r_1007>', '<r_1008>', '<r_1009>', '<r_1010>', '<r_1011>', '<r_1012>', '<r_1013>', '<r_1014>', '<r_1015>', '<r_1016>', '<r_1017>', '<r_1018>', '<r_1019>', '<r_1020>', '<r_1021>', '<r_1022>', '<r_1023>', '<r_1024>', '<r_1025>', '<r_1026>', '<r_1027>', '<r_1028>', '<r_1029>', '<r_1030>', '<r_1031>', '<r_1032>', '<r_1033>', '<r_1034>', '<r_1035>', '<r_1036>', '<r_1037>', '<r_1038>', '<r_1039>', '<r_1040>', '<r_1041>', '<r_1042>', '<r_1043>', '<r_1044>', '<r_1045>', '<r_1046>', '<r_1047>', '<r_1048>', '<r_1049>', '<r_1050>', '<r_1051>', '<r_1052>', '<r_1053>', '<r_1054>', '<r_1055>', '<r_1056>', '<r_1057>', '<r_1058>', '<r_1059>', '<r_1060>', '<r_1061>', '<r_1062>', '<r_1063>', '<r_1064>', '<r_1065>', '<r_1066>', '<r_1067>', '<r_1068>', '<r_1069>', '<r_1070>', '<r_1071>', '<r_1072>', '<r_1073>', '<r_1074>', '<r_1075>', '<r_1076>', '<r_1077>', '<r_1078>', '<r_1079>', '<r_1080>', '<r_1081>', '<r_1082>', '<r_1083>', '<r_1084>', '<r_1085>', '<r_1086>', '<r_1087>', '<r_1088>', '<r_1089>', '<r_1090>', '<r_1091>', '<r_1092>', '<r_1093>', '<r_1094>', '<r_1095>', '<r_1096>', '<r_1097>', '<r_1098>', '<r_1099>', '<r_1100>', '<r_1101>', '<r_1102>', '<r_1103>', '<r_1104>', '<r_1105>', '<r_1106>', '<r_1107>', '<r_1108>', '<r_1109>', '<r_1110>', '<r_1111>', '<r_1112>', '<r_1113>', '<r_1114>', '<r_1115>', '<r_1116>', '<r_1117>', '<r_1118>', '<r_1119>', '<r_1120>', '<r_1121>', '<r_1122>', '<r_1123>', '<r_1124>', '<r_1125>', '<r_1126>', '<r_1127>', '<r_1128>', '<r_1129>', '<r_1130>', '<r_1131>', '<r_1132>', '<r_1133>', '<r_1134>', '<r_1135>', '<r_1136>', '<r_1137>', '<r_1138>', '<r_1139>', '<r_1140>', '<r_1141>', '<r_1142>', '<r_1143>', '<r_1144>', '<r_1145>', '<r_1146>', '<r_1147>', '<r_1148>', '<r_1149>', '<r_1150>', '<r_1151>', '<r_1152>', '<r_1153>', '<r_1154>', '<r_1155>', '<r_1156>', '<r_1157>', '<r_1158>', '<r_1159>', '<r_1160>', '<r_1161>', '<r_1162>', '<r_1163>', '<r_1164>', '<r_1165>', '<r_1166>', '<r_1167>', '<r_1168>', '<r_1169>', '<r_1170>', '<r_1171>', '<r_1172>', '<r_1173>', '<r_1174>', '<r_1175>', '<r_1176>', '<r_1177>', '<r_1178>', '<r_1179>', '<r_1180>', '<r_1181>', '<r_1182>', '<r_1183>', '<r_1184>', '<r_1185>', '<r_1186>', '<r_1187>', '<r_1188>', '<r_1189>', '<r_1190>', '<r_1191>', '<r_1192>', '<r_1193>', '<r_1194>', '<r_1195>', '<r_1196>', '<r_1197>', '<r_1198>', '<r_1199>', '<r_1200>', '<r_1201>', '<r_1202>', '<r_1203>', '<r_1204>', '<r_1205>', '<r_1206>', '<r_1207>', '<r_1208>', '<r_1209>', '<r_1210>', '<r_1211>', '<r_1212>', '<r_1213>', '<r_1214>', '<r_1215>', '<r_1216>', '<r_1217>', '<r_1218>', '<r_1219>', '<r_1220>', '<r_1221>', '<r_1222>', '<r_1223>', '<r_1224>', '<r_1225>', '<r_1226>', '<r_1227>', '<r_1228>', '<r_1229>', '<r_1230>', '<r_1231>', '<r_1232>', '<r_1233>', '<r_1234>', '<r_1235>', '<r_1236>', '<r_1237>', '<r_1238>', '<r_1239>', '<r_1240>', '<r_1241>', '<r_1242>', '<r_1243>', '<r_1244>', '<r_1245>', '<r_1246>', '<r_1247>', '<r_1248>', '<r_1249>', '<r_1250>', '<r_1251>', '<r_1252>', '<r_1253>', '<r_1254>', '<r_1255>', '<r_1256>', '<r_1257>', '<r_1258>', '<r_1259>', '<r_1260>', '<r_1261>', '<r_1262>', '<r_1263>', '<r_1264>', '<r_1265>', '<r_1266>', '<r_1267>', '<r_1268>', '<r_1269>', '<r_1270>', '<r_1271>', '<r_1272>', '<r_1273>', '<r_1274>', '<r_1275>', '<r_1276>', '<r_1277>', '<r_1278>', '<r_1279>', '<r_1280>', '<r_1281>', '<r_1282>', '<r_1283>', '<r_1284>', '<r_1285>', '<r_1286>', '<r_1287>', '<r_1288>', '<r_1289>', '<r_1290>', '<r_1291>', '<r_1292>', '<r_1293>', '<r_1294>', '<r_1295>', '<r_1296>', '<r_1297>', '<r_1298>', '<r_1299>', '<r_1300>', '<r_1301>', '<r_1302>', '<r_1303>', '<r_1304>', '<r_1305>', '<r_1306>', '<r_1307>', '<r_1308>', '<r_1309>', '<r_1310>', '<r_1311>', '<r_1312>', '<r_1313>', '<r_1314>', '<r_1315>', '<r_1316>', '<r_1317>', '<r_1318>', '<r_1319>', '<r_1320>', '<r_1321>', '<r_1322>', '<r_1323>', '<r_1324>', '<r_1325>', '<r_1326>', '<r_1327>', '<r_1328>', '<r_1329>', '<r_1330>', '<r_1331>', '<r_1332>', '<r_1333>', '<r_1334>', '<r_1335>', '<r_1336>', '<r_1337>', '<r_1338>', '<r_1339>', '<r_1340>', '<r_1341>', '<r_1342>', '<r_1343>', '<r_1344>', '<r_1345>', '<r_1346>', '<r_1347>', '<r_1348>', '<r_1349>', '<r_1350>', '<r_1351>', '<r_1352>', '<r_1353>', '<r_1354>', '<r_1355>', '<r_1356>', '<r_1357>', '<r_1358>', '<r_1359>', '<r_1360>', '<r_1361>', '<r_1362>', '<r_1363>', '<r_1364>', '<r_1365>', '<r_1366>', '<r_1367>', '<r_1368>', '<r_1369>', '<r_1370>', '<r_1371>', '<r_1372>', '<r_1373>', '<r_1374>', '<r_1375>', '<r_1376>', '<r_1377>', '<r_1378>', '<r_1379>', '<r_1380>', '<r_1381>', '<r_1382>', '<r_1383>', '<r_1384>', '<r_1385>', '<r_1386>', '<r_1387>', '<r_1388>', '<r_1389>', '<r_1390>', '<r_1391>', '<r_1392>', '<r_1393>', '<r_1394>', '<r_1395>', '<r_1396>', '<r_1397>', '<r_1398>', '<r_1399>', '<r_1400>', '<r_1401>', '<r_1402>', '<r_1403>', '<r_1404>', '<r_1405>', '<r_1406>', '<r_1407>', '<r_1408>', '<r_1409>', '<r_1410>', '<r_1411>', '<r_1412>', '<r_1413>', '<r_1414>', '<r_1415>', '<r_1416>', '<r_1417>', '<r_1418>', '<r_1419>', '<r_1420>', '<r_1421>', '<r_1422>', '<r_1423>', '<r_1424>', '<r_1425>', '<r_1426>', '<r_1427>', '<r_1428>', '<r_1429>', '<r_1430>', '<r_1431>', '<r_1432>', '<r_1433>', '<r_1434>', '<r_1435>', '<r_1436>', '<r_1437>', '<r_1438>', '<r_1439>', '<r_1440>', '<r_1441>', '<r_1442>', '<r_1443>', '<r_1444>', '<r_1445>', '<r_1446>', '<r_1447>', '<r_1448>', '<r_1449>', '<r_1450>', '<r_1451>', '<r_1452>', '<r_1453>', '<r_1454>', '<r_1455>', '<r_1456>', '<r_1457>', '<r_1458>', '<r_1459>', '<r_1460>', '<r_1461>', '<r_1462>', '<r_1463>', '<r_1464>', '<r_1465>', '<r_1466>', '<r_1467>', '<r_1468>', '<r_1469>', '<r_1470>', '<r_1471>', '<r_1472>', '<r_1473>', '<r_1474>', '<r_1475>', '<r_1476>', '<r_1477>', '<r_1478>', '<r_1479>', '<r_1480>', '<r_1481>', '<r_1482>', '<r_1483>', '<r_1484>', '<r_1485>', '<r_1486>', '<r_1487>', '<r_1488>', '<r_1489>', '<r_1490>', '<r_1491>', '<r_1492>', '<r_1493>', '<r_1494>', '<r_1495>', '<r_1496>', '<r_1497>', '<r_1498>', '<r_1499>', '<r_1500>', '<r_1501>', '<r_1502>', '<r_1503>', '<r_1504>', '<r_1505>', '<r_1506>', '<r_1507>', '<r_1508>', '<r_1509>', '<r_1510>', '<r_1511>', '<r_1512>', '<r_1513>', '<r_1514>', '<r_1515>', '<r_1516>', '<r_1517>', '<r_1518>', '<r_1519>', '<r_1520>', '<r_1521>', '<r_1522>', '<r_1523>', '<r_1524>', '<r_1525>', '<r_1526>', '<r_1527>', '<r_1528>', '<r_1529>', '<r_1530>', '<r_1531>', '<r_1532>', '<r_1533>', '<r_1534>', '<r_1535>', '<r_1536>', '<r_1537>', '<r_1538>', '<r_1539>', '<r_1540>', '<r_1541>', '<r_1542>', '<r_1543>', '<r_1544>', '<r_1545>', '<r_1546>', '<r_1547>', '<r_1548>', '<r_1549>', '<r_1550>', '<r_1551>', '<r_1552>', '<r_1553>', '<r_1554>', '<r_1555>', '<r_1556>', '<r_1557>', '<r_1558>', '<r_1559>', '<r_1560>', '<r_1561>', '<r_1562>', '<r_1563>', '<r_1564>', '<r_1565>', '<r_1566>', '<r_1567>', '<r_1568>', '<r_1569>', '<r_1570>', '<r_1571>', '<r_1572>', '<r_1573>', '<r_1574>', '<r_1575>', '<r_1576>', '<r_1577>', '<r_1578>', '<r_1579>', '<r_1580>', '<r_1581>', '<r_1582>', '<r_1583>', '<r_1584>', '<r_1585>', '<r_1586>', '<r_1587>', '<r_1588>', '<r_1589>', '<r_1590>', '<r_1591>', '<r_1592>', '<r_1593>', '<r_1594>', '<r_1595>', '<r_1596>', '<r_1597>', '<r_1598>', '<r_1599>', '<r_1600>', '<r_1601>', '<r_1602>', '<r_1603>', '<r_1604>', '<r_1605>', '<r_1606>', '<r_1607>', '<r_1608>', '<r_1609>', '<r_1610>', '<r_1611>', '<r_1612>', '<r_1613>', '<r_1614>', '<r_1615>', '<r_1616>', '<r_1617>', '<r_1618>', '<r_1619>', '<r_1620>', '<r_1621>', '<r_1622>', '<r_1623>', '<r_1624>', '<r_1625>', '<r_1626>', '<r_1627>', '<r_1628>', '<r_1629>', '<r_1630>', '<r_1631>', '<r_1632>', '<r_1633>', '<r_1634>', '<r_1635>', '<r_1636>', '<r_1637>', '<r_1638>', '<r_1639>', '<r_1640>', '<r_1641>', '<r_1642>', '<r_1643>', '<r_1644>', '<r_1645>', '<r_1646>', '<r_1647>', '<r_1648>', '<r_1649>', '<r_1650>', '<r_1651>', '<r_1652>', '<r_1653>', '<r_1654>', '<r_1655>', '<r_1656>', '<r_1657>', '<r_1658>', '<r_1659>', '<r_1660>', '<r_1661>', '<r_1662>', '<r_1663>', '<r_1664>', '<r_1665>', '<r_1666>', '<r_1667>', '<r_1668>', '<r_1669>', '<r_1670>', '<r_1671>', '<r_1672>', '<r_1673>', '<r_1674>', '<r_1675>', '<r_1676>', '<r_1677>', '<r_1678>', '<r_1679>', '<r_1680>', '<r_1681>', '<r_1682>', '<r_1683>', '<r_1684>', '<r_1685>', '<r_1686>', '<r_1687>', '<r_1688>', '<r_1689>', '<r_1690>', '<r_1691>', '<r_1692>', '<r_1693>', '<r_1694>', '<r_1695>', '<r_1696>', '<r_1697>', '<r_1698>', '<r_1699>', '<r_1700>', '<r_1701>', '<r_1702>', '<r_1703>', '<r_1704>', '<r_1705>', '<r_1706>', '<r_1707>', '<r_1708>', '<r_1709>', '<r_1710>', '<r_1711>', '<r_1712>', '<r_1713>', '<r_1714>', '<r_1715>', '<r_1716>', '<r_1717>', '<r_1718>', '<r_1719>', '<r_1720>', '<r_1721>', '<r_1722>', '<r_1723>', '<r_1724>', '<r_1725>', '<r_1726>', '<r_1727>', '<r_1728>', '<r_1729>', '<r_1730>', '<r_1731>', '<r_1732>', '<r_1733>', '<r_1734>', '<r_1735>', '<r_1736>', '<r_1737>', '<r_1738>', '<r_1739>', '<r_1740>', '<r_1741>', '<r_1742>', '<r_1743>', '<r_1744>', '<r_1745>', '<r_1746>', '<r_1747>', '<r_1748>', '<r_1749>', '<r_1750>', '<r_1751>', '<r_1752>', '<r_1753>', '<r_1754>', '<r_1755>', '<r_1756>', '<r_1757>', '<r_1758>', '<r_1759>', '<r_1760>', '<r_1761>', '<r_1762>', '<r_1763>', '<r_1764>', '<r_1765>', '<r_1766>', '<r_1767>', '<r_1768>', '<r_1769>', '<r_1770>', '<r_1771>', '<r_1772>', '<r_1773>', '<r_1774>', '<r_1775>', '<r_1776>', '<r_1777>', '<r_1778>', '<r_1779>', '<r_1780>', '<r_1781>', '<r_1782>', '<r_1783>', '<r_1784>', '<r_1785>', '<r_1786>', '<r_1787>', '<r_1788>', '<r_1789>', '<r_1790>', '<r_1791>', '<r_1792>', '<r_1793>', '<r_1794>', '<r_1795>', '<r_1796>', '<r_1797>', '<r_1798>', '<r_1799>', '<r_1800>', '<r_1801>', '<r_1802>', '<r_1803>', '<r_1804>', '<r_1805>', '<r_1806>', '<r_1807>', '<r_1808>', '<r_1809>', '<r_1810>', '<r_1811>', '<r_1812>', '<r_1813>', '<r_1814>', '<r_1815>', '<r_1816>', '<r_1817>', '<r_1818>', '<r_1819>', '<r_1820>', '<r_1821>', '<r_1822>', '<r_1823>', '<r_1824>', '<r_1825>', '<r_1826>', '<r_1827>', '<r_1828>', '<r_1829>', '<r_1830>', '<r_1831>', '<r_1832>', '<r_1833>', '<r_1834>', '<r_1835>', '<r_1836>', '<r_1837>', '<r_1838>', '<r_1839>', '<r_1840>', '<r_1841>', '<r_1842>', '<r_1843>', '<r_1844>', '<r_1845>', '<r_1846>', '<r_1847>', '<r_1848>', '<r_1849>', '<r_1850>', '<r_1851>', '<r_1852>', '<r_1853>', '<r_1854>', '<r_1855>', '<r_1856>', '<r_1857>', '<r_1858>', '<r_1859>', '<r_1860>', '<r_1861>', '<r_1862>', '<r_1863>', '<r_1864>', '<r_1865>', '<r_1866>', '<r_1867>', '<r_1868>', '<r_1869>', '<r_1870>', '<r_1871>', '<r_1872>', '<r_1873>', '<r_1874>', '<r_1875>', '<r_1876>', '<r_1877>', '<r_1878>', '<r_1879>', '<r_1880>', '<r_1881>', '<r_1882>', '<r_1883>', '<r_1884>', '<r_1885>', '<r_1886>', '<r_1887>', '<r_1888>', '<r_1889>', '<r_1890>', '<r_1891>', '<r_1892>', '<r_1893>', '<r_1894>', '<r_1895>', '<r_1896>', '<r_1897>', '<r_1898>', '<r_1899>', '<r_1900>', '<r_1901>', '<r_1902>', '<r_1903>', '<r_1904>', '<r_1905>', '<r_1906>', '<r_1907>', '<r_1908>', '<r_1909>', '<r_1910>', '<r_1911>', '<r_1912>', '<r_1913>', '<r_1914>', '<r_1915>', '<r_1916>', '<r_1917>', '<r_1918>', '<r_1919>', '<r_1920>', '<r_1921>', '<r_1922>', '<r_1923>', '<r_1924>', '<r_1925>', '<r_1926>', '<r_1927>', '<r_1928>', '<r_1929>', '<r_1930>', '<r_1931>', '<r_1932>', '<r_1933>', '<r_1934>', '<r_1935>', '<r_1936>', '<r_1937>', '<r_1938>', '<r_1939>', '<r_1940>', '<r_1941>', '<r_1942>', '<r_1943>', '<r_1944>', '<r_1945>', '<r_1946>', '<r_1947>', '<r_1948>', '<r_1949>', '<r_1950>', '<r_1951>', '<r_1952>', '<r_1953>', '<r_1954>', '<r_1955>', '<r_1956>', '<r_1957>', '<r_1958>', '<r_1959>', '<r_1960>', '<r_1961>', '<r_1962>', '<r_1963>', '<r_1964>', '<r_1965>', '<r_1966>', '<r_1967>', '<r_1968>', '<r_1969>', '<r_1970>', '<r_1971>', '<r_1972>', '<r_1973>', '<r_1974>', '<r_1975>', '<r_1976>', '<r_1977>', '<r_1978>', '<r_1979>', '<r_1980>', '<r_1981>', '<r_1982>', '<r_1983>', '<r_1984>', '<r_1985>', '<r_1986>', '<r_1987>', '<r_1988>', '<r_1989>', '<r_1990>', '<r_1991>', '<r_1992>', '<r_1993>', '<r_1994>', '<r_1995>', '<r_1996>', '<r_1997>', '<r_1998>', '<r_1999>', '<r_2000>', '<r_2001>', '<r_2002>', '<r_2003>', '<r_2004>', '<r_2005>', '<r_2006>', '<r_2007>', '<r_2008>', '<r_2009>', '<r_2010>', '<r_2011>', '<r_2012>', '<r_2013>', '<r_2014>', '<r_2015>', '<r_2016>', '<r_2017>', '<r_2018>', '<r_2019>', '<r_2020>', '<r_2021>', '<r_2022>', '<r_2023>', '<r_2024>', '<r_2025>', '<r_2026>', '<r_2027>', '<r_2028>', '<r_2029>', '<r_2030>', '<r_2031>', '<r_2032>', '<r_2033>', '<r_2034>', '<r_2035>', '<r_2036>', '<r_2037>', '<r_2038>', '<r_2039>', '<r_2040>', '<r_2041>', '<r_2042>', '<r_2043>', '<r_2044>', '<r_2045>', '<r_2046>', '<r_2047>', '<r_2048>', '<r_2049>', '<r_2050>', '<r_2051>', '<r_2052>', '<r_2053>', '<r_2054>', '<r_2055>', '<r_2056>', '<r_2057>', '<r_2058>', '<r_2059>', '<r_2060>', '<r_2061>', '<r_2062>', '<r_2063>', '<r_2064>', '<r_2065>', '<r_2066>', '<r_2067>', '<r_2068>', '<r_2069>', '<r_2070>', '<r_2071>', '<r_2072>', '<r_2073>', '<r_2074>', '<r_2075>', '<r_2076>', '<r_2077>', '<r_2078>', '<r_2079>', '<r_2080>', '<r_2081>', '<r_2082>', '<r_2083>', '<r_2084>', '<r_2085>', '<r_2086>', '<r_2087>', '<r_2088>', '<r_2089>', '<r_2090>', '<r_2091>', '<r_2092>', '<r_2093>', '<r_2094>', '<r_2095>', '<r_2096>', '<r_2097>', '<r_2098>', '<r_2099>', '<r_2100>', '<r_2101>', '<r_2102>', '<r_2103>', '<r_2104>', '<r_2105>', '<r_2106>', '<r_2107>', '<r_2108>', '<r_2109>', '<r_2110>', '<r_2111>', '<r_2112>', '<r_2113>', '<r_2114>', '<r_2115>', '<r_2116>', '<r_2117>', '<r_2118>', '<r_2119>', '<r_2120>', '<r_2121>', '<r_2122>', '<r_2123>', '<r_2124>', '<r_2125>', '<r_2126>', '<r_2127>', '<r_2128>', '<r_2129>', '<r_2130>', '<r_2131>', '<r_2132>', '<r_2133>', '<r_2134>', '<r_2135>', '<r_2136>', '<r_2137>', '<r_2138>', '<r_2139>', '<r_2140>', '<r_2141>', '<r_2142>', '<r_2143>', '<r_2144>', '<r_2145>', '<r_2146>', '<r_2147>', '<r_2148>', '<r_2149>', '<r_2150>', '<r_2151>', '<r_2152>', '<r_2153>', '<r_2154>', '<r_2155>', '<r_2156>', '<r_2157>', '<r_2158>', '<r_2159>', '<r_2160>', '<r_2161>', '<r_2162>', '<r_2163>', '<r_2164>', '<r_2165>', '<r_2166>', '<r_2167>', '<r_2168>', '<r_2169>', '<r_2170>', '<r_2171>', '<r_2172>', '<r_2173>', '<r_2174>', '<r_2175>', '<r_2176>', '<r_2177>', '<r_2178>', '<r_2179>', '<r_2180>', '<r_2181>', '<r_2182>', '<r_2183>', '<r_2184>', '<r_2185>', '<r_2186>', '<r_2187>', '<r_2188>', '<r_2189>', '<r_2190>', '<r_2191>', '<r_2192>', '<r_2193>', '<r_2194>', '<r_2195>', '<r_2196>', '<r_2197>', '<r_2198>', '<r_2199>', '<r_2200>', '<r_2201>', '<r_2202>', '<r_2203>', '<r_2204>', '<r_2205>', '<r_2206>', '<r_2207>', '<r_2208>', '<r_2209>', '<r_2210>', '<r_2211>', '<r_2212>', '<r_2213>', '<r_2214>', '<r_2215>', '<r_2216>', '<r_2217>', '<r_2218>', '<r_2219>', '<r_2220>', '<r_2221>', '<r_2222>', '<r_2223>', '<r_2224>', '<r_2225>', '<r_2226>', '<r_2227>', '<r_2228>', '<r_2229>', '<r_2230>', '<r_2231>', '<r_2232>', '<r_2233>', '<r_2234>', '<r_2235>', '<r_2236>', '<r_2237>', '<r_2238>', '<r_2239>', '<r_2240>', '<r_2241>', '<r_2242>', '<r_2243>', '<r_2244>', '<r_2245>', '<r_2246>', '<r_2247>', '<r_2248>', '<r_2249>', '<r_2250>', '<r_2251>', '<r_2252>', '<r_2253>', '<r_2254>', '<r_2255>', '<r_2256>', '<r_2257>', '<r_2258>', '<r_2259>', '<r_2260>', '<r_2261>', '<r_2262>', '<r_2263>', '<r_2264>', '<r_2265>', '<r_2266>', '<r_2267>', '<r_2268>', '<r_2269>', '<r_2270>', '<r_2271>', '<r_2272>', '<r_2273>', '<r_2274>', '<r_2275>', '<r_2276>', '<r_2277>', '<r_2278>', '<r_2279>', '<r_2280>', '<r_2281>', '<r_2282>', '<r_2283>', '<r_2284>', '<r_2285>', '<r_2286>', '<r_2287>', '<r_2288>', '<r_2289>', '<r_2290>', '<r_2291>', '<r_2292>', '<r_2293>', '<r_2294>', '<r_2295>', '<r_2296>', '<r_2297>', '<r_2298>', '<r_2299>', '<r_2300>', '<r_2301>', '<r_2302>', '<r_2303>', '<r_2304>', '<r_2305>', '<r_2306>', '<r_2307>', '<r_2308>', '<r_2309>', '<r_2310>', '<r_2311>', '<r_2312>', '<r_2313>', '<r_2314>', '<r_2315>', '<r_2316>', '<r_2317>', '<r_2318>', '<r_2319>', '<r_2320>', '<r_2321>', '<r_2322>', '<r_2323>', '<r_2324>', '<r_2325>', '<r_2326>', '<r_2327>', '<r_2328>', '<r_2329>', '<r_2330>', '<r_2331>', '<r_2332>', '<r_2333>', '<r_2334>', '<r_2335>', '<r_2336>', '<r_2337>', '<r_2338>', '<r_2339>', '<r_2340>', '<r_2341>', '<r_2342>', '<r_2343>', '<r_2344>', '<r_2345>', '<r_2346>', '<r_2347>', '<r_2348>', '<r_2349>', '<r_2350>', '<r_2351>', '<r_2352>', '<r_2353>', '<r_2354>', '<r_2355>', '<r_2356>', '<r_2357>', '<r_2358>', '<r_2359>', '<r_2360>', '<r_2361>', '<r_2362>', '<r_2363>', '<r_2364>', '<r_2365>', '<r_2366>', '<r_2367>', '<r_2368>', '<r_2369>', '<r_2370>', '<r_2371>', '<r_2372>', '<r_2373>', '<r_2374>', '<r_2375>', '<r_2376>', '<r_2377>', '<r_2378>', '<r_2379>', '<r_2380>', '<r_2381>', '<r_2382>', '<r_2383>', '<r_2384>', '<r_2385>', '<r_2386>', '<r_2387>', '<r_2388>', '<r_2389>', '<r_2390>', '<r_2391>', '<r_2392>', '<r_2393>', '<r_2394>', '<r_2395>', '<r_2396>', '<r_2397>', '<r_2398>', '<r_2399>', '<r_2400>', '<r_2401>', '<r_2402>', '<r_2403>', '<r_2404>', '<r_2405>', '<r_2406>', '<r_2407>', '<r_2408>', '<r_2409>', '<r_2410>', '<r_2411>', '<r_2412>', '<r_2413>', '<r_2414>', '<r_2415>', '<r_2416>', '<r_2417>', '<r_2418>', '<r_2419>', '<r_2420>', '<r_2421>', '<r_2422>', '<r_2423>', '<r_2424>', '<r_2425>', '<r_2426>', '<r_2427>', '<r_2428>', '<r_2429>', '<r_2430>', '<r_2431>', '<r_2432>', '<r_2433>', '<r_2434>', '<r_2435>', '<r_2436>', '<r_2437>', '<r_2438>', '<r_2439>', '<r_2440>', '<r_2441>', '<r_2442>', '<r_2443>', '<r_2444>', '<r_2445>', '<r_2446>', '<r_2447>', '<r_2448>', '<r_2449>', '<r_2450>', '<r_2451>', '<r_2452>', '<r_2453>', '<r_2454>', '<r_2455>', '<r_2456>', '<r_2457>', '<r_2458>', '<r_2459>', '<r_2460>', '<r_2461>', '<r_2462>', '<r_2463>', '<r_2464>', '<r_2465>', '<r_2466>', '<r_2467>', '<r_2468>', '<r_2469>', '<r_2470>', '<r_2471>', '<r_2472>', '<r_2473>', '<r_2474>', '<r_2475>', '<r_2476>', '<r_2477>', '<r_2478>', '<r_2479>', '<r_2480>', '<r_2481>', '<r_2482>', '<r_2483>', '<r_2484>', '<r_2485>', '<r_2486>', '<r_2487>', '<r_2488>', '<r_2489>', '<r_2490>', '<r_2491>', '<r_2492>', '<r_2493>', '<r_2494>', '<r_2495>', '<r_2496>', '<r_2497>', '<r_2498>', '<r_2499>', '<r_2500>', '<r_2501>', '<r_2502>', '<r_2503>', '<r_2504>', '<r_2505>', '<r_2506>', '<r_2507>', '<r_2508>', '<r_2509>', '<r_2510>', '<r_2511>', '<r_2512>', '<r_2513>', '<r_2514>', '<r_2515>', '<r_2516>', '<r_2517>', '<r_2518>', '<r_2519>', '<r_2520>', '<r_2521>', '<r_2522>', '<r_2523>', '<r_2524>', '<r_2525>', '<r_2526>', '<r_2527>', '<r_2528>', '<r_2529>', '<r_2530>', '<r_2531>', '<r_2532>', '<r_2533>', '<r_2534>', '<r_2535>', '<r_2536>', '<r_2537>', '<r_2538>', '<r_2539>', '<r_2540>', '<r_2541>', '<r_2542>', '<r_2543>', '<r_2544>', '<r_2545>', '<r_2546>', '<r_2547>', '<r_2548>', '<r_2549>', '<r_2550>', '<r_2551>', '<r_2552>', '<r_2553>', '<r_2554>', '<r_2555>', '<r_2556>', '<r_2557>', '<r_2558>', '<r_2559>', '<r_2560>', '<r_2561>', '<r_2562>', '<r_2563>', '<r_2564>', '<r_2565>', '<r_2566>', '<r_2567>', '<r_2568>', '<r_2569>', '<r_2570>', '<r_2571>', '<r_2572>', '<r_2573>', '<r_2574>', '<r_2575>', '<r_2576>', '<r_2577>', '<r_2578>', '<r_2579>', '<r_2580>', '<r_2581>', '<r_2582>', '<r_2583>', '<r_2584>', '<r_2585>', '<r_2586>', '<r_2587>', '<r_2588>', '<r_2589>', '<r_2590>', '<r_2591>', '<r_2592>', '<r_2593>', '<r_2594>', '<r_2595>', '<r_2596>', '<r_2597>', '<r_2598>', '<r_2599>', '<r_2600>', '<r_2601>', '<r_2602>', '<r_2603>', '<r_2604>', '<r_2605>', '<r_2606>', '<r_2607>', '<r_2608>', '<r_2609>', '<r_2610>', '<r_2611>', '<r_2612>', '<r_2613>', '<r_2614>', '<r_2615>', '<r_2616>', '<r_2617>', '<r_2618>', '<r_2619>', '<r_2620>', '<r_2621>', '<r_2622>', '<r_2623>', '<r_2624>', '<r_2625>', '<r_2626>', '<r_2627>', '<r_2628>', '<r_2629>', '<r_2630>', '<r_2631>', '<r_2632>', '<r_2633>', '<r_2634>', '<r_2635>', '<r_2636>', '<r_2637>', '<r_2638>', '<r_2639>', '<r_2640>', '<r_2641>', '<r_2642>', '<r_2643>', '<r_2644>', '<r_2645>', '<r_2646>', '<r_2647>', '<r_2648>', '<r_2649>', '<r_2650>', '<r_2651>', '<r_2652>', '<r_2653>', '<r_2654>', '<r_2655>', '<r_2656>', '<r_2657>', '<r_2658>', '<r_2659>', '<r_2660>', '<r_2661>', '<r_2662>', '<r_2663>', '<r_2664>', '<r_2665>', '<r_2666>', '<r_2667>', '<r_2668>', '<r_2669>', '<r_2670>', '<r_2671>', '<r_2672>', '<r_2673>', '<r_2674>', '<r_2675>', '<r_2676>', '<r_2677>', '<r_2678>', '<r_2679>', '<r_2680>', '<r_2681>', '<r_2682>', '<r_2683>', '<r_2684>', '<r_2685>', '<r_2686>', '<r_2687>', '<r_2688>', '<r_2689>', '<r_2690>', '<r_2691>', '<r_2692>', '<r_2693>', '<r_2694>', '<r_2695>', '<r_2696>', '<r_2697>', '<r_2698>', '<r_2699>', '<r_2700>', '<r_2701>', '<r_2702>', '<r_2703>', '<r_2704>', '<r_2705>', '<r_2706>', '<r_2707>', '<r_2708>', '<r_2709>', '<r_2710>', '<r_2711>', '<r_2712>', '<r_2713>', '<r_2714>', '<r_2715>', '<r_2716>', '<r_2717>', '<r_2718>', '<r_2719>', '<r_2720>', '<r_2721>', '<r_2722>', '<r_2723>', '<r_2724>', '<r_2725>', '<r_2726>', '<r_2727>', '<r_2728>', '<r_2729>', '<r_2730>', '<r_2731>', '<r_2732>', '<r_2733>', '<r_2734>', '<r_2735>', '<r_2736>', '<r_2737>', '<r_2738>', '<r_2739>', '<r_2740>', '<r_2741>', '<r_2742>', '<r_2743>', '<r_2744>', '<r_2745>', '<r_2746>', '<r_2747>', '<r_2748>', '<r_2749>', '<r_2750>', '<r_2751>', '<r_2752>', '<r_2753>', '<r_2754>', '<r_2755>', '<r_2756>', '<r_2757>', '<r_2758>', '<r_2759>', '<r_2760>', '<r_2761>', '<r_2762>', '<r_2763>', '<r_2764>', '<r_2765>', '<r_2766>', '<r_2767>', '<r_2768>', '<r_2769>', '<r_2770>', '<r_2771>', '<r_2772>', '<r_2773>', '<r_2774>', '<r_2775>', '<r_2776>', '<r_2777>', '<r_2778>', '<r_2779>', '<r_2780>', '<r_2781>', '<r_2782>', '<r_2783>', '<r_2784>', '<r_2785>', '<r_2786>', '<r_2787>', '<r_2788>', '<r_2789>', '<r_2790>', '<r_2791>', '<r_2792>', '<r_2793>', '<r_2794>', '<r_2795>', '<r_2796>', '<r_2797>', '<r_2798>', '<r_2799>', '<r_2800>', '<r_2801>', '<r_2802>', '<r_2803>', '<r_2804>', '<r_2805>', '<r_2806>', '<r_2807>', '<r_2808>', '<r_2809>', '<r_2810>', '<r_2811>', '<r_2812>', '<r_2813>', '<r_2814>', '<r_2815>', '<r_2816>', '<r_2817>', '<r_2818>', '<r_2819>', '<r_2820>', '<r_2821>', '<r_2822>', '<r_2823>', '<r_2824>', '<r_2825>', '<r_2826>', '<r_2827>', '<r_2828>', '<r_2829>', '<r_2830>', '<r_2831>', '<r_2832>', '<r_2833>', '<r_2834>', '<r_2835>', '<r_2836>', '<r_2837>', '<r_2838>', '<r_2839>', '<r_2840>', '<r_2841>', '<r_2842>', '<r_2843>', '<r_2844>', '<r_2845>', '<r_2846>', '<r_2847>', '<r_2848>', '<r_2849>', '<r_2850>', '<r_2851>', '<r_2852>', '<r_2853>', '<r_2854>', '<r_2855>', '<r_2856>', '<r_2857>', '<r_2858>', '<r_2859>', '<r_2860>', '<r_2861>', '<r_2862>', '<r_2863>', '<r_2864>', '<r_2865>', '<r_2866>', '<r_2867>', '<r_2868>', '<r_2869>', '<r_2870>', '<r_2871>', '<r_2872>', '<r_2873>', '<r_2874>', '<r_2875>', '<r_2876>', '<r_2877>', '<r_2878>', '<r_2879>', '<r_2880>', '<r_2881>', '<r_2882>', '<r_2883>', '<r_2884>', '<r_2885>', '<r_2886>', '<r_2887>', '<r_2888>', '<r_2889>', '<r_2890>', '<r_2891>', '<r_2892>', '<r_2893>', '<r_2894>', '<r_2895>', '<r_2896>', '<r_2897>', '<r_2898>', '<r_2899>', '<r_2900>', '<r_2901>', '<r_2902>', '<r_2903>', '<r_2904>', '<r_2905>', '<r_2906>', '<r_2907>', '<r_2908>', '<r_2909>', '<r_2910>', '<r_2911>', '<r_2912>', '<r_2913>', '<r_2914>', '<r_2915>', '<r_2916>', '<r_2917>', '<r_2918>', '<r_2919>', '<r_2920>', '<r_2921>', '<r_2922>', '<r_2923>', '<r_2924>', '<r_2925>', '<r_2926>', '<r_2927>', '<r_2928>', '<r_2929>', '<r_2930>', '<r_2931>', '<r_2932>', '<r_2933>', '<r_2934>', '<r_2935>', '<r_2936>', '<r_2937>', '<r_2938>', '<r_2939>', '<r_2940>', '<r_2941>', '<r_2942>', '<r_2943>', '<r_2944>', '<r_2945>', '<r_2946>', '<r_2947>', '<r_2948>', '<r_2949>', '<r_2950>', '<r_2951>', '<r_2952>', '<r_2953>', '<r_2954>', '<r_2955>', '<r_2956>', '<r_2957>', '<r_2958>', '<r_2959>', '<r_2960>', '<r_2961>', '<r_2962>', '<r_2963>', '<r_2964>', '<r_2965>', '<r_2966>', '<r_2967>', '<r_2968>', '<r_2969>', '<r_2970>', '<r_2971>', '<r_2972>', '<r_2973>', '<r_2974>', '<r_2975>', '<r_2976>', '<r_2977>', '<r_2978>', '<r_2979>', '<r_2980>', '<r_2981>', '<r_2982>', '<r_2983>', '<r_2984>', '<r_2985>', '<r_2986>', '<r_2987>', '<r_2988>', '<r_2989>', '<r_2990>', '<r_2991>', '<r_2992>', '<r_2993>', '<r_2994>', '<r_2995>', '<r_2996>', '<r_2997>', '<r_2998>', '<r_2999>', '<r_3000>', '<r_3001>', '<r_3002>', '<r_3003>', '<r_3004>', '<r_3005>', '<r_3006>', '<r_3007>', '<r_3008>', '<r_3009>', '<r_3010>', '<r_3011>', '<r_3012>', '<r_3013>', '<r_3014>', '<r_3015>', '<r_3016>', '<r_3017>', '<r_3018>', '<r_3019>', '<r_3020>', '<r_3021>', '<r_3022>', '<r_3023>', '<r_3024>', '<r_3025>', '<r_3026>', '<r_3027>', '<r_3028>', '<r_3029>', '<r_3030>', '<r_3031>', '<r_3032>', '<r_3033>', '<r_3034>', '<r_3035>', '<r_3036>', '<r_3037>', '<r_3038>', '<r_3039>', '<r_3040>', '<r_3041>', '<r_3042>', '<r_3043>', '<r_3044>', '<r_3045>', '<r_3046>', '<r_3047>', '<r_3048>', '<r_3049>', '<r_3050>', '<r_3051>', '<r_3052>', '<r_3053>', '<r_3054>', '<r_3055>', '<r_3056>', '<r_3057>', '<r_3058>', '<r_3059>', '<r_3060>', '<r_3061>', '<r_3062>', '<r_3063>', '<r_3064>', '<r_3065>', '<r_3066>', '<r_3067>', '<r_3068>', '<r_3069>', '<r_3070>', '<r_3071>', '<r_3072>', '<r_3073>', '<r_3074>', '<r_3075>', '<r_3076>', '<r_3077>', '<r_3078>', '<r_3079>', '<r_3080>', '<r_3081>', '<r_3082>', '<r_3083>', '<r_3084>', '<r_3085>', '<r_3086>', '<r_3087>', '<r_3088>', '<r_3089>', '<r_3090>', '<r_3091>', '<r_3092>', '<r_3093>', '<r_3094>', '<r_3095>', '<r_3096>', '<r_3097>', '<r_3098>', '<r_3099>', '<r_3100>', '<r_3101>', '<r_3102>', '<r_3103>', '<r_3104>', '<r_3105>', '<r_3106>', '<r_3107>', '<r_3108>', '<r_3109>', '<r_3110>', '<r_3111>', '<r_3112>', '<r_3113>', '<r_3114>', '<r_3115>', '<r_3116>', '<r_3117>', '<r_3118>', '<r_3119>', '<r_3120>', '<r_3121>', '<r_3122>', '<r_3123>', '<r_3124>', '<r_3125>', '<r_3126>', '<r_3127>', '<r_3128>', '<r_3129>', '<r_3130>', '<r_3131>', '<r_3132>', '<r_3133>', '<r_3134>', '<r_3135>', '<r_3136>', '<r_3137>', '<r_3138>', '<r_3139>', '<r_3140>', '<r_3141>', '<r_3142>', '<r_3143>', '<r_3144>', '<r_3145>', '<r_3146>', '<r_3147>', '<r_3148>', '<r_3149>', '<r_3150>', '<r_3151>', '<r_3152>', '<r_3153>', '<r_3154>', '<r_3155>', '<r_3156>', '<r_3157>', '<r_3158>', '<r_3159>', '<r_3160>', '<r_3161>', '<r_3162>', '<r_3163>', '<r_3164>', '<r_3165>', '<r_3166>', '<r_3167>', '<r_3168>', '<r_3169>', '<r_3170>', '<r_3171>', '<r_3172>', '<r_3173>', '<r_3174>', '<r_3175>', '<r_3176>', '<r_3177>', '<r_3178>', '<r_3179>', '<r_3180>', '<r_3181>', '<r_3182>', '<r_3183>', '<r_3184>', '<r_3185>', '<r_3186>', '<r_3187>', '<r_3188>', '<r_3189>', '<r_3190>', '<r_3191>', '<r_3192>', '<r_3193>', '<r_3194>', '<r_3195>', '<r_3196>', '<r_3197>', '<r_3198>', '<r_3199>', '<r_3200>', '<r_3201>', '<r_3202>', '<r_3203>', '<r_3204>', '<r_3205>', '<r_3206>', '<r_3207>', '<r_3208>', '<r_3209>', '<r_3210>', '<r_3211>', '<r_3212>', '<r_3213>', '<r_3214>', '<r_3215>', '<r_3216>', '<r_3217>', '<r_3218>', '<r_3219>', '<r_3220>', '<r_3221>', '<r_3222>', '<r_3223>', '<r_3224>', '<r_3225>', '<r_3226>', '<r_3227>', '<r_3228>', '<r_3229>', '<r_3230>', '<r_3231>', '<r_3232>', '<r_3233>', '<r_3234>', '<r_3235>', '<r_3236>', '<r_3237>', '<r_3238>', '<r_3239>', '<r_3240>', '<r_3241>', '<r_3242>', '<r_3243>', '<r_3244>', '<r_3245>', '<r_3246>', '<r_3247>', '<r_3248>', '<r_3249>', '<r_3250>', '<r_3251>', '<r_3252>', '<r_3253>', '<r_3254>', '<r_3255>', '<r_3256>', '<r_3257>', '<r_3258>', '<r_3259>', '<r_3260>', '<r_3261>', '<r_3262>', '<r_3263>', '<r_3264>', '<r_3265>', '<r_3266>', '<r_3267>', '<r_3268>', '<r_3269>', '<r_3270>', '<r_3271>', '<r_3272>', '<r_3273>', '<r_3274>', '<r_3275>', '<r_3276>', '<r_3277>', '<r_3278>', '<r_3279>', '<r_3280>', '<r_3281>', '<r_3282>', '<r_3283>', '<r_3284>', '<r_3285>', '<r_3286>', '<r_3287>', '<r_3288>', '<r_3289>', '<r_3290>', '<r_3291>', '<r_3292>', '<r_3293>', '<r_3294>', '<r_3295>', '<r_3296>', '<r_3297>', '<r_3298>', '<r_3299>', '<r_3300>', '<r_3301>', '<r_3302>', '<r_3303>', '<r_3304>', '<r_3305>', '<r_3306>', '<r_3307>', '<r_3308>', '<r_3309>', '<r_3310>', '<r_3311>', '<r_3312>', '<r_3313>', '<r_3314>', '<r_3315>', '<r_3316>', '<r_3317>', '<r_3318>', '<r_3319>', '<r_3320>', '<r_3321>', '<r_3322>', '<r_3323>', '<r_3324>', '<r_3325>', '<r_3326>', '<r_3327>', '<r_3328>', '<r_3329>', '<r_3330>', '<r_3331>', '<r_3332>', '<r_3333>', '<r_3334>', '<r_3335>', '<r_3336>', '<r_3337>', '<r_3338>', '<r_3339>', '<r_3340>', '<r_3341>', '<r_3342>', '<r_3343>', '<r_3344>', '<r_3345>', '<r_3346>', '<r_3347>', '<r_3348>', '<r_3349>', '<r_3350>', '<r_3351>', '<r_3352>', '<r_3353>', '<r_3354>', '<r_3355>', '<r_3356>', '<r_3357>', '<r_3358>', '<r_3359>', '<r_3360>', '<r_3361>', '<r_3362>', '<r_3363>', '<r_3364>', '<r_3365>', '<r_3366>', '<r_3367>', '<r_3368>', '<r_3369>', '<r_3370>', '<r_3371>', '<r_3372>', '<r_3373>', '<r_3374>', '<r_3375>', '<r_3376>', '<r_3377>', '<r_3378>', '<r_3379>', '<r_3380>', '<r_3381>', '<r_3382>', '<r_3383>', '<r_3384>', '<r_3385>', '<r_3386>', '<r_3387>', '<r_3388>', '<r_3389>', '<r_3390>', '<r_3391>', '<r_3392>', '<r_3393>', '<r_3394>', '<r_3395>', '<r_3396>', '<r_3397>', '<r_3398>', '<r_3399>', '<r_3400>', '<r_3401>', '<r_3402>', '<r_3403>', '<r_3404>', '<r_3405>', '<r_3406>', '<r_3407>', '<r_3408>', '<r_3409>', '<r_3410>', '<r_3411>', '<r_3412>', '<r_3413>', '<r_3414>', '<r_3415>', '<r_3416>', '<r_3417>', '<r_3418>', '<r_3419>', '<r_3420>', '<r_3421>', '<r_3422>', '<r_3423>', '<r_3424>', '<r_3425>', '<r_3426>', '<r_3427>', '<r_3428>', '<r_3429>', '<r_3430>', '<r_3431>', '<r_3432>', '<r_3433>', '<r_3434>', '<r_3435>', '<r_3436>', '<r_3437>', '<r_3438>', '<r_3439>', '<r_3440>', '<r_3441>', '<r_3442>', '<r_3443>', '<r_3444>', '<r_3445>', '<r_3446>', '<r_3447>', '<r_3448>', '<r_3449>', '<r_3450>', '<r_3451>', '<r_3452>', '<r_3453>', '<r_3454>', '<r_3455>', '<r_3456>', '<r_3457>', '<r_3458>', '<r_3459>', '<r_3460>', '<r_3461>', '<r_3462>', '<r_3463>', '<r_3464>', '<r_3465>', '<r_3466>', '<r_3467>', '<r_3468>', '<r_3469>', '<r_3470>', '<r_3471>', '<r_3472>', '<r_3473>', '<r_3474>', '<r_3475>', '<r_3476>', '<r_3477>', '<r_3478>', '<r_3479>', '<r_3480>', '<r_3481>', '<r_3482>', '<r_3483>', '<r_3484>', '<r_3485>', '<r_3486>', '<r_3487>', '<r_3488>', '<r_3489>', '<r_3490>', '<r_3491>', '<r_3492>', '<r_3493>', '<r_3494>', '<r_3495>', '<r_3496>', '<r_3497>', '<r_3498>', '<r_3499>', '<r_3500>', '<r_3501>', '<r_3502>', '<r_3503>', '<r_3504>', '<r_3505>', '<r_3506>', '<r_3507>', '<r_3508>', '<r_3509>', '<r_3510>', '<r_3511>', '<r_3512>', '<r_3513>', '<r_3514>', '<r_3515>', '<r_3516>', '<r_3517>', '<r_3518>', '<r_3519>', '<r_3520>', '<r_3521>', '<r_3522>', '<r_3523>', '<r_3524>', '<r_3525>', '<r_3526>', '<r_3527>', '<r_3528>', '<r_3529>', '<r_3530>', '<r_3531>', '<r_3532>', '<r_3533>', '<r_3534>', '<r_3535>', '<r_3536>', '<r_3537>', '<r_3538>', '<r_3539>', '<r_3540>', '<r_3541>', '<r_3542>', '<r_3543>', '<r_3544>', '<r_3545>', '<r_3546>', '<r_3547>', '<r_3548>', '<r_3549>', '<r_3550>', '<r_3551>', '<r_3552>', '<r_3553>', '<r_3554>', '<r_3555>', '<r_3556>', '<r_3557>', '<r_3558>', '<r_3559>', '<r_3560>', '<r_3561>', '<r_3562>', '<r_3563>', '<r_3564>', '<r_3565>', '<r_3566>', '<r_3567>', '<r_3568>', '<r_3569>', '<r_3570>', '<r_3571>', '<r_3572>', '<r_3573>', '<r_3574>', '<r_3575>', '<r_3576>', '<r_3577>', '<r_3578>', '<r_3579>', '<r_3580>', '<r_3581>', '<r_3582>', '<r_3583>', '<r_3584>', '<r_3585>', '<r_3586>', '<r_3587>', '<r_3588>', '<r_3589>', '<r_3590>', '<r_3591>', '<r_3592>', '<r_3593>', '<r_3594>', '<r_3595>', '<r_3596>', '<r_3597>', '<r_3598>', '<r_3599>', '<r_3600>', '<r_3601>', '<r_3602>', '<r_3603>', '<r_3604>', '<r_3605>', '<r_3606>', '<r_3607>', '<r_3608>', '<r_3609>', '<r_3610>', '<r_3611>', '<r_3612>', '<r_3613>', '<r_3614>', '<r_3615>', '<r_3616>', '<r_3617>', '<r_3618>', '<r_3619>', '<r_3620>', '<r_3621>', '<r_3622>', '<r_3623>', '<r_3624>', '<r_3625>', '<r_3626>', '<r_3627>', '<r_3628>', '<r_3629>', '<r_3630>', '<r_3631>', '<r_3632>', '<r_3633>', '<r_3634>', '<r_3635>', '<r_3636>', '<r_3637>', '<r_3638>', '<r_3639>', '<r_3640>', '<r_3641>', '<r_3642>', '<r_3643>', '<r_3644>', '<r_3645>', '<r_3646>', '<r_3647>', '<r_3648>', '<r_3649>', '<r_3650>', '<r_3651>', '<r_3652>', '<r_3653>', '<r_3654>', '<r_3655>', '<r_3656>', '<r_3657>', '<r_3658>', '<r_3659>', '<r_3660>', '<r_3661>', '<r_3662>', '<r_3663>', '<r_3664>', '<r_3665>', '<r_3666>', '<r_3667>', '<r_3668>', '<r_3669>', '<r_3670>', '<r_3671>', '<r_3672>', '<r_3673>', '<r_3674>', '<r_3675>', '<r_3676>', '<r_3677>', '<r_3678>', '<r_3679>', '<r_3680>', '<r_3681>', '<r_3682>', '<r_3683>', '<r_3684>', '<r_3685>', '<r_3686>', '<r_3687>', '<r_3688>', '<r_3689>', '<r_3690>', '<r_3691>', '<r_3692>', '<r_3693>', '<r_3694>', '<r_3695>', '<r_3696>', '<r_3697>', '<r_3698>', '<r_3699>', '<r_3700>', '<r_3701>', '<r_3702>', '<r_3703>', '<r_3704>', '<r_3705>', '<r_3706>', '<r_3707>', '<r_3708>', '<r_3709>', '<r_3710>', '<r_3711>', '<r_3712>', '<r_3713>', '<r_3714>', '<r_3715>', '<r_3716>', '<r_3717>', '<r_3718>', '<r_3719>', '<r_3720>', '<r_3721>', '<r_3722>', '<r_3723>', '<r_3724>', '<r_3725>', '<r_3726>', '<r_3727>', '<r_3728>', '<r_3729>', '<r_3730>', '<r_3731>', '<r_3732>', '<r_3733>', '<r_3734>', '<r_3735>', '<r_3736>', '<r_3737>', '<r_3738>', '<r_3739>', '<r_3740>', '<r_3741>', '<r_3742>', '<r_3743>', '<r_3744>', '<r_3745>', '<r_3746>', '<r_3747>', '<r_3748>', '<r_3749>', '<r_3750>', '<r_3751>', '<r_3752>', '<r_3753>', '<r_3754>', '<r_3755>', '<r_3756>', '<r_3757>', '<r_3758>', '<r_3759>', '<r_3760>', '<r_3761>', '<r_3762>', '<r_3763>', '<r_3764>', '<r_3765>', '<r_3766>', '<r_3767>', '<r_3768>', '<r_3769>', '<r_3770>', '<r_3771>', '<r_3772>', '<r_3773>', '<r_3774>', '<r_3775>', '<r_3776>', '<r_3777>', '<r_3778>', '<r_3779>', '<r_3780>', '<r_3781>', '<r_3782>', '<r_3783>', '<r_3784>', '<r_3785>', '<r_3786>', '<r_3787>', '<r_3788>', '<r_3789>', '<r_3790>', '<r_3791>', '<r_3792>', '<r_3793>', '<r_3794>', '<r_3795>', '<r_3796>', '<r_3797>', '<r_3798>', '<r_3799>', '<r_3800>', '<r_3801>', '<r_3802>', '<r_3803>', '<r_3804>', '<r_3805>', '<r_3806>', '<r_3807>', '<r_3808>', '<r_3809>', '<r_3810>', '<r_3811>', '<r_3812>', '<r_3813>', '<r_3814>', '<r_3815>', '<r_3816>', '<r_3817>', '<r_3818>', '<r_3819>', '<r_3820>', '<r_3821>', '<r_3822>', '<r_3823>', '<r_3824>', '<r_3825>', '<r_3826>', '<r_3827>', '<r_3828>', '<r_3829>', '<r_3830>', '<r_3831>', '<r_3832>', '<r_3833>', '<r_3834>', '<r_3835>', '<r_3836>', '<r_3837>', '<r_3838>', '<r_3839>', '<r_3840>', '<r_3841>', '<r_3842>', '<r_3843>', '<r_3844>', '<r_3845>', '<r_3846>', '<r_3847>', '<r_3848>', '<r_3849>', '<r_3850>', '<r_3851>', '<r_3852>', '<r_3853>', '<r_3854>', '<r_3855>', '<r_3856>', '<r_3857>', '<r_3858>', '<r_3859>', '<r_3860>', '<r_3861>', '<r_3862>', '<r_3863>', '<r_3864>', '<r_3865>', '<r_3866>', '<r_3867>', '<r_3868>', '<r_3869>', '<r_3870>', '<r_3871>', '<r_3872>', '<r_3873>', '<r_3874>', '<r_3875>', '<r_3876>', '<r_3877>', '<r_3878>', '<r_3879>', '<r_3880>', '<r_3881>', '<r_3882>', '<r_3883>', '<r_3884>', '<r_3885>', '<r_3886>', '<r_3887>', '<r_3888>', '<r_3889>', '<r_3890>', '<r_3891>', '<r_3892>', '<r_3893>', '<r_3894>', '<r_3895>', '<r_3896>', '<r_3897>', '<r_3898>', '<r_3899>', '<r_3900>', '<r_3901>', '<r_3902>', '<r_3903>', '<r_3904>', '<r_3905>', '<r_3906>', '<r_3907>', '<r_3908>', '<r_3909>', '<r_3910>', '<r_3911>', '<r_3912>', '<r_3913>', '<r_3914>', '<r_3915>', '<r_3916>', '<r_3917>', '<r_3918>', '<r_3919>', '<r_3920>', '<r_3921>', '<r_3922>', '<r_3923>', '<r_3924>', '<r_3925>', '<r_3926>', '<r_3927>', '<r_3928>', '<r_3929>', '<r_3930>', '<r_3931>', '<r_3932>', '<r_3933>', '<r_3934>', '<r_3935>', '<r_3936>', '<r_3937>', '<r_3938>', '<r_3939>', '<r_3940>', '<r_3941>', '<r_3942>', '<r_3943>', '<r_3944>', '<r_3945>', '<r_3946>', '<r_3947>', '<r_3948>', '<r_3949>', '<r_3950>', '<r_3951>', '<r_3952>', '<r_3953>', '<r_3954>', '<r_3955>', '<r_3956>', '<r_3957>', '<r_3958>', '<r_3959>', '<r_3960>', '<r_3961>', '<r_3962>', '<r_3963>', '<r_3964>', '<r_3965>', '<r_3966>', '<r_3967>', '<r_3968>', '<r_3969>', '<r_3970>', '<r_3971>', '<r_3972>', '<r_3973>', '<r_3974>', '<r_3975>', '<r_3976>', '<r_3977>', '<r_3978>', '<r_3979>', '<r_3980>', '<r_3981>', '<r_3982>', '<r_3983>', '<r_3984>', '<r_3985>', '<r_3986>', '<r_3987>', '<r_3988>', '<r_3989>', '<r_3990>', '<r_3991>', '<r_3992>', '<r_3993>', '<r_3994>', '<r_3995>', '<r_3996>', '<r_3997>', '<r_3998>', '<r_3999>', '<r_4000>', '<r_4001>', '<r_4002>', '<r_4003>', '<r_4004>', '<r_4005>', '<r_4006>', '<r_4007>', '<r_4008>', '<r_4009>', '<r_4010>', '<r_4011>', '<r_4012>', '<r_4013>', '<r_4014>', '<r_4015>', '<r_4016>', '<r_4017>', '<r_4018>', '<r_4019>', '<r_4020>', '<r_4021>', '<r_4022>', '<r_4023>', '<r_4024>', '<r_4025>', '<r_4026>', '<r_4027>', '<r_4028>', '<r_4029>', '<r_4030>', '<r_4031>', '<r_4032>', '<r_4033>', '<r_4034>', '<r_4035>', '<r_4036>', '<r_4037>', '<r_4038>', '<r_4039>', '<r_4040>', '<r_4041>', '<r_4042>', '<r_4043>', '<r_4044>', '<r_4045>', '<r_4046>', '<r_4047>', '<r_4048>', '<r_4049>', '<r_4050>', '<r_4051>', '<r_4052>', '<r_4053>', '<r_4054>', '<r_4055>', '<r_4056>', '<r_4057>', '<r_4058>', '<r_4059>', '<r_4060>', '<r_4061>', '<r_4062>', '<r_4063>', '<r_4064>', '<r_4065>', '<r_4066>', '<r_4067>', '<r_4068>', '<r_4069>', '<r_4070>', '<r_4071>', '<r_4072>', '<r_4073>', '<r_4074>', '<r_4075>', '<r_4076>', '<r_4077>', '<r_4078>', '<r_4079>', '<r_4080>', '<r_4081>', '<r_4082>', '<r_4083>', '<r_4084>', '<r_4085>', '<r_4086>', '<r_4087>', '<r_4088>', '<r_4089>', '<r_4090>', '<r_4091>', '<r_4092>', '<r_4093>', '<r_4094>', '<r_4095>', '<r_4096>', '<r_4097>', '<r_4098>', '<r_4099>', '<r_4100>', '<r_4101>', '<r_4102>', '<r_4103>', '<r_4104>', '<r_4105>', '<r_4106>', '<r_4107>', '<r_4108>', '<r_4109>', '<r_4110>', '<r_4111>', '<r_4112>', '<r_4113>', '<r_4114>', '<r_4115>', '<r_4116>', '<r_4117>', '<r_4118>', '<r_4119>', '<r_4120>', '<r_4121>', '<r_4122>', '<r_4123>', '<r_4124>', '<r_4125>', '<r_4126>', '<r_4127>', '<r_4128>', '<r_4129>', '<r_4130>', '<r_4131>', '<r_4132>', '<r_4133>', '<r_4134>', '<r_4135>', '<r_4136>', '<r_4137>', '<r_4138>', '<r_4139>', '<r_4140>', '<r_4141>', '<r_4142>', '<r_4143>', '<r_4144>', '<r_4145>', '<r_4146>', '<r_4147>', '<r_4148>', '<r_4149>', '<r_4150>', '<r_4151>', '<r_4152>', '<r_4153>', '<r_4154>', '<r_4155>', '<r_4156>', '<r_4157>', '<r_4158>', '<r_4159>', '<r_4160>', '<r_4161>', '<r_4162>', '<r_4163>', '<r_4164>', '<r_4165>', '<r_4166>', '<r_4167>', '<r_4168>', '<r_4169>', '<r_4170>', '<r_4171>', '<r_4172>', '<r_4173>', '<r_4174>', '<r_4175>', '<r_4176>', '<r_4177>', '<r_4178>', '<r_4179>', '<r_4180>', '<r_4181>', '<r_4182>', '<r_4183>', '<r_4184>', '<r_4185>', '<r_4186>', '<r_4187>', '<r_4188>', '<r_4189>', '<r_4190>', '<r_4191>', '<r_4192>', '<r_4193>', '<r_4194>', '<r_4195>', '<r_4196>', '<r_4197>', '<r_4198>', '<r_4199>', '<r_4200>', '<r_4201>', '<r_4202>', '<r_4203>', '<r_4204>', '<r_4205>', '<r_4206>', '<r_4207>', '<r_4208>', '<r_4209>', '<r_4210>', '<r_4211>', '<r_4212>', '<r_4213>', '<r_4214>', '<r_4215>', '<r_4216>', '<r_4217>', '<r_4218>', '<r_4219>', '<r_4220>', '<r_4221>', '<r_4222>', '<r_4223>', '<r_4224>', '<r_4225>', '<r_4226>', '<r_4227>', '<r_4228>', '<r_4229>', '<r_4230>', '<r_4231>', '<r_4232>', '<r_4233>', '<r_4234>', '<r_4235>', '<r_4236>', '<r_4237>', '<r_4238>', '<r_4239>', '<r_4240>', '<r_4241>', '<r_4242>', '<r_4243>', '<r_4244>', '<r_4245>', '<r_4246>', '<r_4247>', '<r_4248>', '<r_4249>', '<r_4250>', '<r_4251>', '<r_4252>', '<r_4253>', '<r_4254>', '<r_4255>', '<r_4256>', '<r_4257>', '<r_4258>', '<r_4259>', '<r_4260>', '<r_4261>', '<r_4262>', '<r_4263>', '<r_4264>', '<r_4265>', '<r_4266>', '<r_4267>', '<r_4268>', '<r_4269>', '<r_4270>', '<r_4271>', '<r_4272>', '<r_4273>', '<r_4274>', '<r_4275>', '<r_4276>', '<r_4277>', '<r_4278>', '<r_4279>', '<r_4280>', '<r_4281>', '<r_4282>', '<r_4283>', '<r_4284>', '<r_4285>', '<r_4286>', '<r_4287>', '<r_4288>', '<r_4289>', '<r_4290>', '<r_4291>', '<r_4292>', '<r_4293>', '<r_4294>', '<r_4295>', '<r_4296>', '<r_4297>', '<r_4298>', '<r_4299>', '<r_4300>', '<r_4301>', '<r_4302>', '<r_4303>', '<r_4304>', '<r_4305>', '<r_4306>', '<r_4307>', '<r_4308>', '<r_4309>', '<r_4310>', '<r_4311>', '<r_4312>', '<r_4313>', '<r_4314>', '<r_4315>', '<r_4316>', '<r_4317>', '<r_4318>', '<r_4319>', '<r_4320>', '<r_4321>', '<r_4322>', '<r_4323>', '<r_4324>', '<r_4325>', '<r_4326>', '<r_4327>', '<r_4328>', '<r_4329>', '<r_4330>', '<r_4331>', '<r_4332>', '<r_4333>', '<r_4334>', '<r_4335>', '<r_4336>', '<r_4337>', '<r_4338>', '<r_4339>', '<r_4340>', '<r_4341>', '<r_4342>', '<r_4343>', '<r_4344>', '<r_4345>', '<r_4346>', '<r_4347>', '<r_4348>', '<r_4349>', '<r_4350>', '<r_4351>', '<r_4352>', '<r_4353>', '<r_4354>', '<r_4355>', '<r_4356>', '<r_4357>', '<r_4358>', '<r_4359>', '<r_4360>', '<r_4361>', '<r_4362>', '<r_4363>', '<r_4364>', '<r_4365>', '<r_4366>', '<r_4367>', '<r_4368>', '<r_4369>', '<r_4370>', '<r_4371>', '<r_4372>', '<r_4373>', '<r_4374>', '<r_4375>', '<r_4376>', '<r_4377>', '<r_4378>', '<r_4379>', '<r_4380>', '<r_4381>', '<r_4382>', '<r_4383>', '<r_4384>', '<r_4385>', '<r_4386>', '<r_4387>', '<r_4388>', '<r_4389>', '<r_4390>', '<r_4391>', '<r_4392>', '<r_4393>', '<r_4394>', '<r_4395>', '<r_4396>', '<r_4397>', '<r_4398>', '<r_4399>', '<r_4400>', '<r_4401>', '<r_4402>', '<r_4403>', '<r_4404>', '<r_4405>', '<r_4406>', '<r_4407>', '<r_4408>', '<r_4409>', '<r_4410>', '<r_4411>', '<r_4412>', '<r_4413>', '<r_4414>', '<r_4415>', '<r_4416>', '<r_4417>', '<r_4418>', '<r_4419>', '<r_4420>', '<r_4421>', '<r_4422>', '<r_4423>', '<r_4424>', '<r_4425>', '<r_4426>', '<r_4427>', '<r_4428>', '<r_4429>', '<r_4430>', '<r_4431>', '<r_4432>', '<r_4433>', '<r_4434>', '<r_4435>', '<r_4436>', '<r_4437>', '<r_4438>', '<r_4439>', '<r_4440>', '<r_4441>', '<r_4442>', '<r_4443>', '<r_4444>', '<r_4445>', '<r_4446>', '<r_4447>', '<r_4448>', '<r_4449>', '<r_4450>', '<r_4451>', '<r_4452>', '<r_4453>', '<r_4454>', '<r_4455>', '<r_4456>', '<r_4457>', '<r_4458>', '<r_4459>', '<r_4460>', '<r_4461>', '<r_4462>', '<r_4463>', '<r_4464>', '<r_4465>', '<r_4466>', '<r_4467>', '<r_4468>', '<r_4469>', '<r_4470>', '<r_4471>', '<r_4472>', '<r_4473>', '<r_4474>', '<r_4475>', '<r_4476>', '<r_4477>', '<r_4478>', '<r_4479>', '<r_4480>', '<r_4481>', '<r_4482>', '<r_4483>', '<r_4484>', '<r_4485>', '<r_4486>', '<r_4487>', '<r_4488>', '<r_4489>', '<r_4490>', '<r_4491>', '<r_4492>', '<r_4493>', '<r_4494>', '<r_4495>', '<r_4496>', '<r_4497>', '<r_4498>', '<r_4499>', '<r_4500>', '<r_4501>', '<r_4502>', '<r_4503>', '<r_4504>', '<r_4505>', '<r_4506>', '<r_4507>', '<r_4508>', '<r_4509>', '<r_4510>', '<r_4511>', '<r_4512>', '<r_4513>', '<r_4514>', '<r_4515>', '<r_4516>', '<r_4517>', '<r_4518>', '<r_4519>', '<r_4520>', '<r_4521>', '<r_4522>', '<r_4523>', '<r_4524>', '<r_4525>', '<r_4526>', '<r_4527>', '<r_4528>', '<r_4529>', '<r_4530>', '<r_4531>', '<r_4532>', '<r_4533>', '<r_4534>', '<r_4535>', '<r_4536>', '<r_4537>', '<r_4538>', '<r_4539>', '<r_4540>', '<r_4541>', '<r_4542>', '<r_4543>', '<r_4544>', '<r_4545>', '<r_4546>', '<r_4547>', '<r_4548>', '<r_4549>', '<r_4550>', '<r_4551>', '<r_4552>', '<r_4553>', '<r_4554>', '<r_4555>', '<r_4556>', '<r_4557>', '<r_4558>', '<r_4559>', '<r_4560>', '<r_4561>', '<r_4562>', '<r_4563>', '<r_4564>', '<r_4565>', '<r_4566>', '<r_4567>', '<r_4568>', '<r_4569>', '<r_4570>', '<r_4571>', '<r_4572>', '<r_4573>', '<r_4574>', '<r_4575>', '<r_4576>', '<r_4577>', '<r_4578>', '<r_4579>', '<r_4580>', '<r_4581>', '<r_4582>', '<r_4583>', '<r_4584>', '<r_4585>', '<r_4586>', '<r_4587>', '<r_4588>', '<r_4589>', '<r_4590>', '<r_4591>', '<r_4592>', '<r_4593>', '<r_4594>', '<r_4595>', '<r_4596>', '<r_4597>', '<r_4598>', '<r_4599>', '<r_4600>', '<r_4601>', '<r_4602>', '<r_4603>', '<r_4604>', '<r_4605>', '<r_4606>', '<r_4607>', '<r_4608>', '<r_4609>', '<r_4610>', '<r_4611>', '<r_4612>', '<r_4613>', '<r_4614>', '<r_4615>', '<r_4616>', '<r_4617>', '<r_4618>', '<r_4619>', '<r_4620>', '<r_4621>', '<r_4622>', '<r_4623>', '<r_4624>', '<r_4625>', '<r_4626>', '<r_4627>', '<r_4628>', '<r_4629>', '<r_4630>', '<r_4631>', '<r_4632>', '<r_4633>', '<r_4634>', '<r_4635>', '<r_4636>', '<r_4637>', '<r_4638>', '<r_4639>', '<r_4640>', '<r_4641>', '<r_4642>', '<r_4643>', '<r_4644>', '<r_4645>', '<r_4646>', '<r_4647>', '<r_4648>', '<r_4649>', '<r_4650>', '<r_4651>', '<r_4652>', '<r_4653>', '<r_4654>', '<r_4655>', '<r_4656>', '<r_4657>', '<r_4658>', '<r_4659>', '<r_4660>', '<r_4661>', '<r_4662>', '<r_4663>', '<r_4664>', '<r_4665>', '<r_4666>', '<r_4667>', '<r_4668>', '<r_4669>', '<r_4670>', '<r_4671>', '<r_4672>', '<r_4673>', '<r_4674>', '<r_4675>', '<r_4676>', '<r_4677>', '<r_4678>', '<r_4679>', '<r_4680>', '<r_4681>', '<r_4682>', '<r_4683>', '<r_4684>', '<r_4685>', '<r_4686>', '<r_4687>', '<r_4688>', '<r_4689>', '<r_4690>', '<r_4691>', '<r_4692>', '<r_4693>', '<r_4694>', '<r_4695>', '<r_4696>', '<r_4697>', '<r_4698>', '<r_4699>', '<r_4700>', '<r_4701>', '<r_4702>', '<r_4703>', '<r_4704>', '<r_4705>', '<r_4706>', '<r_4707>', '<r_4708>', '<r_4709>', '<r_4710>', '<r_4711>', '<r_4712>', '<r_4713>', '<r_4714>', '<r_4715>', '<r_4716>', '<r_4717>', '<r_4718>', '<r_4719>', '<r_4720>', '<r_4721>', '<r_4722>', '<r_4723>', '<r_4724>', '<r_4725>', '<r_4726>', '<r_4727>', '<r_4728>', '<r_4729>', '<r_4730>', '<r_4731>', '<r_4732>', '<r_4733>', '<r_4734>', '<r_4735>', '<r_4736>', '<r_4737>', '<r_4738>', '<r_4739>', '<r_4740>', '<r_4741>', '<r_4742>', '<r_4743>', '<r_4744>', '<r_4745>', '<r_4746>', '<r_4747>', '<r_4748>', '<r_4749>', '<r_4750>', '<r_4751>', '<r_4752>', '<r_4753>', '<r_4754>', '<r_4755>', '<r_4756>', '<r_4757>', '<r_4758>', '<r_4759>', '<r_4760>', '<r_4761>', '<r_4762>', '<r_4763>', '<r_4764>', '<r_4765>', '<r_4766>', '<r_4767>', '<r_4768>', '<r_4769>', '<r_4770>', '<r_4771>', '<r_4772>', '<r_4773>', '<r_4774>', '<r_4775>', '<r_4776>', '<r_4777>', '<r_4778>', '<r_4779>', '<r_4780>', '<r_4781>', '<r_4782>', '<r_4783>', '<r_4784>', '<r_4785>', '<r_4786>', '<r_4787>', '<r_4788>', '<r_4789>', '<r_4790>', '<r_4791>', '<r_4792>', '<r_4793>', '<r_4794>', '<r_4795>', '<r_4796>', '<r_4797>', '<r_4798>', '<r_4799>', '<r_4800>', '<r_4801>', '<r_4802>', '<r_4803>', '<r_4804>', '<r_4805>', '<r_4806>', '<r_4807>', '<r_4808>', '<r_4809>', '<r_4810>', '<r_4811>', '<r_4812>', '<r_4813>', '<r_4814>', '<r_4815>', '<r_4816>', '<r_4817>', '<r_4818>', '<r_4819>', '<r_4820>', '<r_4821>', '<r_4822>', '<r_4823>', '<r_4824>', '<r_4825>', '<r_4826>', '<r_4827>', '<r_4828>', '<r_4829>', '<r_4830>', '<r_4831>', '<r_4832>', '<r_4833>', '<r_4834>', '<r_4835>', '<r_4836>', '<r_4837>', '<r_4838>', '<r_4839>', '<r_4840>', '<r_4841>', '<r_4842>', '<r_4843>', '<r_4844>', '<r_4845>', '<r_4846>', '<r_4847>', '<r_4848>', '<r_4849>', '<r_4850>', '<r_4851>', '<r_4852>', '<r_4853>', '<r_4854>', '<r_4855>', '<r_4856>', '<r_4857>', '<r_4858>', '<r_4859>', '<r_4860>', '<r_4861>', '<r_4862>', '<r_4863>', '<r_4864>', '<r_4865>', '<r_4866>', '<r_4867>', '<r_4868>', '<r_4869>', '<r_4870>', '<r_4871>', '<r_4872>', '<r_4873>', '<r_4874>', '<r_4875>', '<r_4876>', '<r_4877>', '<r_4878>', '<r_4879>', '<r_4880>', '<r_4881>', '<r_4882>', '<r_4883>', '<r_4884>', '<r_4885>', '<r_4886>', '<r_4887>', '<r_4888>', '<r_4889>', '<r_4890>', '<r_4891>', '<r_4892>', '<r_4893>', '<r_4894>', '<r_4895>', '<r_4896>', '<r_4897>', '<r_4898>', '<r_4899>', '<r_4900>', '<r_4901>', '<r_4902>', '<r_4903>', '<r_4904>', '<r_4905>', '<r_4906>', '<r_4907>', '<r_4908>', '<r_4909>', '<r_4910>', '<r_4911>', '<r_4912>', '<r_4913>', '<r_4914>', '<r_4915>', '<r_4916>', '<r_4917>', '<r_4918>', '<r_4919>', '<r_4920>', '<r_4921>', '<r_4922>', '<r_4923>', '<r_4924>', '<r_4925>', '<r_4926>', '<r_4927>', '<r_4928>', '<r_4929>', '<r_4930>', '<r_4931>', '<r_4932>', '<r_4933>', '<r_4934>', '<r_4935>', '<r_4936>', '<r_4937>', '<r_4938>', '<r_4939>', '<r_4940>', '<r_4941>', '<r_4942>', '<r_4943>', '<r_4944>', '<r_4945>', '<r_4946>', '<r_4947>', '<r_4948>', '<r_4949>', '<r_4950>', '<r_4951>', '<r_4952>', '<r_4953>', '<r_4954>', '<r_4955>', '<r_4956>', '<r_4957>', '<r_4958>', '<r_4959>', '<r_4960>', '<r_4961>', '<r_4962>', '<r_4963>', '<r_4964>', '<r_4965>', '<r_4966>', '<r_4967>', '<r_4968>', '<r_4969>', '<r_4970>', '<r_4971>', '<r_4972>', '<r_4973>', '<r_4974>', '<r_4975>', '<r_4976>', '<r_4977>', '<r_4978>', '<r_4979>', '<r_4980>', '<r_4981>', '<r_4982>', '<r_4983>', '<r_4984>', '<r_4985>', '<r_4986>', '<r_4987>', '<r_4988>', '<r_4989>', '<r_4990>', '<r_4991>', '<r_4992>', '<r_4993>', '<r_4994>', '<r_4995>', '<r_4996>', '<r_4997>', '<r_4998>', '<r_4999>', '<r_5000>', '<r_5001>', '<r_5002>', '<r_5003>', '<r_5004>', '<r_5005>', '<r_5006>', '<r_5007>', '<r_5008>', '<r_5009>', '<r_5010>', '<r_5011>', '<r_5012>', '<r_5013>', '<r_5014>', '<r_5015>', '<r_5016>', '<r_5017>', '<r_5018>', '<r_5019>', '<r_5020>', '<r_5021>', '<r_5022>', '<r_5023>', '<r_5024>', '<r_5025>', '<r_5026>', '<r_5027>', '<r_5028>', '<r_5029>', '<r_5030>', '<r_5031>', '<r_5032>', '<r_5033>', '<r_5034>', '<r_5035>', '<r_5036>', '<r_5037>', '<r_5038>', '<r_5039>', '<r_5040>', '<r_5041>', '<r_5042>', '<r_5043>', '<r_5044>', '<r_5045>', '<r_5046>', '<r_5047>', '<r_5048>', '<r_5049>', '<r_5050>', '<r_5051>', '<r_5052>', '<r_5053>', '<r_5054>', '<r_5055>', '<r_5056>', '<r_5057>', '<r_5058>', '<r_5059>', '<r_5060>', '<r_5061>', '<r_5062>', '<r_5063>', '<r_5064>', '<r_5065>', '<r_5066>', '<r_5067>', '<r_5068>', '<r_5069>', '<r_5070>', '<r_5071>', '<r_5072>', '<r_5073>', '<r_5074>', '<r_5075>', '<r_5076>', '<r_5077>', '<r_5078>', '<r_5079>', '<r_5080>', '<r_5081>', '<r_5082>', '<r_5083>', '<r_5084>', '<r_5085>', '<r_5086>', '<r_5087>', '<r_5088>', '<r_5089>', '<r_5090>', '<r_5091>', '<r_5092>', '<r_5093>', '<r_5094>', '<r_5095>', '<r_5096>', '<r_5097>', '<r_5098>', '<r_5099>', '<r_5100>', '<r_5101>', '<r_5102>', '<r_5103>', '<r_5104>', '<r_5105>', '<r_5106>', '<r_5107>', '<r_5108>', '<r_5109>', '<r_5110>', '<r_5111>', '<r_5112>', '<r_5113>', '<r_5114>', '<r_5115>', '<r_5116>', '<r_5117>', '<r_5118>', '<r_5119>', '<r_5120>', '<r_5121>', '<r_5122>', '<r_5123>', '<r_5124>', '<r_5125>', '<r_5126>', '<r_5127>', '<r_5128>', '<r_5129>', '<r_5130>', '<r_5131>', '<r_5132>', '<r_5133>', '<r_5134>', '<r_5135>', '<r_5136>', '<r_5137>', '<r_5138>', '<r_5139>', '<r_5140>', '<r_5141>', '<r_5142>', '<r_5143>', '<r_5144>', '<r_5145>', '<r_5146>', '<r_5147>', '<r_5148>', '<r_5149>', '<r_5150>', '<r_5151>', '<r_5152>', '<r_5153>', '<r_5154>', '<r_5155>', '<r_5156>', '<r_5157>', '<r_5158>', '<r_5159>', '<r_5160>', '<r_5161>', '<r_5162>', '<r_5163>', '<r_5164>', '<r_5165>', '<r_5166>', '<r_5167>', '<r_5168>', '<r_5169>', '<r_5170>', '<r_5171>', '<r_5172>', '<r_5173>', '<r_5174>', '<r_5175>', '<r_5176>', '<r_5177>', '<r_5178>', '<r_5179>', '<r_5180>', '<r_5181>', '<r_5182>', '<r_5183>', '<r_5184>', '<r_5185>', '<r_5186>', '<r_5187>', '<r_5188>', '<r_5189>', '<r_5190>', '<r_5191>', '<r_5192>', '<r_5193>', '<r_5194>', '<r_5195>', '<r_5196>', '<r_5197>', '<r_5198>', '<r_5199>', '<r_5200>', '<r_5201>', '<r_5202>', '<r_5203>', '<r_5204>', '<r_5205>', '<r_5206>', '<r_5207>', '<r_5208>', '<r_5209>', '<r_5210>', '<r_5211>', '<r_5212>', '<r_5213>', '<r_5214>', '<r_5215>', '<r_5216>', '<r_5217>', '<r_5218>', '<r_5219>', '<r_5220>', '<r_5221>', '<r_5222>', '<r_5223>', '<r_5224>', '<r_5225>', '<r_5226>', '<r_5227>', '<r_5228>', '<r_5229>', '<r_5230>', '<r_5231>', '<r_5232>', '<r_5233>', '<r_5234>', '<r_5235>', '<r_5236>', '<r_5237>', '<r_5238>', '<r_5239>', '<r_5240>', '<r_5241>', '<r_5242>', '<r_5243>', '<r_5244>', '<r_5245>', '<r_5246>', '<r_5247>', '<r_5248>', '<r_5249>', '<r_5250>', '<r_5251>', '<r_5252>', '<r_5253>', '<r_5254>', '<r_5255>', '<r_5256>', '<r_5257>', '<r_5258>', '<r_5259>', '<r_5260>', '<r_5261>', '<r_5262>', '<r_5263>', '<r_5264>', '<r_5265>', '<r_5266>', '<r_5267>', '<r_5268>', '<r_5269>', '<r_5270>', '<r_5271>', '<r_5272>', '<r_5273>', '<r_5274>', '<r_5275>', '<r_5276>', '<r_5277>', '<r_5278>', '<r_5279>', '<r_5280>', '<r_5281>', '<r_5282>', '<r_5283>', '<r_5284>', '<r_5285>', '<r_5286>', '<r_5287>', '<r_5288>', '<r_5289>', '<r_5290>', '<r_5291>', '<r_5292>', '<r_5293>', '<r_5294>', '<r_5295>', '<r_5296>', '<r_5297>', '<r_5298>', '<r_5299>', '<r_5300>', '<r_5301>', '<r_5302>', '<r_5303>', '<r_5304>', '<r_5305>', '<r_5306>', '<r_5307>', '<r_5308>', '<r_5309>', '<r_5310>', '<r_5311>', '<r_5312>', '<r_5313>', '<r_5314>', '<r_5315>', '<r_5316>', '<r_5317>', '<r_5318>', '<r_5319>', '<r_5320>', '<r_5321>', '<r_5322>', '<r_5323>', '<r_5324>', '<r_5325>', '<r_5326>', '<r_5327>', '<r_5328>', '<r_5329>', '<r_5330>', '<r_5331>', '<r_5332>', '<r_5333>', '<r_5334>', '<r_5335>', '<r_5336>', '<r_5337>', '<r_5338>', '<r_5339>', '<r_5340>', '<r_5341>', '<r_5342>', '<r_5343>', '<r_5344>', '<r_5345>', '<r_5346>', '<r_5347>', '<r_5348>', '<r_5349>', '<r_5350>', '<r_5351>', '<r_5352>', '<r_5353>', '<r_5354>', '<r_5355>', '<r_5356>', '<r_5357>', '<r_5358>', '<r_5359>', '<r_5360>', '<r_5361>', '<r_5362>', '<r_5363>', '<r_5364>', '<r_5365>', '<r_5366>', '<r_5367>', '<r_5368>', '<r_5369>', '<r_5370>', '<r_5371>', '<r_5372>', '<r_5373>', '<r_5374>', '<r_5375>', '<r_5376>', '<r_5377>', '<r_5378>', '<r_5379>', '<r_5380>', '<r_5381>', '<r_5382>', '<r_5383>', '<r_5384>', '<r_5385>', '<r_5386>', '<r_5387>', '<r_5388>', '<r_5389>', '<r_5390>', '<r_5391>', '<r_5392>', '<r_5393>', '<r_5394>', '<r_5395>', '<r_5396>', '<r_5397>', '<r_5398>', '<r_5399>', '<r_5400>', '<r_5401>', '<r_5402>', '<r_5403>', '<r_5404>', '<r_5405>', '<r_5406>', '<r_5407>', '<r_5408>', '<r_5409>', '<r_5410>', '<r_5411>', '<r_5412>', '<r_5413>', '<r_5414>', '<r_5415>', '<r_5416>', '<r_5417>', '<r_5418>', '<r_5419>', '<r_5420>', '<r_5421>', '<r_5422>', '<r_5423>', '<r_5424>', '<r_5425>', '<r_5426>', '<r_5427>', '<r_5428>', '<r_5429>', '<r_5430>', '<r_5431>', '<r_5432>', '<r_5433>', '<r_5434>', '<r_5435>', '<r_5436>', '<r_5437>', '<r_5438>', '<r_5439>', '<r_5440>', '<r_5441>', '<r_5442>', '<r_5443>', '<r_5444>', '<r_5445>', '<r_5446>', '<r_5447>', '<r_5448>', '<r_5449>', '<r_5450>', '<r_5451>', '<r_5452>', '<r_5453>', '<r_5454>', '<r_5455>', '<r_5456>', '<r_5457>', '<r_5458>', '<r_5459>', '<r_5460>', '<r_5461>', '<r_5462>', '<r_5463>', '<r_5464>', '<r_5465>', '<r_5466>', '<r_5467>', '<r_5468>', '<r_5469>', '<r_5470>', '<r_5471>', '<r_5472>', '<r_5473>', '<r_5474>', '<r_5475>', '<r_5476>', '<r_5477>', '<r_5478>', '<r_5479>', '<r_5480>', '<r_5481>', '<r_5482>', '<r_5483>', '<r_5484>', '<r_5485>', '<r_5486>', '<r_5487>', '<r_5488>', '<r_5489>', '<r_5490>', '<r_5491>', '<r_5492>', '<r_5493>', '<r_5494>', '<r_5495>', '<r_5496>', '<r_5497>', '<r_5498>', '<r_5499>', '<r_5500>', '<r_5501>', '<r_5502>', '<r_5503>', '<r_5504>', '<r_5505>', '<r_5506>', '<r_5507>', '<r_5508>', '<r_5509>', '<r_5510>', '<r_5511>', '<r_5512>', '<r_5513>', '<r_5514>', '<r_5515>', '<r_5516>', '<r_5517>', '<r_5518>', '<r_5519>', '<r_5520>', '<r_5521>', '<r_5522>', '<r_5523>', '<r_5524>', '<r_5525>', '<r_5526>', '<r_5527>', '<r_5528>', '<r_5529>', '<r_5530>', '<r_5531>', '<r_5532>', '<r_5533>', '<r_5534>', '<r_5535>', '<r_5536>', '<r_5537>', '<r_5538>', '<r_5539>', '<r_5540>', '<r_5541>', '<r_5542>', '<r_5543>', '<r_5544>', '<r_5545>', '<r_5546>', '<r_5547>', '<r_5548>', '<r_5549>', '<r_5550>', '<r_5551>', '<r_5552>', '<r_5553>', '<r_5554>', '<r_5555>', '<r_5556>', '<r_5557>', '<r_5558>', '<r_5559>', '<r_5560>', '<r_5561>', '<r_5562>', '<r_5563>', '<r_5564>', '<r_5565>', '<r_5566>', '<r_5567>', '<r_5568>', '<r_5569>', '<r_5570>', '<r_5571>', '<r_5572>', '<r_5573>', '<r_5574>', '<r_5575>', '<r_5576>', '<r_5577>', '<r_5578>', '<r_5579>', '<r_5580>', '<r_5581>', '<r_5582>', '<r_5583>', '<r_5584>', '<r_5585>', '<r_5586>', '<r_5587>', '<r_5588>', '<r_5589>', '<r_5590>', '<r_5591>', '<r_5592>', '<r_5593>', '<r_5594>', '<r_5595>', '<r_5596>', '<r_5597>', '<r_5598>', '<r_5599>', '<r_5600>', '<r_5601>', '<r_5602>', '<r_5603>', '<r_5604>', '<r_5605>', '<r_5606>', '<r_5607>', '<r_5608>', '<r_5609>', '<r_5610>', '<r_5611>', '<r_5612>', '<r_5613>', '<r_5614>', '<r_5615>', '<r_5616>', '<r_5617>', '<r_5618>', '<r_5619>', '<r_5620>', '<r_5621>', '<r_5622>', '<r_5623>', '<r_5624>', '<r_5625>', '<r_5626>', '<r_5627>', '<r_5628>', '<r_5629>', '<r_5630>', '<r_5631>', '<r_5632>', '<r_5633>', '<r_5634>', '<r_5635>', '<r_5636>', '<r_5637>', '<r_5638>', '<r_5639>', '<r_5640>', '<r_5641>', '<r_5642>', '<r_5643>', '<r_5644>', '<r_5645>', '<r_5646>', '<r_5647>', '<r_5648>', '<r_5649>', '<r_5650>', '<r_5651>', '<r_5652>', '<r_5653>', '<r_5654>', '<r_5655>', '<r_5656>', '<r_5657>', '<r_5658>', '<r_5659>', '<r_5660>', '<r_5661>', '<r_5662>', '<r_5663>', '<r_5664>', '<r_5665>', '<r_5666>', '<r_5667>', '<r_5668>', '<r_5669>', '<r_5670>', '<r_5671>', '<r_5672>', '<r_5673>', '<r_5674>', '<r_5675>', '<r_5676>', '<r_5677>', '<r_5678>', '<r_5679>', '<r_5680>', '<r_5681>', '<r_5682>', '<r_5683>', '<r_5684>', '<r_5685>', '<r_5686>', '<r_5687>', '<r_5688>', '<r_5689>', '<r_5690>', '<r_5691>', '<r_5692>', '<r_5693>', '<r_5694>', '<r_5695>', '<r_5696>', '<r_5697>', '<r_5698>', '<r_5699>', '<r_5700>', '<r_5701>', '<r_5702>', '<r_5703>', '<r_5704>', '<r_5705>', '<r_5706>', '<r_5707>', '<r_5708>', '<r_5709>', '<r_5710>', '<r_5711>', '<r_5712>', '<r_5713>', '<r_5714>', '<r_5715>', '<r_5716>', '<r_5717>', '<r_5718>', '<r_5719>', '<r_5720>', '<r_5721>', '<r_5722>', '<r_5723>', '<r_5724>', '<r_5725>', '<r_5726>', '<r_5727>', '<r_5728>', '<r_5729>', '<r_5730>', '<r_5731>', '<r_5732>', '<r_5733>', '<r_5734>', '<r_5735>', '<r_5736>', '<r_5737>', '<r_5738>', '<r_5739>', '<r_5740>', '<r_5741>', '<r_5742>', '<r_5743>', '<r_5744>', '<r_5745>', '<r_5746>', '<r_5747>', '<r_5748>', '<r_5749>', '<r_5750>', '<r_5751>', '<r_5752>', '<r_5753>', '<r_5754>', '<r_5755>', '<r_5756>', '<r_5757>', '<r_5758>', '<r_5759>', '<r_5760>', '<r_5761>', '<r_5762>', '<r_5763>', '<r_5764>', '<r_5765>', '<r_5766>', '<r_5767>', '<r_5768>', '<r_5769>', '<r_5770>', '<r_5771>', '<r_5772>', '<r_5773>', '<r_5774>', '<r_5775>', '<r_5776>', '<r_5777>', '<r_5778>', '<r_5779>', '<r_5780>', '<r_5781>', '<r_5782>', '<r_5783>', '<r_5784>', '<r_5785>', '<r_5786>', '<r_5787>', '<r_5788>', '<r_5789>', '<r_5790>', '<r_5791>', '<r_5792>', '<r_5793>', '<r_5794>', '<r_5795>', '<r_5796>', '<r_5797>', '<r_5798>', '<r_5799>', '<r_5800>', '<r_5801>', '<r_5802>', '<r_5803>', '<r_5804>', '<r_5805>', '<r_5806>', '<r_5807>', '<r_5808>', '<r_5809>', '<r_5810>', '<r_5811>', '<r_5812>', '<r_5813>', '<r_5814>', '<r_5815>', '<r_5816>', '<r_5817>', '<r_5818>', '<r_5819>', '<r_5820>', '<r_5821>', '<r_5822>', '<r_5823>', '<r_5824>', '<r_5825>', '<r_5826>', '<r_5827>', '<r_5828>', '<r_5829>', '<r_5830>', '<r_5831>', '<r_5832>', '<r_5833>', '<r_5834>', '<r_5835>', '<r_5836>', '<r_5837>', '<r_5838>', '<r_5839>', '<r_5840>', '<r_5841>', '<r_5842>', '<r_5843>', '<r_5844>', '<r_5845>', '<r_5846>', '<r_5847>', '<r_5848>', '<r_5849>', '<r_5850>', '<r_5851>', '<r_5852>', '<r_5853>', '<r_5854>', '<r_5855>', '<r_5856>', '<r_5857>', '<r_5858>', '<r_5859>', '<r_5860>', '<r_5861>', '<r_5862>', '<r_5863>', '<r_5864>', '<r_5865>', '<r_5866>', '<r_5867>', '<r_5868>', '<r_5869>', '<r_5870>', '<r_5871>', '<r_5872>', '<r_5873>', '<r_5874>', '<r_5875>', '<r_5876>', '<r_5877>', '<r_5878>', '<r_5879>', '<r_5880>', '<r_5881>', '<r_5882>', '<r_5883>', '<r_5884>', '<r_5885>', '<r_5886>', '<r_5887>', '<r_5888>', '<r_5889>', '<r_5890>', '<r_5891>', '<r_5892>', '<r_5893>', '<r_5894>', '<r_5895>', '<r_5896>', '<r_5897>', '<r_5898>', '<r_5899>', '<r_5900>', '<r_5901>', '<r_5902>', '<r_5903>', '<r_5904>', '<r_5905>', '<r_5906>', '<r_5907>', '<r_5908>', '<r_5909>', '<r_5910>', '<r_5911>', '<r_5912>', '<r_5913>', '<r_5914>', '<r_5915>', '<r_5916>', '<r_5917>', '<r_5918>', '<r_5919>', '<r_5920>', '<r_5921>', '<r_5922>', '<r_5923>', '<r_5924>', '<r_5925>', '<r_5926>', '<r_5927>', '<r_5928>', '<r_5929>', '<r_5930>', '<r_5931>', '<r_5932>', '<r_5933>', '<r_5934>', '<r_5935>', '<r_5936>', '<r_5937>', '<r_5938>', '<r_5939>', '<r_5940>', '<r_5941>', '<r_5942>', '<r_5943>', '<r_5944>', '<r_5945>', '<r_5946>', '<r_5947>', '<r_5948>', '<r_5949>', '<r_5950>', '<r_5951>', '<r_5952>', '<r_5953>', '<r_5954>', '<r_5955>', '<r_5956>', '<r_5957>', '<r_5958>', '<r_5959>', '<r_5960>', '<r_5961>', '<r_5962>', '<r_5963>', '<r_5964>', '<r_5965>', '<r_5966>', '<r_5967>', '<r_5968>', '<r_5969>', '<r_5970>', '<r_5971>', '<r_5972>', '<r_5973>', '<r_5974>', '<r_5975>', '<r_5976>', '<r_5977>', '<r_5978>', '<r_5979>', '<r_5980>', '<r_5981>', '<r_5982>', '<r_5983>', '<r_5984>', '<r_5985>', '<r_5986>', '<r_5987>', '<r_5988>', '<r_5989>', '<r_5990>', '<r_5991>', '<r_5992>', '<r_5993>', '<r_5994>', '<r_5995>', '<r_5996>', '<r_5997>', '<r_5998>', '<r_5999>', '<r_6000>', '<r_6001>', '<r_6002>', '<r_6003>', '<r_6004>', '<r_6005>', '<r_6006>', '<r_6007>', '<r_6008>', '<r_6009>', '<r_6010>', '<r_6011>', '<r_6012>', '<r_6013>', '<r_6014>', '<r_6015>', '<r_6016>', '<r_6017>', '<r_6018>', '<r_6019>', '<r_6020>', '<r_6021>', '<r_6022>', '<r_6023>', '<r_6024>', '<r_6025>', '<r_6026>', '<r_6027>', '<r_6028>', '<r_6029>', '<r_6030>', '<r_6031>', '<r_6032>', '<r_6033>', '<r_6034>', '<r_6035>', '<r_6036>', '<r_6037>', '<r_6038>', '<r_6039>', '<r_6040>', '<r_6041>', '<r_6042>', '<r_6043>', '<r_6044>', '<r_6045>', '<r_6046>', '<r_6047>', '<r_6048>', '<r_6049>', '<r_6050>', '<r_6051>', '<r_6052>', '<r_6053>', '<r_6054>', '<r_6055>', '<r_6056>', '<r_6057>', '<r_6058>', '<r_6059>', '<r_6060>', '<r_6061>', '<r_6062>', '<r_6063>', '<r_6064>', '<r_6065>', '<r_6066>', '<r_6067>', '<r_6068>', '<r_6069>', '<r_6070>', '<r_6071>', '<r_6072>', '<r_6073>', '<r_6074>', '<r_6075>', '<r_6076>', '<r_6077>', '<r_6078>', '<r_6079>', '<r_6080>', '<r_6081>', '<r_6082>', '<r_6083>', '<r_6084>', '<r_6085>', '<r_6086>', '<r_6087>', '<r_6088>', '<r_6089>', '<r_6090>', '<r_6091>', '<r_6092>', '<r_6093>', '<r_6094>', '<r_6095>', '<r_6096>', '<r_6097>', '<r_6098>', '<r_6099>', '<r_6100>', '<r_6101>', '<r_6102>', '<r_6103>', '<r_6104>', '<r_6105>', '<r_6106>', '<r_6107>', '<r_6108>', '<r_6109>', '<r_6110>', '<r_6111>', '<r_6112>', '<r_6113>', '<r_6114>', '<r_6115>', '<r_6116>', '<r_6117>', '<r_6118>', '<r_6119>', '<r_6120>', '<r_6121>', '<r_6122>', '<r_6123>', '<r_6124>', '<r_6125>', '<r_6126>', '<r_6127>', '<r_6128>', '<r_6129>', '<r_6130>', '<r_6131>', '<r_6132>', '<r_6133>', '<r_6134>', '<r_6135>', '<r_6136>', '<r_6137>', '<r_6138>', '<r_6139>', '<r_6140>', '<r_6141>', '<r_6142>', '<r_6143>', '<r_6144>', '<r_6145>', '<r_6146>', '<r_6147>', '<r_6148>', '<r_6149>', '<r_6150>', '<r_6151>', '<r_6152>', '<r_6153>', '<r_6154>', '<r_6155>', '<r_6156>', '<r_6157>', '<r_6158>', '<r_6159>', '<r_6160>', '<r_6161>', '<r_6162>', '<r_6163>', '<r_6164>', '<r_6165>', '<r_6166>', '<r_6167>', '<r_6168>', '<r_6169>', '<r_6170>', '<r_6171>', '<r_6172>', '<r_6173>', '<r_6174>', '<r_6175>', '<r_6176>', '<r_6177>', '<r_6178>', '<r_6179>', '<r_6180>', '<r_6181>', '<r_6182>', '<r_6183>', '<r_6184>', '<r_6185>', '<r_6186>', '<r_6187>', '<r_6188>', '<r_6189>', '<r_6190>', '<r_6191>', '<r_6192>', '<r_6193>', '<r_6194>', '<r_6195>', '<r_6196>', '<r_6197>', '<r_6198>', '<r_6199>', '<r_6200>', '<r_6201>', '<r_6202>', '<r_6203>', '<r_6204>', '<r_6205>', '<r_6206>', '<r_6207>', '<r_6208>', '<r_6209>', '<r_6210>', '<r_6211>', '<r_6212>', '<r_6213>', '<r_6214>', '<r_6215>', '<r_6216>', '<r_6217>', '<r_6218>', '<r_6219>', '<r_6220>', '<r_6221>', '<r_6222>', '<r_6223>', '<r_6224>', '<r_6225>', '<r_6226>', '<r_6227>', '<r_6228>', '<r_6229>', '<r_6230>', '<r_6231>', '<r_6232>', '<r_6233>', '<r_6234>', '<r_6235>', '<r_6236>', '<r_6237>', '<r_6238>', '<r_6239>', '<r_6240>', '<r_6241>', '<r_6242>', '<r_6243>', '<r_6244>', '<r_6245>', '<r_6246>', '<r_6247>', '<r_6248>', '<r_6249>', '<r_6250>', '<r_6251>', '<r_6252>', '<r_6253>', '<r_6254>', '<r_6255>', '<r_6256>', '<r_6257>', '<r_6258>', '<r_6259>', '<r_6260>', '<r_6261>', '<r_6262>', '<r_6263>', '<r_6264>', '<r_6265>', '<r_6266>', '<r_6267>', '<r_6268>', '<r_6269>', '<r_6270>', '<r_6271>', '<r_6272>', '<r_6273>', '<r_6274>', '<r_6275>', '<r_6276>', '<r_6277>', '<r_6278>', '<r_6279>', '<r_6280>', '<r_6281>', '<r_6282>', '<r_6283>', '<r_6284>', '<r_6285>', '<r_6286>', '<r_6287>', '<r_6288>', '<r_6289>', '<r_6290>', '<r_6291>', '<r_6292>', '<r_6293>', '<r_6294>', '<r_6295>', '<r_6296>', '<r_6297>', '<r_6298>', '<r_6299>', '<r_6300>', '<r_6301>', '<r_6302>', '<r_6303>', '<r_6304>', '<r_6305>', '<r_6306>', '<r_6307>', '<r_6308>', '<r_6309>', '<r_6310>', '<r_6311>', '<r_6312>', '<r_6313>', '<r_6314>', '<r_6315>', '<r_6316>', '<r_6317>', '<r_6318>', '<r_6319>', '<r_6320>', '<r_6321>', '<r_6322>', '<r_6323>', '<r_6324>', '<r_6325>', '<r_6326>', '<r_6327>', '<r_6328>', '<r_6329>', '<r_6330>', '<r_6331>', '<r_6332>', '<r_6333>', '<r_6334>', '<r_6335>', '<r_6336>', '<r_6337>', '<r_6338>', '<r_6339>', '<r_6340>', '<r_6341>', '<r_6342>', '<r_6343>', '<r_6344>', '<r_6345>', '<r_6346>', '<r_6347>', '<r_6348>', '<r_6349>', '<r_6350>', '<r_6351>', '<r_6352>', '<r_6353>', '<r_6354>', '<r_6355>', '<r_6356>', '<r_6357>', '<r_6358>', '<r_6359>', '<r_6360>', '<r_6361>', '<r_6362>', '<r_6363>', '<r_6364>', '<r_6365>', '<r_6366>', '<r_6367>', '<r_6368>', '<r_6369>', '<r_6370>', '<r_6371>', '<r_6372>', '<r_6373>', '<r_6374>', '<r_6375>', '<r_6376>', '<r_6377>', '<r_6378>', '<r_6379>', '<r_6380>', '<r_6381>', '<r_6382>', '<r_6383>', '<r_6384>', '<r_6385>', '<r_6386>', '<r_6387>', '<r_6388>', '<r_6389>', '<r_6390>', '<r_6391>', '<r_6392>', '<r_6393>', '<r_6394>', '<r_6395>', '<r_6396>', '<r_6397>', '<r_6398>', '<r_6399>', '<r_6400>', '<r_6401>', '<r_6402>', '<r_6403>', '<r_6404>', '<r_6405>', '<r_6406>', '<r_6407>', '<r_6408>', '<r_6409>', '<r_6410>', '<r_6411>', '<r_6412>', '<r_6413>', '<r_6414>', '<r_6415>', '<r_6416>', '<r_6417>', '<r_6418>', '<r_6419>', '<r_6420>', '<r_6421>', '<r_6422>', '<r_6423>', '<r_6424>', '<r_6425>', '<r_6426>', '<r_6427>', '<r_6428>', '<r_6429>', '<r_6430>', '<r_6431>', '<r_6432>', '<r_6433>', '<r_6434>', '<r_6435>', '<r_6436>', '<r_6437>', '<r_6438>', '<r_6439>', '<r_6440>', '<r_6441>', '<r_6442>', '<r_6443>', '<r_6444>', '<r_6445>', '<r_6446>', '<r_6447>', '<r_6448>', '<r_6449>', '<r_6450>', '<r_6451>', '<r_6452>', '<r_6453>', '<r_6454>', '<r_6455>', '<r_6456>', '<r_6457>', '<r_6458>', '<r_6459>', '<r_6460>', '<r_6461>', '<r_6462>', '<r_6463>', '<r_6464>', '<r_6465>', '<r_6466>', '<r_6467>', '<r_6468>', '<r_6469>', '<r_6470>', '<r_6471>', '<r_6472>', '<r_6473>', '<r_6474>', '<r_6475>', '<r_6476>', '<r_6477>', '<r_6478>', '<r_6479>', '<r_6480>', '<r_6481>', '<r_6482>', '<r_6483>', '<r_6484>', '<r_6485>', '<r_6486>', '<r_6487>', '<r_6488>', '<r_6489>', '<r_6490>', '<r_6491>', '<r_6492>', '<r_6493>', '<r_6494>', '<r_6495>', '<r_6496>', '<r_6497>', '<r_6498>', '<r_6499>', '<r_6500>', '<r_6501>', '<r_6502>', '<r_6503>', '<r_6504>', '<r_6505>', '<r_6506>', '<r_6507>', '<r_6508>', '<r_6509>', '<r_6510>', '<r_6511>', '<r_6512>', '<r_6513>', '<r_6514>', '<r_6515>', '<r_6516>', '<r_6517>', '<r_6518>', '<r_6519>', '<r_6520>', '<r_6521>', '<r_6522>', '<r_6523>', '<r_6524>', '<r_6525>', '<r_6526>', '<r_6527>', '<r_6528>', '<r_6529>', '<r_6530>', '<r_6531>', '<r_6532>', '<r_6533>', '<r_6534>', '<r_6535>', '<r_6536>', '<r_6537>', '<r_6538>', '<r_6539>', '<r_6540>', '<r_6541>', '<r_6542>', '<r_6543>', '<r_6544>', '<r_6545>', '<r_6546>', '<r_6547>', '<r_6548>', '<r_6549>', '<r_6550>', '<r_6551>', '<r_6552>', '<r_6553>', '<r_6554>', '<r_6555>', '<r_6556>', '<r_6557>', '<r_6558>', '<r_6559>', '<r_6560>', '<r_6561>', '<r_6562>', '<r_6563>', '<r_6564>', '<r_6565>', '<r_6566>', '<r_6567>', '<r_6568>', '<r_6569>', '<r_6570>', '<r_6571>', '<r_6572>', '<r_6573>', '<r_6574>', '<r_6575>', '<r_6576>', '<r_6577>', '<r_6578>', '<r_6579>', '<r_6580>', '<r_6581>', '<r_6582>', '<r_6583>', '<r_6584>', '<r_6585>', '<r_6586>', '<r_6587>', '<r_6588>', '<r_6589>', '<r_6590>', '<r_6591>', '<r_6592>', '<r_6593>', '<r_6594>', '<r_6595>', '<r_6596>', '<r_6597>', '<r_6598>', '<r_6599>', '<r_6600>', '<r_6601>', '<r_6602>', '<r_6603>', '<r_6604>', '<r_6605>', '<r_6606>', '<r_6607>', '<r_6608>', '<r_6609>', '<r_6610>', '<r_6611>', '<r_6612>', '<r_6613>', '<r_6614>', '<r_6615>', '<r_6616>', '<r_6617>', '<r_6618>', '<r_6619>', '<r_6620>', '<r_6621>', '<r_6622>', '<r_6623>', '<r_6624>', '<r_6625>', '<r_6626>', '<r_6627>', '<r_6628>', '<r_6629>', '<r_6630>', '<r_6631>', '<r_6632>', '<r_6633>', '<r_6634>', '<r_6635>', '<r_6636>', '<r_6637>', '<r_6638>', '<r_6639>', '<r_6640>', '<r_6641>', '<r_6642>', '<r_6643>', '<r_6644>', '<r_6645>', '<r_6646>', '<r_6647>', '<r_6648>', '<r_6649>', '<r_6650>', '<r_6651>', '<r_6652>', '<r_6653>', '<r_6654>', '<r_6655>', '<r_6656>', '<r_6657>', '<r_6658>', '<r_6659>', '<r_6660>', '<r_6661>', '<r_6662>', '<r_6663>', '<r_6664>', '<r_6665>', '<r_6666>', '<r_6667>', '<r_6668>', '<r_6669>', '<r_6670>', '<r_6671>', '<r_6672>', '<r_6673>', '<r_6674>', '<r_6675>', '<r_6676>', '<r_6677>', '<r_6678>', '<r_6679>', '<r_6680>', '<r_6681>', '<r_6682>', '<r_6683>', '<r_6684>', '<r_6685>', '<r_6686>', '<r_6687>', '<r_6688>', '<r_6689>', '<r_6690>', '<r_6691>', '<r_6692>', '<r_6693>', '<r_6694>', '<r_6695>', '<r_6696>', '<r_6697>', '<r_6698>', '<r_6699>', '<r_6700>', '<r_6701>', '<r_6702>', '<r_6703>', '<r_6704>', '<r_6705>', '<r_6706>', '<r_6707>', '<r_6708>', '<r_6709>', '<r_6710>', '<r_6711>', '<r_6712>', '<r_6713>', '<r_6714>', '<r_6715>', '<r_6716>', '<r_6717>', '<r_6718>', '<r_6719>', '<r_6720>', '<r_6721>', '<r_6722>', '<r_6723>', '<r_6724>', '<r_6725>', '<r_6726>', '<r_6727>', '<r_6728>', '<r_6729>', '<r_6730>', '<r_6731>', '<r_6732>', '<r_6733>', '<r_6734>', '<r_6735>', '<r_6736>', '<r_6737>', '<r_6738>', '<r_6739>', '<r_6740>', '<r_6741>', '<r_6742>', '<r_6743>', '<r_6744>', '<r_6745>', '<r_6746>', '<r_6747>', '<r_6748>', '<r_6749>', '<r_6750>', '<r_6751>', '<r_6752>', '<r_6753>', '<r_6754>', '<r_6755>', '<r_6756>', '<r_6757>', '<r_6758>', '<r_6759>', '<r_6760>', '<r_6761>', '<r_6762>', '<r_6763>', '<r_6764>', '<r_6765>', '<r_6766>', '<r_6767>', '<r_6768>', '<r_6769>', '<r_6770>', '<r_6771>', '<r_6772>', '<r_6773>', '<r_6774>', '<r_6775>', '<r_6776>', '<r_6777>', '<r_6778>', '<r_6779>', '<r_6780>', '<r_6781>', '<r_6782>', '<r_6783>', '<r_6784>', '<r_6785>', '<r_6786>', '<r_6787>', '<r_6788>', '<r_6789>', '<r_6790>', '<r_6791>', '<r_6792>', '<r_6793>', '<r_6794>', '<r_6795>', '<r_6796>', '<r_6797>', '<r_6798>', '<r_6799>', '<r_6800>', '<r_6801>', '<r_6802>', '<r_6803>', '<r_6804>', '<r_6805>', '<r_6806>', '<r_6807>', '<r_6808>', '<r_6809>', '<r_6810>', '<r_6811>', '<r_6812>', '<r_6813>', '<r_6814>', '<r_6815>', '<r_6816>', '<r_6817>', '<r_6818>', '<r_6819>', '<r_6820>', '<r_6821>', '<r_6822>', '<r_6823>', '<r_6824>', '<r_6825>', '<r_6826>', '<r_6827>', '<r_6828>', '<r_6829>', '<r_6830>', '<r_6831>', '<r_6832>', '<r_6833>', '<r_6834>', '<r_6835>', '<r_6836>', '<r_6837>', '<r_6838>', '<r_6839>', '<r_6840>', '<r_6841>', '<r_6842>', '<r_6843>', '<r_6844>', '<r_6845>', '<r_6846>', '<r_6847>', '<r_6848>', '<r_6849>', '<r_6850>', '<r_6851>', '<r_6852>', '<r_6853>', '<r_6854>', '<r_6855>', '<r_6856>', '<r_6857>', '<r_6858>', '<r_6859>', '<r_6860>', '<r_6861>', '<r_6862>', '<r_6863>', '<r_6864>', '<r_6865>', '<r_6866>', '<r_6867>', '<r_6868>', '<r_6869>', '<r_6870>', '<r_6871>', '<r_6872>', '<r_6873>', '<r_6874>', '<r_6875>', '<r_6876>', '<r_6877>', '<r_6878>', '<r_6879>', '<r_6880>', '<r_6881>', '<r_6882>', '<r_6883>', '<r_6884>', '<r_6885>', '<r_6886>', '<r_6887>', '<r_6888>', '<r_6889>', '<r_6890>', '<r_6891>', '<r_6892>', '<r_6893>', '<r_6894>', '<r_6895>', '<r_6896>', '<r_6897>', '<r_6898>', '<r_6899>', '<r_6900>', '<r_6901>', '<r_6902>', '<r_6903>', '<r_6904>', '<r_6905>', '<r_6906>', '<r_6907>', '<r_6908>', '<r_6909>', '<r_6910>', '<r_6911>', '<r_6912>', '<r_6913>', '<r_6914>', '<r_6915>', '<r_6916>', '<r_6917>', '<r_6918>', '<r_6919>', '<r_6920>', '<r_6921>', '<r_6922>', '<r_6923>', '<r_6924>', '<r_6925>', '<r_6926>', '<r_6927>', '<r_6928>', '<r_6929>', '<r_6930>', '<r_6931>', '<r_6932>', '<r_6933>', '<r_6934>', '<r_6935>', '<r_6936>', '<r_6937>', '<r_6938>', '<r_6939>', '<r_6940>', '<r_6941>', '<r_6942>', '<r_6943>', '<r_6944>', '<r_6945>', '<r_6946>', '<r_6947>', '<r_6948>', '<r_6949>', '<r_6950>', '<r_6951>', '<r_6952>', '<r_6953>', '<r_6954>', '<r_6955>', '<r_6956>', '<r_6957>', '<r_6958>', '<r_6959>', '<r_6960>', '<r_6961>', '<r_6962>', '<r_6963>', '<r_6964>', '<r_6965>', '<r_6966>', '<r_6967>', '<r_6968>', '<r_6969>', '<r_6970>', '<r_6971>', '<r_6972>', '<r_6973>', '<r_6974>', '<r_6975>', '<r_6976>', '<r_6977>', '<r_6978>', '<r_6979>', '<r_6980>', '<r_6981>', '<r_6982>', '<r_6983>', '<r_6984>', '<r_6985>', '<r_6986>', '<r_6987>', '<r_6988>', '<r_6989>', '<r_6990>', '<r_6991>', '<r_6992>', '<r_6993>', '<r_6994>', '<r_6995>', '<r_6996>', '<r_6997>', '<r_6998>', '<r_6999>', '<r_7000>', '<r_7001>', '<r_7002>', '<r_7003>', '<r_7004>', '<r_7005>', '<r_7006>', '<r_7007>', '<r_7008>', '<r_7009>', '<r_7010>', '<r_7011>', '<r_7012>', '<r_7013>', '<r_7014>', '<r_7015>', '<r_7016>', '<r_7017>', '<r_7018>', '<r_7019>', '<r_7020>', '<r_7021>', '<r_7022>', '<r_7023>', '<r_7024>', '<r_7025>', '<r_7026>', '<r_7027>', '<r_7028>', '<r_7029>', '<r_7030>', '<r_7031>', '<r_7032>', '<r_7033>', '<r_7034>', '<r_7035>', '<r_7036>', '<r_7037>', '<r_7038>', '<r_7039>', '<r_7040>', '<r_7041>', '<r_7042>', '<r_7043>', '<r_7044>', '<r_7045>', '<r_7046>', '<r_7047>', '<r_7048>', '<r_7049>', '<r_7050>', '<r_7051>', '<r_7052>', '<r_7053>', '<r_7054>', '<r_7055>', '<r_7056>', '<r_7057>', '<r_7058>', '<r_7059>', '<r_7060>', '<r_7061>', '<r_7062>', '<r_7063>', '<r_7064>', '<r_7065>', '<r_7066>', '<r_7067>', '<r_7068>', '<r_7069>', '<r_7070>', '<r_7071>', '<r_7072>', '<r_7073>', '<r_7074>', '<r_7075>', '<r_7076>', '<r_7077>', '<r_7078>', '<r_7079>', '<r_7080>', '<r_7081>', '<r_7082>', '<r_7083>', '<r_7084>', '<r_7085>', '<r_7086>', '<r_7087>', '<r_7088>', '<r_7089>', '<r_7090>', '<r_7091>', '<r_7092>', '<r_7093>', '<r_7094>', '<r_7095>', '<r_7096>', '<r_7097>', '<r_7098>', '<r_7099>', '<r_7100>', '<r_7101>', '<r_7102>', '<r_7103>', '<r_7104>', '<r_7105>', '<r_7106>', '<r_7107>', '<r_7108>', '<r_7109>', '<r_7110>', '<r_7111>', '<r_7112>', '<r_7113>', '<r_7114>', '<r_7115>', '<r_7116>', '<r_7117>', '<r_7118>', '<r_7119>', '<r_7120>', '<r_7121>', '<r_7122>', '<r_7123>', '<r_7124>', '<r_7125>', '<r_7126>', '<r_7127>', '<r_7128>', '<r_7129>', '<r_7130>', '<r_7131>', '<r_7132>', '<r_7133>', '<r_7134>', '<r_7135>', '<r_7136>', '<r_7137>', '<r_7138>', '<r_7139>', '<r_7140>', '<r_7141>', '<r_7142>', '<r_7143>', '<r_7144>', '<r_7145>', '<r_7146>', '<r_7147>', '<r_7148>', '<r_7149>', '<r_7150>', '<r_7151>', '<r_7152>', '<r_7153>', '<r_7154>', '<r_7155>', '<r_7156>', '<r_7157>', '<r_7158>', '<r_7159>', '<r_7160>', '<r_7161>', '<r_7162>', '<r_7163>', '<r_7164>', '<r_7165>', '<r_7166>', '<r_7167>', '<r_7168>', '<r_7169>', '<r_7170>', '<r_7171>', '<r_7172>', '<r_7173>', '<r_7174>', '<r_7175>', '<r_7176>', '<r_7177>', '<r_7178>', '<r_7179>', '<r_7180>', '<r_7181>', '<r_7182>', '<r_7183>', '<r_7184>', '<r_7185>', '<r_7186>', '<r_7187>', '<r_7188>', '<r_7189>', '<r_7190>', '<r_7191>', '<r_7192>', '<r_7193>', '<r_7194>', '<r_7195>', '<r_7196>', '<r_7197>', '<r_7198>', '<r_7199>', '<r_7200>', '<r_7201>', '<r_7202>', '<r_7203>', '<r_7204>', '<r_7205>', '<r_7206>', '<r_7207>', '<r_7208>', '<r_7209>', '<r_7210>', '<r_7211>', '<r_7212>', '<r_7213>', '<r_7214>', '<r_7215>', '<r_7216>', '<r_7217>', '<r_7218>', '<r_7219>', '<r_7220>', '<r_7221>', '<r_7222>', '<r_7223>', '<r_7224>', '<r_7225>', '<r_7226>', '<r_7227>', '<r_7228>', '<r_7229>', '<r_7230>', '<r_7231>', '<r_7232>', '<r_7233>', '<r_7234>', '<r_7235>', '<r_7236>', '<r_7237>', '<r_7238>', '<r_7239>', '<r_7240>', '<r_7241>', '<r_7242>', '<r_7243>', '<r_7244>', '<r_7245>', '<r_7246>', '<r_7247>', '<r_7248>', '<r_7249>', '<r_7250>', '<r_7251>', '<r_7252>', '<r_7253>', '<r_7254>', '<r_7255>', '<r_7256>', '<r_7257>', '<r_7258>', '<r_7259>', '<r_7260>', '<r_7261>', '<r_7262>', '<r_7263>', '<r_7264>', '<r_7265>', '<r_7266>', '<r_7267>', '<r_7268>', '<r_7269>', '<r_7270>', '<r_7271>', '<r_7272>', '<r_7273>', '<r_7274>', '<r_7275>', '<r_7276>', '<r_7277>', '<r_7278>', '<r_7279>', '<r_7280>', '<r_7281>', '<r_7282>', '<r_7283>', '<r_7284>', '<r_7285>', '<r_7286>', '<r_7287>', '<r_7288>', '<r_7289>', '<r_7290>', '<r_7291>', '<r_7292>', '<r_7293>', '<r_7294>', '<r_7295>', '<r_7296>', '<r_7297>', '<r_7298>', '<r_7299>', '<r_7300>', '<r_7301>', '<r_7302>', '<r_7303>', '<r_7304>', '<r_7305>', '<r_7306>', '<r_7307>', '<r_7308>', '<r_7309>', '<r_7310>', '<r_7311>', '<r_7312>', '<r_7313>', '<r_7314>', '<r_7315>', '<r_7316>', '<r_7317>', '<r_7318>', '<r_7319>', '<r_7320>', '<r_7321>', '<r_7322>', '<r_7323>', '<r_7324>', '<r_7325>', '<r_7326>', '<r_7327>', '<r_7328>', '<r_7329>', '<r_7330>', '<r_7331>', '<r_7332>', '<r_7333>', '<r_7334>', '<r_7335>', '<r_7336>', '<r_7337>', '<r_7338>', '<r_7339>', '<r_7340>', '<r_7341>', '<r_7342>', '<r_7343>', '<r_7344>', '<r_7345>', '<r_7346>', '<r_7347>', '<r_7348>', '<r_7349>', '<r_7350>', '<r_7351>', '<r_7352>', '<r_7353>', '<r_7354>', '<r_7355>', '<r_7356>', '<r_7357>', '<r_7358>', '<r_7359>', '<r_7360>', '<r_7361>', '<r_7362>', '<r_7363>', '<r_7364>', '<r_7365>', '<r_7366>', '<r_7367>', '<r_7368>', '<r_7369>', '<r_7370>', '<r_7371>', '<r_7372>', '<r_7373>', '<r_7374>', '<r_7375>', '<r_7376>', '<r_7377>', '<r_7378>', '<r_7379>', '<r_7380>', '<r_7381>', '<r_7382>', '<r_7383>', '<r_7384>', '<r_7385>', '<r_7386>', '<r_7387>', '<r_7388>', '<r_7389>', '<r_7390>', '<r_7391>', '<r_7392>', '<r_7393>', '<r_7394>', '<r_7395>', '<r_7396>', '<r_7397>', '<r_7398>', '<r_7399>', '<r_7400>', '<r_7401>', '<r_7402>', '<r_7403>', '<r_7404>', '<r_7405>', '<r_7406>', '<r_7407>', '<r_7408>', '<r_7409>', '<r_7410>', '<r_7411>', '<r_7412>', '<r_7413>', '<r_7414>', '<r_7415>', '<r_7416>', '<r_7417>', '<r_7418>', '<r_7419>', '<r_7420>', '<r_7421>', '<r_7422>', '<r_7423>', '<r_7424>', '<r_7425>', '<r_7426>', '<r_7427>', '<r_7428>', '<r_7429>', '<r_7430>', '<r_7431>', '<r_7432>', '<r_7433>', '<r_7434>', '<r_7435>', '<r_7436>', '<r_7437>', '<r_7438>', '<r_7439>', '<r_7440>', '<r_7441>', '<r_7442>', '<r_7443>', '<r_7444>', '<r_7445>', '<r_7446>', '<r_7447>', '<r_7448>', '<r_7449>', '<r_7450>', '<r_7451>', '<r_7452>', '<r_7453>', '<r_7454>', '<r_7455>', '<r_7456>', '<r_7457>', '<r_7458>', '<r_7459>', '<r_7460>', '<r_7461>', '<r_7462>', '<r_7463>', '<r_7464>', '<r_7465>', '<r_7466>', '<r_7467>', '<r_7468>', '<r_7469>', '<r_7470>', '<r_7471>', '<r_7472>', '<r_7473>', '<r_7474>', '<r_7475>', '<r_7476>', '<r_7477>', '<r_7478>', '<r_7479>', '<r_7480>', '<r_7481>', '<r_7482>', '<r_7483>', '<r_7484>', '<r_7485>', '<r_7486>', '<r_7487>', '<r_7488>', '<r_7489>', '<r_7490>', '<r_7491>', '<r_7492>', '<r_7493>', '<r_7494>', '<r_7495>', '<r_7496>', '<r_7497>', '<r_7498>', '<r_7499>', '<r_7500>', '<r_7501>', '<r_7502>', '<r_7503>', '<r_7504>', '<r_7505>', '<r_7506>', '<r_7507>', '<r_7508>', '<r_7509>', '<r_7510>', '<r_7511>', '<r_7512>', '<r_7513>', '<r_7514>', '<r_7515>', '<r_7516>', '<r_7517>', '<r_7518>', '<r_7519>', '<r_7520>', '<r_7521>', '<r_7522>', '<r_7523>', '<r_7524>', '<r_7525>', '<r_7526>', '<r_7527>', '<r_7528>', '<r_7529>', '<r_7530>', '<r_7531>', '<r_7532>', '<r_7533>', '<r_7534>', '<r_7535>', '<r_7536>', '<r_7537>', '<r_7538>', '<r_7539>', '<r_7540>', '<r_7541>', '<r_7542>', '<r_7543>', '<r_7544>', '<r_7545>', '<r_7546>', '<r_7547>', '<r_7548>', '<r_7549>', '<r_7550>', '<r_7551>', '<r_7552>', '<r_7553>', '<r_7554>', '<r_7555>', '<r_7556>', '<r_7557>', '<r_7558>', '<r_7559>', '<r_7560>', '<r_7561>', '<r_7562>', '<r_7563>', '<r_7564>', '<r_7565>', '<r_7566>', '<r_7567>', '<r_7568>', '<r_7569>', '<r_7570>', '<r_7571>', '<r_7572>', '<r_7573>', '<r_7574>', '<r_7575>', '<r_7576>', '<r_7577>', '<r_7578>', '<r_7579>', '<r_7580>', '<r_7581>', '<r_7582>', '<r_7583>', '<r_7584>', '<r_7585>', '<r_7586>', '<r_7587>', '<r_7588>', '<r_7589>', '<r_7590>', '<r_7591>', '<r_7592>', '<r_7593>', '<r_7594>', '<r_7595>', '<r_7596>', '<r_7597>', '<r_7598>', '<r_7599>', '<r_7600>', '<r_7601>', '<r_7602>', '<r_7603>', '<r_7604>', '<r_7605>', '<r_7606>', '<r_7607>', '<r_7608>', '<r_7609>', '<r_7610>', '<r_7611>', '<r_7612>', '<r_7613>', '<r_7614>', '<r_7615>', '<r_7616>', '<r_7617>', '<r_7618>', '<r_7619>', '<r_7620>', '<r_7621>', '<r_7622>', '<r_7623>', '<r_7624>', '<r_7625>', '<r_7626>', '<r_7627>', '<r_7628>', '<r_7629>', '<r_7630>', '<r_7631>', '<r_7632>', '<r_7633>', '<r_7634>', '<r_7635>', '<r_7636>', '<r_7637>', '<r_7638>', '<r_7639>', '<r_7640>', '<r_7641>', '<r_7642>', '<r_7643>', '<r_7644>', '<r_7645>', '<r_7646>', '<r_7647>', '<r_7648>', '<r_7649>', '<r_7650>', '<r_7651>', '<r_7652>', '<r_7653>', '<r_7654>', '<r_7655>', '<r_7656>', '<r_7657>', '<r_7658>', '<r_7659>', '<r_7660>', '<r_7661>', '<r_7662>', '<r_7663>', '<r_7664>', '<r_7665>', '<r_7666>', '<r_7667>', '<r_7668>', '<r_7669>', '<r_7670>', '<r_7671>', '<r_7672>', '<r_7673>', '<r_7674>', '<r_7675>', '<r_7676>', '<r_7677>', '<r_7678>', '<r_7679>', '<r_7680>', '<r_7681>', '<r_7682>', '<r_7683>', '<r_7684>', '<r_7685>', '<r_7686>', '<r_7687>', '<r_7688>', '<r_7689>', '<r_7690>', '<r_7691>', '<r_7692>', '<r_7693>', '<r_7694>', '<r_7695>', '<r_7696>', '<r_7697>', '<r_7698>', '<r_7699>', '<r_7700>', '<r_7701>', '<r_7702>', '<r_7703>', '<r_7704>', '<r_7705>', '<r_7706>', '<r_7707>', '<r_7708>', '<r_7709>', '<r_7710>', '<r_7711>', '<r_7712>', '<r_7713>', '<r_7714>', '<r_7715>', '<r_7716>', '<r_7717>', '<r_7718>', '<r_7719>', '<r_7720>', '<r_7721>', '<r_7722>', '<r_7723>', '<r_7724>', '<r_7725>', '<r_7726>', '<r_7727>', '<r_7728>', '<r_7729>', '<r_7730>', '<r_7731>', '<r_7732>', '<r_7733>', '<r_7734>', '<r_7735>', '<r_7736>', '<r_7737>', '<r_7738>', '<r_7739>', '<r_7740>', '<r_7741>', '<r_7742>', '<r_7743>', '<r_7744>', '<r_7745>', '<r_7746>', '<r_7747>', '<r_7748>', '<r_7749>', '<r_7750>', '<r_7751>', '<r_7752>', '<r_7753>', '<r_7754>', '<r_7755>', '<r_7756>', '<r_7757>', '<r_7758>', '<r_7759>', '<r_7760>', '<r_7761>', '<r_7762>', '<r_7763>', '<r_7764>', '<r_7765>', '<r_7766>', '<r_7767>', '<r_7768>', '<r_7769>', '<r_7770>', '<r_7771>', '<r_7772>', '<r_7773>', '<r_7774>', '<r_7775>', '<r_7776>', '<r_7777>', '<r_7778>', '<r_7779>', '<r_7780>', '<r_7781>', '<r_7782>', '<r_7783>', '<r_7784>', '<r_7785>', '<r_7786>', '<r_7787>', '<r_7788>', '<r_7789>', '<r_7790>', '<r_7791>', '<r_7792>', '<r_7793>', '<r_7794>', '<r_7795>', '<r_7796>', '<r_7797>', '<r_7798>', '<r_7799>', '<r_7800>', '<r_7801>', '<r_7802>', '<r_7803>', '<r_7804>', '<r_7805>', '<r_7806>', '<r_7807>', '<r_7808>', '<r_7809>', '<r_7810>', '<r_7811>', '<r_7812>', '<r_7813>', '<r_7814>', '<r_7815>', '<r_7816>', '<r_7817>', '<r_7818>', '<r_7819>', '<r_7820>', '<r_7821>', '<r_7822>', '<r_7823>', '<r_7824>', '<r_7825>', '<r_7826>', '<r_7827>', '<r_7828>', '<r_7829>', '<r_7830>', '<r_7831>', '<r_7832>', '<r_7833>', '<r_7834>', '<r_7835>', '<r_7836>', '<r_7837>', '<r_7838>', '<r_7839>', '<r_7840>', '<r_7841>', '<r_7842>', '<r_7843>', '<r_7844>', '<r_7845>', '<r_7846>', '<r_7847>', '<r_7848>', '<r_7849>', '<r_7850>', '<r_7851>', '<r_7852>', '<r_7853>', '<r_7854>', '<r_7855>', '<r_7856>', '<r_7857>', '<r_7858>', '<r_7859>', '<r_7860>', '<r_7861>', '<r_7862>', '<r_7863>', '<r_7864>', '<r_7865>', '<r_7866>', '<r_7867>', '<r_7868>', '<r_7869>', '<r_7870>', '<r_7871>', '<r_7872>', '<r_7873>', '<r_7874>', '<r_7875>', '<r_7876>', '<r_7877>', '<r_7878>', '<r_7879>', '<r_7880>', '<r_7881>', '<r_7882>', '<r_7883>', '<r_7884>', '<r_7885>', '<r_7886>', '<r_7887>', '<r_7888>', '<r_7889>', '<r_7890>', '<r_7891>', '<r_7892>', '<r_7893>', '<r_7894>', '<r_7895>', '<r_7896>', '<r_7897>', '<r_7898>', '<r_7899>', '<r_7900>', '<r_7901>', '<r_7902>', '<r_7903>', '<r_7904>', '<r_7905>', '<r_7906>', '<r_7907>', '<r_7908>', '<r_7909>', '<r_7910>', '<r_7911>', '<r_7912>', '<r_7913>', '<r_7914>', '<r_7915>', '<r_7916>', '<r_7917>', '<r_7918>', '<r_7919>', '<r_7920>', '<r_7921>', '<r_7922>', '<r_7923>', '<r_7924>', '<r_7925>', '<r_7926>', '<r_7927>', '<r_7928>', '<r_7929>', '<r_7930>', '<r_7931>', '<r_7932>', '<r_7933>', '<r_7934>', '<r_7935>', '<r_7936>', '<r_7937>', '<r_7938>', '<r_7939>', '<r_7940>', '<r_7941>', '<r_7942>', '<r_7943>', '<r_7944>', '<r_7945>', '<r_7946>', '<r_7947>', '<r_7948>', '<r_7949>', '<r_7950>', '<r_7951>', '<r_7952>', '<r_7953>', '<r_7954>', '<r_7955>', '<r_7956>', '<r_7957>', '<r_7958>', '<r_7959>', '<r_7960>', '<r_7961>', '<r_7962>', '<r_7963>', '<r_7964>', '<r_7965>', '<r_7966>', '<r_7967>', '<r_7968>', '<r_7969>', '<r_7970>', '<r_7971>', '<r_7972>', '<r_7973>', '<r_7974>', '<r_7975>', '<r_7976>', '<r_7977>', '<r_7978>', '<r_7979>', '<r_7980>', '<r_7981>', '<r_7982>', '<r_7983>', '<r_7984>', '<r_7985>', '<r_7986>', '<r_7987>', '<r_7988>', '<r_7989>', '<r_7990>', '<r_7991>', '<r_7992>', '<r_7993>', '<r_7994>', '<r_7995>', '<r_7996>', '<r_7997>', '<r_7998>', '<r_7999>', '<r_8000>', '<r_8001>', '<r_8002>', '<r_8003>', '<r_8004>', '<r_8005>', '<r_8006>', '<r_8007>', '<r_8008>', '<r_8009>', '<r_8010>', '<r_8011>', '<r_8012>', '<r_8013>', '<r_8014>', '<r_8015>', '<r_8016>', '<r_8017>', '<r_8018>', '<r_8019>', '<r_8020>', '<r_8021>', '<r_8022>', '<r_8023>', '<r_8024>', '<r_8025>', '<r_8026>', '<r_8027>', '<r_8028>', '<r_8029>', '<r_8030>', '<r_8031>', '<r_8032>', '<r_8033>', '<r_8034>', '<r_8035>', '<r_8036>', '<r_8037>', '<r_8038>', '<r_8039>', '<r_8040>', '<r_8041>', '<r_8042>', '<r_8043>', '<r_8044>', '<r_8045>', '<r_8046>', '<r_8047>', '<r_8048>', '<r_8049>', '<r_8050>', '<r_8051>', '<r_8052>', '<r_8053>', '<r_8054>', '<r_8055>', '<r_8056>', '<r_8057>', '<r_8058>', '<r_8059>', '<r_8060>', '<r_8061>', '<r_8062>', '<r_8063>', '<r_8064>', '<r_8065>', '<r_8066>', '<r_8067>', '<r_8068>', '<r_8069>', '<r_8070>', '<r_8071>', '<r_8072>', '<r_8073>', '<r_8074>', '<r_8075>', '<r_8076>', '<r_8077>', '<r_8078>', '<r_8079>', '<r_8080>', '<r_8081>', '<r_8082>', '<r_8083>', '<r_8084>', '<r_8085>', '<r_8086>', '<r_8087>', '<r_8088>', '<r_8089>', '<r_8090>', '<r_8091>', '<r_8092>', '<r_8093>', '<r_8094>', '<r_8095>', '<r_8096>', '<r_8097>', '<r_8098>', '<r_8099>', '<r_8100>', '<r_8101>', '<r_8102>', '<r_8103>', '<r_8104>', '<r_8105>', '<r_8106>', '<r_8107>', '<r_8108>', '<r_8109>', '<r_8110>', '<r_8111>', '<r_8112>', '<r_8113>', '<r_8114>', '<r_8115>', '<r_8116>', '<r_8117>', '<r_8118>', '<r_8119>', '<r_8120>', '<r_8121>', '<r_8122>', '<r_8123>', '<r_8124>', '<r_8125>', '<r_8126>', '<r_8127>', '<r_8128>', '<r_8129>', '<r_8130>', '<r_8131>', '<r_8132>', '<r_8133>', '<r_8134>', '<r_8135>', '<r_8136>', '<r_8137>', '<r_8138>', '<r_8139>', '<r_8140>', '<r_8141>', '<r_8142>', '<r_8143>', '<r_8144>', '<r_8145>', '<r_8146>', '<r_8147>', '<r_8148>', '<r_8149>', '<r_8150>', '<r_8151>', '<r_8152>', '<r_8153>', '<r_8154>', '<r_8155>', '<r_8156>', '<r_8157>', '<r_8158>', '<r_8159>', '<r_8160>', '<r_8161>', '<r_8162>', '<r_8163>', '<r_8164>', '<r_8165>', '<r_8166>', '<r_8167>', '<r_8168>', '<r_8169>', '<r_8170>', '<r_8171>', '<r_8172>', '<r_8173>', '<r_8174>', '<r_8175>', '<r_8176>', '<r_8177>', '<r_8178>', '<r_8179>', '<r_8180>', '<r_8181>', '<r_8182>', '<r_8183>', '<r_8184>', '<r_8185>', '<r_8186>', '<r_8187>', '<r_8188>', '<r_8189>', '<r_8190>', '<r_8191>', '<r_8192>', '<r_8193>', '<r_8194>', '<r_8195>', '<r_8196>', '<r_8197>', '<r_8198>', '<r_8199>', '<r_8200>', '<r_8201>', '<r_8202>', '<r_8203>', '<r_8204>', '<r_8205>', '<r_8206>', '<r_8207>', '<r_8208>', '<r_8209>', '<r_8210>', '<r_8211>', '<r_8212>', '<r_8213>', '<r_8214>', '<r_8215>', '<r_8216>', '<r_8217>', '<r_8218>', '<r_8219>', '<r_8220>', '<r_8221>', '<r_8222>', '<r_8223>', '<r_8224>', '<r_8225>', '<r_8226>', '<r_8227>', '<r_8228>', '<r_8229>', '<r_8230>', '<r_8231>', '<r_8232>', '<r_8233>', '<r_8234>', '<r_8235>', '<r_8236>', '<r_8237>', '<r_8238>', '<r_8239>', '<r_8240>', '<r_8241>', '<r_8242>', '<r_8243>', '<r_8244>', '<r_8245>', '<r_8246>', '<r_8247>', '<r_8248>', '<r_8249>', '<r_8250>', '<r_8251>', '<r_8252>', '<r_8253>', '<r_8254>', '<r_8255>', '<r_8256>', '<r_8257>', '<r_8258>', '<r_8259>', '<r_8260>', '<r_8261>', '<r_8262>', '<r_8263>', '<r_8264>', '<r_8265>', '<r_8266>', '<r_8267>', '<r_8268>', '<r_8269>', '<r_8270>', '<r_8271>', '<r_8272>', '<r_8273>', '<r_8274>', '<r_8275>', '<r_8276>', '<r_8277>', '<r_8278>', '<r_8279>', '<r_8280>', '<r_8281>', '<r_8282>', '<r_8283>', '<r_8284>', '<r_8285>', '<r_8286>', '<r_8287>', '<r_8288>', '<r_8289>', '<r_8290>', '<r_8291>', '<r_8292>', '<r_8293>', '<r_8294>', '<r_8295>', '<r_8296>', '<r_8297>', '<r_8298>', '<r_8299>', '<r_8300>', '<r_8301>', '<r_8302>', '<r_8303>', '<r_8304>', '<r_8305>', '<r_8306>', '<r_8307>', '<r_8308>', '<r_8309>', '<r_8310>', '<r_8311>', '<r_8312>', '<r_8313>', '<r_8314>', '<r_8315>', '<r_8316>', '<r_8317>', '<r_8318>', '<r_8319>', '<r_8320>', '<r_8321>', '<r_8322>', '<r_8323>', '<r_8324>', '<r_8325>', '<r_8326>', '<r_8327>', '<r_8328>', '<r_8329>', '<r_8330>', '<r_8331>', '<r_8332>', '<r_8333>', '<r_8334>', '<r_8335>', '<r_8336>', '<r_8337>', '<r_8338>', '<r_8339>', '<r_8340>', '<r_8341>', '<r_8342>', '<r_8343>', '<r_8344>', '<r_8345>', '<r_8346>', '<r_8347>', '<r_8348>', '<r_8349>', '<r_8350>', '<r_8351>', '<r_8352>', '<r_8353>', '<r_8354>', '<r_8355>', '<r_8356>', '<r_8357>', '<r_8358>', '<r_8359>', '<r_8360>', '<r_8361>', '<r_8362>', '<r_8363>', '<r_8364>', '<r_8365>', '<r_8366>', '<r_8367>', '<r_8368>', '<r_8369>', '<r_8370>', '<r_8371>', '<r_8372>', '<r_8373>', '<r_8374>', '<r_8375>', '<r_8376>', '<r_8377>', '<r_8378>', '<r_8379>', '<r_8380>', '<r_8381>', '<r_8382>', '<r_8383>', '<r_8384>', '<r_8385>', '<r_8386>', '<r_8387>', '<r_8388>', '<r_8389>', '<r_8390>', '<r_8391>', '<r_8392>', '<r_8393>', '<r_8394>', '<r_8395>', '<r_8396>', '<r_8397>', '<r_8398>', '<r_8399>', '<r_8400>', '<r_8401>', '<r_8402>', '<r_8403>', '<r_8404>', '<r_8405>', '<r_8406>', '<r_8407>', '<r_8408>', '<r_8409>', '<r_8410>', '<r_8411>', '<r_8412>', '<r_8413>', '<r_8414>', '<r_8415>', '<r_8416>', '<r_8417>', '<r_8418>', '<r_8419>', '<r_8420>', '<r_8421>', '<r_8422>', '<r_8423>', '<r_8424>', '<r_8425>', '<r_8426>', '<r_8427>', '<r_8428>', '<r_8429>', '<r_8430>', '<r_8431>', '<r_8432>', '<r_8433>', '<r_8434>', '<r_8435>', '<r_8436>', '<r_8437>', '<r_8438>', '<r_8439>', '<r_8440>', '<r_8441>', '<r_8442>', '<r_8443>', '<r_8444>', '<r_8445>', '<r_8446>', '<r_8447>', '<r_8448>', '<r_8449>', '<r_8450>', '<r_8451>', '<r_8452>', '<r_8453>', '<r_8454>', '<r_8455>', '<r_8456>', '<r_8457>', '<r_8458>', '<r_8459>', '<r_8460>', '<r_8461>', '<r_8462>', '<r_8463>', '<r_8464>', '<r_8465>', '<r_8466>', '<r_8467>', '<r_8468>', '<r_8469>', '<r_8470>', '<r_8471>', '<r_8472>', '<r_8473>', '<r_8474>', '<r_8475>', '<r_8476>', '<r_8477>', '<r_8478>', '<r_8479>', '<r_8480>', '<r_8481>', '<r_8482>', '<r_8483>', '<r_8484>', '<r_8485>', '<r_8486>', '<r_8487>', '<r_8488>', '<r_8489>', '<r_8490>', '<r_8491>', '<r_8492>', '<r_8493>', '<r_8494>', '<r_8495>', '<r_8496>', '<r_8497>', '<r_8498>', '<r_8499>', '<r_8500>', '<r_8501>', '<r_8502>', '<r_8503>', '<r_8504>', '<r_8505>', '<r_8506>', '<r_8507>', '<r_8508>', '<r_8509>', '<r_8510>', '<r_8511>', '<r_8512>', '<r_8513>', '<r_8514>', '<r_8515>', '<r_8516>', '<r_8517>', '<r_8518>', '<r_8519>', '<r_8520>', '<r_8521>', '<r_8522>', '<r_8523>', '<r_8524>', '<r_8525>', '<r_8526>', '<r_8527>', '<r_8528>', '<r_8529>', '<r_8530>', '<r_8531>', '<r_8532>', '<r_8533>', '<r_8534>', '<r_8535>', '<r_8536>', '<r_8537>', '<r_8538>', '<r_8539>', '<r_8540>', '<r_8541>', '<r_8542>', '<r_8543>', '<r_8544>', '<r_8545>', '<r_8546>', '<r_8547>', '<r_8548>', '<r_8549>', '<r_8550>', '<r_8551>', '<r_8552>', '<r_8553>', '<r_8554>', '<r_8555>', '<r_8556>', '<r_8557>', '<r_8558>', '<r_8559>', '<r_8560>', '<r_8561>', '<r_8562>', '<r_8563>', '<r_8564>', '<r_8565>', '<r_8566>', '<r_8567>', '<r_8568>', '<r_8569>', '<r_8570>', '<r_8571>', '<r_8572>', '<r_8573>', '<r_8574>', '<r_8575>', '<r_8576>', '<r_8577>', '<r_8578>', '<r_8579>', '<r_8580>', '<r_8581>', '<r_8582>', '<r_8583>', '<r_8584>', '<r_8585>', '<r_8586>', '<r_8587>', '<r_8588>', '<r_8589>', '<r_8590>', '<r_8591>', '<r_8592>', '<r_8593>', '<r_8594>', '<r_8595>', '<r_8596>', '<r_8597>', '<r_8598>', '<r_8599>', '<r_8600>', '<r_8601>', '<r_8602>', '<r_8603>', '<r_8604>', '<r_8605>', '<r_8606>', '<r_8607>', '<r_8608>', '<r_8609>', '<r_8610>', '<r_8611>', '<r_8612>', '<r_8613>', '<r_8614>', '<r_8615>', '<r_8616>', '<r_8617>', '<r_8618>', '<r_8619>', '<r_8620>', '<r_8621>', '<r_8622>', '<r_8623>', '<r_8624>', '<r_8625>', '<r_8626>', '<r_8627>', '<r_8628>', '<r_8629>', '<r_8630>', '<r_8631>', '<r_8632>', '<r_8633>', '<r_8634>', '<r_8635>', '<r_8636>', '<r_8637>', '<r_8638>', '<r_8639>', '<r_8640>', '<r_8641>', '<r_8642>', '<r_8643>', '<r_8644>', '<r_8645>', '<r_8646>', '<r_8647>', '<r_8648>', '<r_8649>', '<r_8650>', '<r_8651>', '<r_8652>', '<r_8653>', '<r_8654>', '<r_8655>', '<r_8656>', '<r_8657>', '<r_8658>', '<r_8659>', '<r_8660>', '<r_8661>', '<r_8662>', '<r_8663>', '<r_8664>', '<r_8665>', '<r_8666>', '<r_8667>', '<r_8668>', '<r_8669>', '<r_8670>', '<r_8671>', '<r_8672>', '<r_8673>', '<r_8674>', '<r_8675>', '<r_8676>', '<r_8677>', '<r_8678>', '<r_8679>', '<r_8680>', '<r_8681>', '<r_8682>', '<r_8683>', '<r_8684>', '<r_8685>', '<r_8686>', '<r_8687>', '<r_8688>', '<r_8689>', '<r_8690>', '<r_8691>', '<r_8692>', '<r_8693>', '<r_8694>', '<r_8695>', '<r_8696>', '<r_8697>', '<r_8698>', '<r_8699>', '<r_8700>', '<r_8701>', '<r_8702>', '<r_8703>', '<r_8704>', '<r_8705>', '<r_8706>', '<r_8707>', '<r_8708>', '<r_8709>', '<r_8710>', '<r_8711>', '<r_8712>', '<r_8713>', '<r_8714>', '<r_8715>', '<r_8716>', '<r_8717>', '<r_8718>', '<r_8719>', '<r_8720>', '<r_8721>', '<r_8722>', '<r_8723>', '<r_8724>', '<r_8725>', '<r_8726>', '<r_8727>', '<r_8728>', '<r_8729>', '<r_8730>', '<r_8731>', '<r_8732>', '<r_8733>', '<r_8734>', '<r_8735>', '<r_8736>', '<r_8737>', '<r_8738>', '<r_8739>', '<r_8740>', '<r_8741>', '<r_8742>', '<r_8743>', '<r_8744>', '<r_8745>', '<r_8746>', '<r_8747>', '<r_8748>', '<r_8749>', '<r_8750>', '<r_8751>', '<r_8752>', '<r_8753>', '<r_8754>', '<r_8755>', '<r_8756>', '<r_8757>', '<r_8758>', '<r_8759>', '<r_8760>', '<r_8761>', '<r_8762>', '<r_8763>', '<r_8764>', '<r_8765>', '<r_8766>', '<r_8767>', '<r_8768>', '<r_8769>', '<r_8770>', '<r_8771>', '<r_8772>', '<r_8773>', '<r_8774>', '<r_8775>', '<r_8776>', '<r_8777>', '<r_8778>', '<r_8779>', '<r_8780>', '<r_8781>', '<r_8782>', '<r_8783>', '<r_8784>', '<r_8785>', '<r_8786>', '<r_8787>', '<r_8788>', '<r_8789>', '<r_8790>', '<r_8791>', '<r_8792>', '<r_8793>', '<r_8794>', '<r_8795>', '<r_8796>', '<r_8797>', '<r_8798>', '<r_8799>', '<r_8800>', '<r_8801>', '<r_8802>', '<r_8803>', '<r_8804>', '<r_8805>', '<r_8806>', '<r_8807>', '<r_8808>', '<r_8809>', '<r_8810>', '<r_8811>', '<r_8812>', '<r_8813>', '<r_8814>', '<r_8815>', '<r_8816>', '<r_8817>', '<r_8818>', '<r_8819>', '<r_8820>', '<r_8821>', '<r_8822>', '<r_8823>', '<r_8824>', '<r_8825>', '<r_8826>', '<r_8827>', '<r_8828>', '<r_8829>', '<r_8830>', '<r_8831>', '<r_8832>', '<r_8833>', '<r_8834>', '<r_8835>', '<r_8836>', '<r_8837>', '<r_8838>', '<r_8839>', '<r_8840>', '<r_8841>', '<r_8842>', '<r_8843>', '<r_8844>', '<r_8845>', '<r_8846>', '<r_8847>', '<r_8848>', '<r_8849>', '<r_8850>', '<r_8851>', '<r_8852>', '<r_8853>', '<r_8854>', '<r_8855>', '<r_8856>', '<r_8857>', '<r_8858>', '<r_8859>', '<r_8860>', '<r_8861>', '<r_8862>', '<r_8863>', '<r_8864>', '<r_8865>', '<r_8866>', '<r_8867>', '<r_8868>', '<r_8869>', '<r_8870>', '<r_8871>', '<r_8872>', '<r_8873>', '<r_8874>', '<r_8875>', '<r_8876>', '<r_8877>', '<r_8878>', '<r_8879>', '<r_8880>', '<r_8881>', '<r_8882>', '<r_8883>', '<r_8884>', '<r_8885>', '<r_8886>', '<r_8887>', '<r_8888>', '<r_8889>', '<r_8890>', '<r_8891>', '<r_8892>', '<r_8893>', '<r_8894>', '<r_8895>', '<r_8896>', '<r_8897>', '<r_8898>', '<r_8899>', '<r_8900>', '<r_8901>', '<r_8902>', '<r_8903>', '<r_8904>', '<r_8905>', '<r_8906>', '<r_8907>', '<r_8908>', '<r_8909>', '<r_8910>', '<r_8911>', '<r_8912>', '<r_8913>', '<r_8914>', '<r_8915>', '<r_8916>', '<r_8917>', '<r_8918>', '<r_8919>', '<r_8920>', '<r_8921>', '<r_8922>', '<r_8923>', '<r_8924>', '<r_8925>', '<r_8926>', '<r_8927>', '<r_8928>', '<r_8929>', '<r_8930>', '<r_8931>', '<r_8932>', '<r_8933>', '<r_8934>', '<r_8935>', '<r_8936>', '<r_8937>', '<r_8938>', '<r_8939>', '<r_8940>', '<r_8941>', '<r_8942>', '<r_8943>', '<r_8944>', '<r_8945>', '<r_8946>', '<r_8947>', '<r_8948>', '<r_8949>', '<r_8950>', '<r_8951>', '<r_8952>', '<r_8953>', '<r_8954>', '<r_8955>', '<r_8956>', '<r_8957>', '<r_8958>', '<r_8959>', '<r_8960>', '<r_8961>', '<r_8962>', '<r_8963>', '<r_8964>', '<r_8965>', '<r_8966>', '<r_8967>', '<r_8968>', '<r_8969>', '<r_8970>', '<r_8971>', '<r_8972>', '<r_8973>', '<r_8974>', '<r_8975>', '<r_8976>', '<r_8977>', '<r_8978>', '<r_8979>', '<r_8980>', '<r_8981>', '<r_8982>', '<r_8983>', '<r_8984>', '<r_8985>', '<r_8986>', '<r_8987>', '<r_8988>', '<r_8989>', '<r_8990>', '<r_8991>', '<r_8992>', '<r_8993>', '<r_8994>', '<r_8995>', '<r_8996>', '<r_8997>', '<r_8998>', '<r_8999>', '<r_9000>', '<r_9001>', '<r_9002>', '<r_9003>', '<r_9004>', '<r_9005>', '<r_9006>', '<r_9007>', '<r_9008>', '<r_9009>', '<r_9010>', '<r_9011>', '<r_9012>', '<r_9013>', '<r_9014>', '<r_9015>', '<r_9016>', '<r_9017>', '<r_9018>', '<r_9019>', '<r_9020>', '<r_9021>', '<r_9022>', '<r_9023>', '<r_9024>', '<r_9025>', '<r_9026>', '<r_9027>', '<r_9028>', '<r_9029>', '<r_9030>', '<r_9031>', '<r_9032>', '<r_9033>', '<r_9034>', '<r_9035>', '<r_9036>', '<r_9037>', '<r_9038>', '<r_9039>', '<r_9040>', '<r_9041>', '<r_9042>', '<r_9043>', '<r_9044>', '<r_9045>', '<r_9046>', '<r_9047>', '<r_9048>', '<r_9049>', '<r_9050>', '<r_9051>', '<r_9052>', '<r_9053>', '<r_9054>', '<r_9055>', '<r_9056>', '<r_9057>', '<r_9058>', '<r_9059>', '<r_9060>', '<r_9061>', '<r_9062>', '<r_9063>', '<r_9064>', '<r_9065>', '<r_9066>', '<r_9067>', '<r_9068>', '<r_9069>', '<r_9070>', '<r_9071>', '<r_9072>', '<r_9073>', '<r_9074>', '<r_9075>', '<r_9076>', '<r_9077>', '<r_9078>', '<r_9079>', '<r_9080>', '<r_9081>', '<r_9082>', '<r_9083>', '<r_9084>', '<r_9085>', '<r_9086>', '<r_9087>', '<r_9088>', '<r_9089>', '<r_9090>', '<r_9091>', '<r_9092>', '<r_9093>', '<r_9094>', '<r_9095>', '<r_9096>', '<r_9097>', '<r_9098>', '<r_9099>', '<r_9100>', '<r_9101>', '<r_9102>', '<r_9103>', '<r_9104>', '<r_9105>', '<r_9106>', '<r_9107>', '<r_9108>', '<r_9109>', '<r_9110>', '<r_9111>', '<r_9112>', '<r_9113>', '<r_9114>', '<r_9115>', '<r_9116>', '<r_9117>', '<r_9118>', '<r_9119>', '<r_9120>', '<r_9121>', '<r_9122>', '<r_9123>', '<r_9124>', '<r_9125>', '<r_9126>', '<r_9127>', '<r_9128>', '<r_9129>', '<r_9130>', '<r_9131>', '<r_9132>', '<r_9133>', '<r_9134>', '<r_9135>', '<r_9136>', '<r_9137>', '<r_9138>', '<r_9139>', '<r_9140>', '<r_9141>', '<r_9142>', '<r_9143>', '<r_9144>', '<r_9145>', '<r_9146>', '<r_9147>', '<r_9148>', '<r_9149>', '<r_9150>', '<r_9151>', '<r_9152>', '<r_9153>', '<r_9154>', '<r_9155>', '<r_9156>', '<r_9157>', '<r_9158>', '<r_9159>', '<r_9160>', '<r_9161>', '<r_9162>', '<r_9163>', '<r_9164>', '<r_9165>', '<r_9166>', '<r_9167>', '<r_9168>', '<r_9169>', '<r_9170>', '<r_9171>', '<r_9172>', '<r_9173>', '<r_9174>', '<r_9175>', '<r_9176>', '<r_9177>', '<r_9178>', '<r_9179>', '<r_9180>', '<r_9181>', '<r_9182>', '<r_9183>', '<r_9184>', '<r_9185>', '<r_9186>', '<r_9187>', '<r_9188>', '<r_9189>', '<r_9190>', '<r_9191>', '<r_9192>', '<r_9193>', '<r_9194>', '<r_9195>', '<r_9196>', '<r_9197>', '<r_9198>', '<r_9199>', '<r_9200>', '<r_9201>', '<r_9202>', '<r_9203>', '<r_9204>', '<r_9205>', '<r_9206>', '<r_9207>', '<r_9208>', '<r_9209>', '<r_9210>', '<r_9211>', '<r_9212>', '<r_9213>', '<r_9214>', '<r_9215>', '<r_9216>', '<r_9217>', '<r_9218>', '<r_9219>', '<r_9220>', '<r_9221>', '<r_9222>', '<r_9223>', '<r_9224>', '<r_9225>', '<r_9226>', '<r_9227>', '<r_9228>', '<r_9229>', '<r_9230>', '<r_9231>', '<r_9232>', '<r_9233>', '<r_9234>', '<r_9235>', '<r_9236>', '<r_9237>', '<r_9238>', '<r_9239>', '<r_9240>', '<r_9241>', '<r_9242>', '<r_9243>', '<r_9244>', '<r_9245>', '<r_9246>', '<r_9247>', '<r_9248>', '<r_9249>', '<r_9250>', '<r_9251>', '<r_9252>', '<r_9253>', '<r_9254>', '<r_9255>', '<r_9256>', '<r_9257>', '<r_9258>', '<r_9259>', '<r_9260>', '<r_9261>', '<r_9262>', '<r_9263>', '<r_9264>', '<r_9265>', '<r_9266>', '<r_9267>', '<r_9268>', '<r_9269>', '<r_9270>', '<r_9271>', '<r_9272>', '<r_9273>', '<r_9274>', '<r_9275>', '<r_9276>', '<r_9277>', '<r_9278>', '<r_9279>', '<r_9280>', '<r_9281>', '<r_9282>', '<r_9283>', '<r_9284>', '<r_9285>', '<r_9286>', '<r_9287>', '<r_9288>', '<r_9289>', '<r_9290>', '<r_9291>', '<r_9292>', '<r_9293>', '<r_9294>', '<r_9295>', '<r_9296>', '<r_9297>', '<r_9298>', '<r_9299>', '<r_9300>', '<r_9301>', '<r_9302>', '<r_9303>', '<r_9304>', '<r_9305>', '<r_9306>', '<r_9307>', '<r_9308>', '<r_9309>', '<r_9310>', '<r_9311>', '<r_9312>', '<r_9313>', '<r_9314>', '<r_9315>', '<r_9316>', '<r_9317>', '<r_9318>', '<r_9319>', '<r_9320>', '<r_9321>', '<r_9322>', '<r_9323>', '<r_9324>', '<r_9325>', '<r_9326>', '<r_9327>', '<r_9328>', '<r_9329>', '<r_9330>', '<r_9331>', '<r_9332>', '<r_9333>', '<r_9334>', '<r_9335>', '<r_9336>', '<r_9337>', '<r_9338>', '<r_9339>', '<r_9340>', '<r_9341>', '<r_9342>', '<r_9343>', '<r_9344>', '<r_9345>', '<r_9346>', '<r_9347>', '<r_9348>', '<r_9349>', '<r_9350>', '<r_9351>', '<r_9352>', '<r_9353>', '<r_9354>', '<r_9355>', '<r_9356>', '<r_9357>', '<r_9358>', '<r_9359>', '<r_9360>', '<r_9361>', '<r_9362>', '<r_9363>', '<r_9364>', '<r_9365>', '<r_9366>', '<r_9367>', '<r_9368>', '<r_9369>', '<r_9370>', '<r_9371>', '<r_9372>', '<r_9373>', '<r_9374>', '<r_9375>', '<r_9376>', '<r_9377>', '<r_9378>', '<r_9379>', '<r_9380>', '<r_9381>', '<r_9382>', '<r_9383>', '<r_9384>', '<r_9385>', '<r_9386>', '<r_9387>', '<r_9388>', '<r_9389>', '<r_9390>', '<r_9391>', '<r_9392>', '<r_9393>', '<r_9394>', '<r_9395>', '<r_9396>', '<r_9397>', '<r_9398>', '<r_9399>', '<r_9400>', '<r_9401>', '<r_9402>', '<r_9403>', '<r_9404>', '<r_9405>', '<r_9406>', '<r_9407>', '<r_9408>', '<r_9409>', '<r_9410>', '<r_9411>', '<r_9412>', '<r_9413>', '<r_9414>', '<r_9415>', '<r_9416>', '<r_9417>', '<r_9418>', '<r_9419>', '<r_9420>', '<r_9421>', '<r_9422>', '<r_9423>', '<r_9424>', '<r_9425>', '<r_9426>', '<r_9427>', '<r_9428>', '<r_9429>', '<r_9430>', '<r_9431>', '<r_9432>', '<r_9433>', '<r_9434>', '<r_9435>', '<r_9436>', '<r_9437>', '<r_9438>', '<r_9439>', '<r_9440>', '<r_9441>', '<r_9442>', '<r_9443>', '<r_9444>', '<r_9445>', '<r_9446>', '<r_9447>', '<r_9448>', '<r_9449>', '<r_9450>', '<r_9451>', '<r_9452>', '<r_9453>', '<r_9454>', '<r_9455>', '<r_9456>', '<r_9457>', '<r_9458>', '<r_9459>', '<r_9460>', '<r_9461>', '<r_9462>', '<r_9463>', '<r_9464>', '<r_9465>', '<r_9466>', '<r_9467>', '<r_9468>', '<r_9469>', '<r_9470>', '<r_9471>', '<r_9472>', '<r_9473>', '<r_9474>', '<r_9475>', '<r_9476>', '<r_9477>', '<r_9478>', '<r_9479>', '<r_9480>', '<r_9481>', '<r_9482>', '<r_9483>', '<r_9484>', '<r_9485>', '<r_9486>', '<r_9487>', '<r_9488>', '<r_9489>', '<r_9490>', '<r_9491>', '<r_9492>', '<r_9493>', '<r_9494>', '<r_9495>', '<r_9496>', '<r_9497>', '<r_9498>', '<r_9499>', '<r_9500>', '<r_9501>', '<r_9502>', '<r_9503>', '<r_9504>', '<r_9505>', '<r_9506>', '<r_9507>', '<r_9508>', '<r_9509>', '<r_9510>', '<r_9511>', '<r_9512>', '<r_9513>', '<r_9514>', '<r_9515>', '<r_9516>', '<r_9517>', '<r_9518>', '<r_9519>', '<r_9520>', '<r_9521>', '<r_9522>', '<r_9523>', '<r_9524>', '<r_9525>', '<r_9526>', '<r_9527>', '<r_9528>', '<r_9529>', '<r_9530>', '<r_9531>', '<r_9532>', '<r_9533>', '<r_9534>', '<r_9535>', '<r_9536>', '<r_9537>', '<r_9538>', '<r_9539>', '<r_9540>', '<r_9541>', '<r_9542>', '<r_9543>', '<r_9544>', '<r_9545>', '<r_9546>', '<r_9547>', '<r_9548>', '<r_9549>', '<r_9550>', '<r_9551>', '<r_9552>', '<r_9553>', '<r_9554>', '<r_9555>', '<r_9556>', '<r_9557>', '<r_9558>', '<r_9559>', '<r_9560>', '<r_9561>', '<r_9562>', '<r_9563>', '<r_9564>', '<r_9565>', '<r_9566>', '<r_9567>', '<r_9568>', '<r_9569>', '<r_9570>', '<r_9571>', '<r_9572>', '<r_9573>', '<r_9574>', '<r_9575>', '<r_9576>', '<r_9577>', '<r_9578>', '<r_9579>', '<r_9580>', '<r_9581>', '<r_9582>', '<r_9583>', '<r_9584>', '<r_9585>', '<r_9586>', '<r_9587>', '<r_9588>', '<r_9589>', '<r_9590>', '<r_9591>', '<r_9592>', '<r_9593>', '<r_9594>', '<r_9595>', '<r_9596>', '<r_9597>', '<r_9598>', '<r_9599>', '<r_9600>', '<r_9601>', '<r_9602>', '<r_9603>', '<r_9604>', '<r_9605>', '<r_9606>', '<r_9607>', '<r_9608>', '<r_9609>', '<r_9610>', '<r_9611>', '<r_9612>', '<r_9613>', '<r_9614>', '<r_9615>', '<r_9616>', '<r_9617>', '<r_9618>', '<r_9619>', '<r_9620>', '<r_9621>', '<r_9622>', '<r_9623>', '<r_9624>', '<r_9625>', '<r_9626>', '<r_9627>', '<r_9628>', '<r_9629>', '<r_9630>', '<r_9631>', '<r_9632>', '<r_9633>', '<r_9634>', '<r_9635>', '<r_9636>', '<r_9637>', '<r_9638>', '<r_9639>', '<r_9640>', '<r_9641>', '<r_9642>', '<r_9643>', '<r_9644>', '<r_9645>', '<r_9646>', '<r_9647>', '<r_9648>', '<r_9649>', '<r_9650>', '<r_9651>', '<r_9652>', '<r_9653>', '<r_9654>', '<r_9655>', '<r_9656>', '<r_9657>', '<r_9658>', '<r_9659>', '<r_9660>', '<r_9661>', '<r_9662>', '<r_9663>', '<r_9664>', '<r_9665>', '<r_9666>', '<r_9667>', '<r_9668>', '<r_9669>', '<r_9670>', '<r_9671>', '<r_9672>', '<r_9673>', '<r_9674>', '<r_9675>', '<r_9676>', '<r_9677>', '<r_9678>', '<r_9679>', '<r_9680>', '<r_9681>', '<r_9682>', '<r_9683>', '<r_9684>', '<r_9685>', '<r_9686>', '<r_9687>', '<r_9688>', '<r_9689>', '<r_9690>', '<r_9691>', '<r_9692>', '<r_9693>', '<r_9694>', '<r_9695>', '<r_9696>', '<r_9697>', '<r_9698>', '<r_9699>', '<r_9700>', '<r_9701>', '<r_9702>', '<r_9703>', '<r_9704>', '<r_9705>', '<r_9706>', '<r_9707>', '<r_9708>', '<r_9709>', '<r_9710>', '<r_9711>', '<r_9712>', '<r_9713>', '<r_9714>', '<r_9715>', '<r_9716>', '<r_9717>', '<r_9718>', '<r_9719>', '<r_9720>', '<r_9721>', '<r_9722>', '<r_9723>', '<r_9724>', '<r_9725>', '<r_9726>', '<r_9727>', '<r_9728>', '<r_9729>', '<r_9730>', '<r_9731>', '<r_9732>', '<r_9733>', '<r_9734>', '<r_9735>', '<r_9736>', '<r_9737>', '<r_9738>', '<r_9739>', '<r_9740>', '<r_9741>', '<r_9742>', '<r_9743>', '<r_9744>', '<r_9745>', '<r_9746>', '<r_9747>', '<r_9748>', '<r_9749>', '<r_9750>', '<r_9751>', '<r_9752>', '<r_9753>', '<r_9754>', '<r_9755>', '<r_9756>', '<r_9757>', '<r_9758>', '<r_9759>', '<r_9760>', '<r_9761>', '<r_9762>', '<r_9763>', '<r_9764>', '<r_9765>', '<r_9766>', '<r_9767>', '<r_9768>', '<r_9769>', '<r_9770>', '<r_9771>', '<r_9772>', '<r_9773>', '<r_9774>', '<r_9775>', '<r_9776>', '<r_9777>', '<r_9778>', '<r_9779>', '<r_9780>', '<r_9781>', '<r_9782>', '<r_9783>', '<r_9784>', '<r_9785>', '<r_9786>', '<r_9787>', '<r_9788>', '<r_9789>', '<r_9790>', '<r_9791>', '<r_9792>', '<r_9793>', '<r_9794>', '<r_9795>', '<r_9796>', '<r_9797>', '<r_9798>', '<r_9799>', '<r_9800>', '<r_9801>', '<r_9802>', '<r_9803>', '<r_9804>', '<r_9805>', '<r_9806>', '<r_9807>', '<r_9808>', '<r_9809>', '<r_9810>', '<r_9811>', '<r_9812>', '<r_9813>', '<r_9814>', '<r_9815>', '<r_9816>', '<r_9817>', '<r_9818>', '<r_9819>', '<r_9820>', '<r_9821>', '<r_9822>', '<r_9823>', '<r_9824>', '<r_9825>', '<r_9826>', '<r_9827>', '<r_9828>', '<r_9829>', '<r_9830>', '<r_9831>', '<r_9832>', '<r_9833>', '<r_9834>', '<r_9835>', '<r_9836>', '<r_9837>', '<r_9838>', '<r_9839>', '<r_9840>', '<r_9841>', '<r_9842>', '<r_9843>', '<r_9844>', '<r_9845>', '<r_9846>', '<r_9847>', '<r_9848>', '<r_9849>', '<r_9850>', '<r_9851>', '<r_9852>', '<r_9853>', '<r_9854>', '<r_9855>', '<r_9856>', '<r_9857>', '<r_9858>', '<r_9859>', '<r_9860>', '<r_9861>', '<r_9862>', '<r_9863>', '<r_9864>', '<r_9865>', '<r_9866>', '<r_9867>', '<r_9868>', '<r_9869>', '<r_9870>', '<r_9871>', '<r_9872>', '<r_9873>', '<r_9874>', '<r_9875>', '<r_9876>', '<r_9877>', '<r_9878>', '<r_9879>', '<r_9880>', '<r_9881>', '<r_9882>', '<r_9883>', '<r_9884>', '<r_9885>', '<r_9886>', '<r_9887>', '<r_9888>', '<r_9889>', '<r_9890>', '<r_9891>', '<r_9892>', '<r_9893>', '<r_9894>', '<r_9895>', '<r_9896>', '<r_9897>', '<r_9898>', '<r_9899>', '<r_9900>', '<r_9901>', '<r_9902>', '<r_9903>', '<r_9904>', '<r_9905>', '<r_9906>', '<r_9907>', '<r_9908>', '<r_9909>', '<r_9910>', '<r_9911>', '<r_9912>', '<r_9913>', '<r_9914>', '<r_9915>', '<r_9916>', '<r_9917>', '<r_9918>', '<r_9919>', '<r_9920>', '<r_9921>', '<r_9922>', '<r_9923>', '<r_9924>', '<r_9925>', '<r_9926>', '<r_9927>', '<r_9928>', '<r_9929>', '<r_9930>', '<r_9931>', '<r_9932>', '<r_9933>', '<r_9934>', '<r_9935>', '<r_9936>', '<r_9937>', '<r_9938>', '<r_9939>', '<r_9940>', '<r_9941>', '<r_9942>', '<r_9943>', '<r_9944>', '<r_9945>', '<r_9946>', '<r_9947>', '<r_9948>', '<r_9949>', '<r_9950>', '<r_9951>', '<r_9952>', '<r_9953>', '<r_9954>', '<r_9955>', '<r_9956>', '<r_9957>', '<r_9958>', '<r_9959>', '<r_9960>', '<r_9961>', '<r_9962>', '<r_9963>', '<r_9964>', '<r_9965>', '<r_9966>', '<r_9967>', '<r_9968>', '<r_9969>', '<r_9970>', '<r_9971>', '<r_9972>', '<r_9973>', '<r_9974>', '<r_9975>', '<r_9976>', '<r_9977>', '<r_9978>', '<r_9979>', '<r_9980>', '<r_9981>', '<r_9982>', '<r_9983>', '<r_9984>', '<r_9985>', '<r_9986>', '<r_9987>', '<r_9988>', '<r_9989>', '<r_9990>', '<r_9991>', '<r_9992>', '<r_9993>', '<r_9994>', '<r_9995>', '<r_9996>', '<r_9997>', '<r_9998>', '<r_9999>', '<f>']\n",
      "vocab size: 10011\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "vocab = vocab + entities + relations\n",
    "print(vocab)\n",
    "# special tokens\n",
    "vocab = vocab\n",
    "assert len(vocab) == len(set(vocab))\n",
    "print(\"vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "id": "9MAC5YytRsfF"
   },
   "outputs": [],
   "source": [
    "dataset_name = \"composition_functor.{}.{}.{}\".format(NUM_ENTITY_IN, NUM_RELATION, SUB_SIZE)\n",
    "os.makedirs(\"data/{}\".format(dataset_name), exist_ok=True)\n",
    "\n",
    "probes = []\n",
    "for item in id_atomic_facts:\n",
    "    probes.append(deepcopy(item))\n",
    "    probes[-1][\"type\"] = \"id_atomic\"\n",
    "\n",
    "for item in ood_atomic_facts:\n",
    "    probes.append(deepcopy(item))\n",
    "    probes[-1][\"type\"] = \"ood_atomic\"\n",
    "\n",
    "for item in id_compositional_facts:\n",
    "    probes.append(deepcopy(item))\n",
    "    probes[-1]['type'] = 'id_compositional'\n",
    "\n",
    "for item in near_ood_compositional_facts:\n",
    "    probes.append(deepcopy(item))\n",
    "    probes[-1]['type'] = 'near_ood_compositional'\n",
    "\n",
    "for item in far_ood_compositional_facts:\n",
    "    probes.append(deepcopy(item))\n",
    "    probes[-1][\"type\"] = \"far_ood_compositional\"\n",
    "\n",
    "for item in id_analogical_facts:\n",
    "    probes.append(deepcopy(item))\n",
    "    probes[-1][\"type\"] = \"id_analogical\"\n",
    "\n",
    "for item in ood_analogical_facts:\n",
    "    probes.append(deepcopy(item))\n",
    "    probes[-1][\"type\"] = \"ood_analogical\"\n",
    "\n",
    "with open(\"data/{}/train.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(id_atomic_facts + id_compositional_facts + id_analogical_facts, f)\n",
    "    # json.dump(iid_atomic_facts_ds + f_id_atomic_facts_ds, f)\n",
    "with open(\"data/{}/ood_atomic.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(ood_atomic_facts, f)\n",
    "with open(\"data/{}/id_compositional.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(id_compositional_facts, f)\n",
    "with open(\"data/{}/near_ood_compositional.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(near_ood_compositional_facts, f)\n",
    "with open(\"data/{}/far_ood_compositional.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(far_ood_compositional_facts, f)\n",
    "with open(\"data/{}/id_analogical.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(id_analogical_facts, f)\n",
    "with open(\"data/{}/ood_analogical.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(ood_analogical_facts, f)\n",
    "with open(\"data/{}/test.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(probes, f)\n",
    "# add vocab\n",
    "with open(\"data/{}/vocab.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVsXq8fXbwgC"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zw4vc2kakVG-",
    "outputId": "e4d1efef-9841-40b0-bf06-20120d56dac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_API_KEY=6c4044553b7df05658822925678583e3e13ae54f\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_API_KEY=6c4044553b7df05658822925678583e3e13ae54f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p53DaSOnkiyG",
    "outputId": "33e29ffd-8b56-4b7e-bf3d-c4970cb2253e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -q wandb\n",
    "import wandb; wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jv3uVmVMXsfA"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "id": "1Vn9ez5VXink"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----------------------------\n",
    "# RoPE utilities\n",
    "# ----------------------------\n",
    "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "    # x: (..., d)\n",
    "    d = x.size(-1)\n",
    "    x1 = x[..., : d // 2]\n",
    "    x2 = x[..., d // 2 :]\n",
    "    return torch.cat([-x2, x1], dim=-1)\n",
    "\n",
    "def apply_rope(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
    "    # x: (B, H, L, D)\n",
    "    # cos/sin: (1, 1, L, D)\n",
    "    return (x * cos) + (rotate_half(x) * sin)\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Precompute cos/sin for RoPE.\n",
    "    - head_dim must be even.\n",
    "    \"\"\"\n",
    "    def __init__(self, head_dim: int, max_len: int = 1024, base: float = 10000.0):\n",
    "        super().__init__()\n",
    "        assert head_dim % 2 == 0, \"RoPE requires even head_dim\"\n",
    "        self.head_dim = head_dim\n",
    "        self.max_len = max_len\n",
    "        self.base = base\n",
    "\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))  # (D/2,)\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        # cache cos/sin up to max_len\n",
    "        self._build_cache(max_len)\n",
    "\n",
    "    def _build_cache(self, max_len: int):\n",
    "        t = torch.arange(max_len, dtype=torch.float32)  # (L,)\n",
    "        freqs = torch.einsum(\"l,d->ld\", t, self.inv_freq)  # (L, D/2)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)  # (L, D)\n",
    "        cos = emb.cos()[None, None, :, :]  # (1,1,L,D)\n",
    "        sin = emb.sin()[None, None, :, :]  # (1,1,L,D)\n",
    "        self.register_buffer(\"cos_cached\", cos, persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", sin, persistent=False)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, seq_len: int, device=None, dtype=None):\n",
    "        if seq_len > self.max_len:\n",
    "            # extend cache if needed\n",
    "            self._build_cache(seq_len)\n",
    "        cos = self.cos_cached[:, :, :seq_len, :]\n",
    "        sin = self.sin_cached[:, :, :seq_len, :]\n",
    "        if device is not None:\n",
    "            cos = cos.to(device)\n",
    "            sin = sin.to(device)\n",
    "        if dtype is not None:\n",
    "            cos = cos.to(dtype=dtype)\n",
    "            sin = sin.to(dtype=dtype)\n",
    "        return cos, sin\n",
    "\n",
    "# ----------------------------\n",
    "# RoPE Causal Self-Attention\n",
    "# ----------------------------\n",
    "class CausalSelfAttentionRoPE(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, dropout: float, max_len: int, rope_base: float = 10000.0):\n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0, \"d_model must be divisible by n_head\"\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = d_model // n_head\n",
    "        assert self.head_dim % 2 == 0, \"head_dim must be even for RoPE\"\n",
    "\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.resid_drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.rope = RotaryEmbedding(self.head_dim, max_len=max_len, base=rope_base)\n",
    "\n",
    "        # causal mask: (1, 1, L, L) broadcastable\n",
    "        causal = torch.triu(torch.ones(max_len, max_len), diagonal=1).bool()\n",
    "        self.register_buffer(\"causal_mask\", causal[None, None, :, :], persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, pad_mask: torch.Tensor | None = None):\n",
    "        # x: (B, L, C)\n",
    "        B, L, C = x.shape\n",
    "\n",
    "        qkv = self.qkv(x)  # (B, L, 3C)\n",
    "        q, k, v = qkv.split(C, dim=-1)\n",
    "\n",
    "        # (B, H, L, D)\n",
    "        q = q.view(B, L, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, L, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, L, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        cos, sin = self.rope(seq_len=L, device=x.device, dtype=x.dtype)\n",
    "        q = apply_rope(q, cos, sin)\n",
    "        k = apply_rope(k, cos, sin)\n",
    "\n",
    "        # attention scores: (B, H, L, L)\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # causal\n",
    "        att = att.masked_fill(self.causal_mask[:, :, :L, :L], float(\"-inf\"))\n",
    "\n",
    "        # padding (pad_mask: True=PAD assumed)\n",
    "        if pad_mask is not None:\n",
    "            # pad_mask: (B, L) -> (B, 1, 1, L)\n",
    "            att = att.masked_fill(pad_mask[:, None, None, :], float(\"-inf\"))\n",
    "\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "\n",
    "        y = att @ v  # (B, H, L, D)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, L, C)\n",
    "        y = self.resid_drop(self.out(y))\n",
    "        return y\n",
    "\n",
    "# ----------------------------\n",
    "# GPT2-like Block (Pre-LN)\n",
    "# ----------------------------\n",
    "class GPT2BlockRoPE(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, dropout: float, max_len: int, rope_base: float = 10000.0):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttentionRoPE(d_model, n_head, dropout, max_len, rope_base=rope_base)\n",
    "        self.ln_2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, pad_mask: torch.Tensor | None = None):\n",
    "        x = x + self.attn(self.ln_1(x), pad_mask=pad_mask)\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "# ----------------------------\n",
    "# Model (RoPE version)\n",
    "# ----------------------------\n",
    "class GPT2LikeEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=768, n_layer=8, n_head=12, dropout=0, max_len=1024, rope_base=100.0):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GPT2BlockRoPE(d_model, n_head, dropout, max_len, rope_base=rope_base)\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        # weight tying\n",
    "        self.head.weight = self.tok_emb.weight\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.apply(self._init)\n",
    "\n",
    "    def _init(self, m):\n",
    "        if isinstance(m, (nn.Linear, nn.Embedding)):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        if isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, input_ids, pad_mask=None):\n",
    "        # input_ids: (B, L), pad_mask: (B, L) with True=PAD\n",
    "        B, L = input_ids.shape\n",
    "        if L > self.max_len:\n",
    "            raise ValueError(f\"seq_len {L} exceeds max_len {self.max_len}. Increase max_len.\")\n",
    "\n",
    "        x = self.drop(self.tok_emb(input_ids))\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, pad_mask=pad_mask)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TglhUaNXuK9"
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "id": "IqSjw35mXm1V"
   },
   "outputs": [],
   "source": [
    "from torch._C import Value\n",
    "# === Colab-ready: GPT-2-like training with functor dataset support (last-entity loss + CE/PPL/ACC + W&B logging) ===\n",
    "import os, json, re, math, random, time\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------------\n",
    "# User settings\n",
    "# ----------------------------\n",
    "data_dir   = \"data/composition_functor.10.10000.5\"   # ← functor correspondence dataset folder\n",
    "save_dir   = \"runs/functor_colab\"\n",
    "project    = \"emergent_analogy_functor_20260115\"\n",
    "run_name   = f\"gpt2like_functor-{os.path.basename(data_dir)}_{int(time.time())}\"\n",
    "\n",
    "max_len    = 64\n",
    "batch_size = 64\n",
    "epochs     = 1000\n",
    "d_model    = 128  #768\n",
    "n_layer    = 2   #8\n",
    "n_head     = 1   #12\n",
    "dropout    = 0\n",
    "lr         = 1e-2\n",
    "weight_decay = 0.01\n",
    "warmup_steps = 0\n",
    "use_amp    = True\n",
    "seed       = 42\n",
    "use_wandb  = True\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysckbmZ5XvfF"
   },
   "source": [
    "## Tokenize And Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "id": "6sEM-En6XrOn"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# Tokenizer for <e_#>/<r_#>/<f>/<f_inv>\n",
    "# ----------------------------\n",
    "TOKEN_PATTERN = re.compile(r\"<e_\\d+>|<r_\\d+>|<f>|<f_inv>\")\n",
    "\n",
    "def tokenize_strict(s: str) -> List[str]:\n",
    "    toks = TOKEN_PATTERN.findall(s)\n",
    "    if \"\".join(toks) != s:\n",
    "        bad = s.replace(\"\".join(toks), \"\")\n",
    "        raise ValueError(f\"Non-token residue: '{bad}' in '{s}'\")\n",
    "    return toks\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset\n",
    "# ----------------------------\n",
    "class CompDataset(Dataset):\n",
    "    def __init__(self, path_json: str, vocab_path: str, max_len: int, expect_type: bool):\n",
    "        self.items = json.load(open(path_json, \"r\", encoding=\"utf-8\"))\n",
    "        self.vocab = json.load(open(vocab_path, \"r\", encoding=\"utf-8\"))\n",
    "        self.tok2id = {t:i for i,t in enumerate(self.vocab)}\n",
    "        self.max_len = max_len\n",
    "        self.expect_type = expect_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def encode(self, toks):\n",
    "        return [self.tok2id[t] for t in toks]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        it = self.items[idx]\n",
    "        inp = tokenize_strict(it[\"input_text\"])\n",
    "        tgt = tokenize_strict(it[\"target_text\"])\n",
    "        input_ids = self.encode(tgt[:-1])\n",
    "        target_ids = self.encode(tgt[1:])\n",
    "        last_pos = len(target_ids) - 1\n",
    "        out = {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"target_ids\": torch.tensor(target_ids, dtype=torch.long),\n",
    "            \"last_pos\": last_pos,\n",
    "            \"length\": len(input_ids),\n",
    "        }\n",
    "        # raise ValueError(\"debug\")\n",
    "        if self.expect_type:\n",
    "            out[\"type\"] = it.get(\"type\", \"unknown\")\n",
    "        return out\n",
    "\n",
    "def collate_pad(batch, pad_id=0):\n",
    "    B = len(batch)\n",
    "    maxL = max(ex[\"length\"] for ex in batch)\n",
    "    input_ids = torch.full((B,maxL), pad_id, dtype=torch.long)\n",
    "    target_ids = torch.full((B,maxL), -100, dtype=torch.long)\n",
    "    loss_mask = torch.zeros((B,maxL), dtype=torch.bool)\n",
    "    pad_mask  = torch.ones((B,maxL), dtype=torch.bool)\n",
    "    types = []\n",
    "    for i, ex in enumerate(batch):\n",
    "        L = ex[\"length\"]\n",
    "        input_ids[i,:L] = ex[\"input_ids\"]\n",
    "        target_ids[i,:L] = ex[\"target_ids\"]\n",
    "        loss_mask[i, ex[\"last_pos\"]] = True\n",
    "        pad_mask[i,:L] = False\n",
    "        if \"type\" in ex:\n",
    "            types.append(ex[\"type\"])\n",
    "    out = {\"input_ids\":input_ids, \"target_ids\":target_ids, \"loss_mask\":loss_mask, \"pad_mask\":pad_mask}\n",
    "    if types:\n",
    "        out[\"type\"] = types\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# Warmup scheduler\n",
    "# ----------------------------\n",
    "class WarmupThenConstant(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, opt, warmup_steps=2000, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super().__init__(opt, last_epoch)\n",
    "    def get_lr(self):\n",
    "        step = max(1, self.last_epoch+1)\n",
    "        scale = step/self.warmup_steps if step <= self.warmup_steps else 1.0\n",
    "        return [base*scale for base in self.base_lrs]\n",
    "\n",
    "# ----------------------------\n",
    "# Data loading\n",
    "# ----------------------------\n",
    "vocab_path = os.path.join(data_dir, \"vocab.json\")\n",
    "train_path = os.path.join(data_dir, \"train.json\")\n",
    "test_path  = os.path.join(data_dir, \"test.json\")\n",
    "\n",
    "train_ds = CompDataset(train_path, vocab_path, max_len=max_len, expect_type=False)\n",
    "test_ds  = CompDataset(test_path,  vocab_path, max_len=max_len, expect_type=True)\n",
    "\n",
    "collate = lambda b: collate_pad(b, pad_id=0)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=1, collate_fn=collate, pin_memory=True)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=1, collate_fn=collate, pin_memory=True)\n",
    "\n",
    "def make_eval_loader(path):\n",
    "    if os.path.exists(path):\n",
    "        ds = CompDataset(path, vocab_path, max_len=max_len, expect_type=False)\n",
    "        return DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=1, collate_fn=collate, pin_memory=True)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f2AqcTc-KQRI",
    "outputId": "f0fa3d84-a084-41ba-8aa4-c645a54f3f4e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇█████</td></tr><tr><td>global_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/ACC_last</td><td>▁▂▂████████████████████▆████████████████</td></tr><tr><td>train/CE_epoch</td><td>██▇▇▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/CE_last</td><td>█▇▅▃▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/macro/ACC</td><td>▂▁▇▇█▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▅▇▇▇▇▇▇▇▇▇▅▆▇▇▇</td></tr><tr><td>val/macro/CE</td><td>█▄▂▁▁▂▂▂▂▂▁▂▁▂▄▃▄▄▃▃▅▅▅▆▃▄▄▄▄▄▄▄▅▅▅▃▃▄▄▅</td></tr><tr><td>val_ACC/id_analogical</td><td>██████████████████▁██████████████▁██████</td></tr><tr><td>val_ACC/id_atomic</td><td>▁▁██████████████████▆███████████████████</td></tr><tr><td>+18</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>584</td></tr><tr><td>global_step</td><td>1753</td></tr><tr><td>lr</td><td>0.01</td></tr><tr><td>train/ACC_last</td><td>1</td></tr><tr><td>train/CE_epoch</td><td>0.00898</td></tr><tr><td>train/CE_last</td><td>0.00913</td></tr><tr><td>val/macro/ACC</td><td>0.8</td></tr><tr><td>val/macro/CE</td><td>2.64905</td></tr><tr><td>val_ACC/id_analogical</td><td>1</td></tr><tr><td>val_ACC/id_atomic</td><td>1</td></tr><tr><td>+18</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gpt2like_functor-composition_functor.10.10000.5_1768478939</strong> at: <a href='https://wandb.ai/gouki/emergent_analogy_functor_20260115/runs/yh4kf13t' target=\"_blank\">https://wandb.ai/gouki/emergent_analogy_functor_20260115/runs/yh4kf13t</a><br> View project at: <a href='https://wandb.ai/gouki/emergent_analogy_functor_20260115' target=\"_blank\">https://wandb.ai/gouki/emergent_analogy_functor_20260115</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260115_120901-yh4kf13t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20260115_121136-h5wwjcw8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gouki/emergent_analogy_functor_20260115/runs/h5wwjcw8' target=\"_blank\">gpt2like_functor-composition_functor.10.10000.5_1768479094</a></strong> to <a href='https://wandb.ai/gouki/emergent_analogy_functor_20260115' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gouki/emergent_analogy_functor_20260115' target=\"_blank\">https://wandb.ai/gouki/emergent_analogy_functor_20260115</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gouki/emergent_analogy_functor_20260115/runs/h5wwjcw8' target=\"_blank\">https://wandb.ai/gouki/emergent_analogy_functor_20260115/runs/h5wwjcw8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-4216942173.py:33: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
      "/tmp/ipython-input-4216942173.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 | train CE: 0.8334\n",
      "\n",
      "== epoch 10 validation ==\n",
      "       id_analogical: CE=1.9256 | PPL=6.859 | ACC=0.000 | N=3\n",
      "           id_atomic: CE=0.6851 | PPL=1.984 | ACC=0.600 | N=40\n",
      "    id_compositional: CE=1.0099 | PPL=2.745 | ACC=0.623 | N=106\n",
      "near_ood_compositional: CE=0.8442 | PPL=2.326 | ACC=0.500 | N=14\n",
      "      ood_analogical: CE=5.7232 | PPL=305.885 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=2.0376\n",
      "          macro(ACC): ACC=0.345\n",
      "\n",
      "epoch 20 | train CE: 0.1092\n",
      "\n",
      "== epoch 20 validation ==\n",
      "       id_analogical: CE=1.8183 | PPL=6.161 | ACC=0.333 | N=3\n",
      "           id_atomic: CE=0.0545 | PPL=1.056 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0573 | PPL=1.059 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0635 | PPL=1.066 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=5.8528 | PPL=348.214 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=1.5693\n",
      "          macro(ACC): ACC=0.667\n",
      "\n",
      "epoch 30 | train CE: 0.0577\n",
      "\n",
      "== epoch 30 validation ==\n",
      "       id_analogical: CE=0.4434 | PPL=1.558 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0375 | PPL=1.038 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0389 | PPL=1.040 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0450 | PPL=1.046 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=5.3415 | PPL=208.835 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=1.1813\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 40 | train CE: 0.0571\n",
      "\n",
      "== epoch 40 validation ==\n",
      "       id_analogical: CE=0.3605 | PPL=1.434 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0453 | PPL=1.046 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0489 | PPL=1.050 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0577 | PPL=1.059 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=6.1177 | PPL=453.842 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=1.3260\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 50 | train CE: 0.0338\n",
      "\n",
      "== epoch 50 validation ==\n",
      "       id_analogical: CE=0.1377 | PPL=1.148 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0299 | PPL=1.030 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0289 | PPL=1.029 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0251 | PPL=1.025 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=6.1523 | PPL=469.775 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=1.2748\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 60 | train CE: 0.0200\n",
      "\n",
      "== epoch 60 validation ==\n",
      "       id_analogical: CE=0.0787 | PPL=1.082 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0140 | PPL=1.014 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0171 | PPL=1.017 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0223 | PPL=1.023 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.1235 | PPL=22.725 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.6511\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 70 | train CE: 0.0137\n",
      "\n",
      "== epoch 70 validation ==\n",
      "       id_analogical: CE=0.0492 | PPL=1.050 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0110 | PPL=1.011 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0136 | PPL=1.014 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0189 | PPL=1.019 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=2.2632 | PPL=9.614 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.4712\n",
      "          macro(ACC): ACC=0.800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 259, in _feed\n",
      "    reader_close()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 178, in close\n",
      "    self._close()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 377, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 80 | train CE: 0.0119\n",
      "\n",
      "== epoch 80 validation ==\n",
      "       id_analogical: CE=0.0376 | PPL=1.038 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0089 | PPL=1.009 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0114 | PPL=1.011 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0179 | PPL=1.018 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=1.6584 | PPL=5.251 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.3468\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 90 | train CE: 0.0105\n",
      "\n",
      "== epoch 90 validation ==\n",
      "       id_analogical: CE=0.0313 | PPL=1.032 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0073 | PPL=1.007 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0108 | PPL=1.011 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0137 | PPL=1.014 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=0.5481 | PPL=1.730 | ACC=1.000 | N=2\n",
      "           macro(CE): CE=0.1222\n",
      "          macro(ACC): ACC=1.000\n",
      "\n",
      "epoch 100 | train CE: 0.0100\n",
      "\n",
      "== epoch 100 validation ==\n",
      "       id_analogical: CE=0.0567 | PPL=1.058 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0086 | PPL=1.009 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0101 | PPL=1.010 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0145 | PPL=1.015 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=0.3526 | PPL=1.423 | ACC=1.000 | N=2\n",
      "           macro(CE): CE=0.0885\n",
      "          macro(ACC): ACC=1.000\n",
      "\n",
      "epoch 110 | train CE: 0.1224\n",
      "\n",
      "== epoch 110 validation ==\n",
      "       id_analogical: CE=0.0317 | PPL=1.032 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0090 | PPL=1.009 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0901 | PPL=1.094 | ACC=0.962 | N=106\n",
      "near_ood_compositional: CE=0.0124 | PPL=1.012 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=2.4186 | PPL=11.230 | ACC=0.500 | N=2\n",
      "           macro(CE): CE=0.5123\n",
      "          macro(ACC): ACC=0.892\n",
      "\n",
      "epoch 120 | train CE: 0.0032\n",
      "\n",
      "== epoch 120 validation ==\n",
      "       id_analogical: CE=0.0064 | PPL=1.006 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0029 | PPL=1.003 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0036 | PPL=1.004 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0056 | PPL=1.006 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=0.0582 | PPL=1.060 | ACC=1.000 | N=2\n",
      "           macro(CE): CE=0.0153\n",
      "          macro(ACC): ACC=1.000\n",
      "\n",
      "epoch 130 | train CE: 0.0065\n",
      "\n",
      "== epoch 130 validation ==\n",
      "       id_analogical: CE=0.0166 | PPL=1.017 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0053 | PPL=1.005 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0069 | PPL=1.007 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0087 | PPL=1.009 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=0.2323 | PPL=1.262 | ACC=1.000 | N=2\n",
      "           macro(CE): CE=0.0540\n",
      "          macro(ACC): ACC=1.000\n",
      "\n",
      "epoch 140 | train CE: 0.0083\n",
      "\n",
      "== epoch 140 validation ==\n",
      "       id_analogical: CE=0.0296 | PPL=1.030 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0063 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0083 | PPL=1.008 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0100 | PPL=1.010 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=0.3963 | PPL=1.486 | ACC=1.000 | N=2\n",
      "           macro(CE): CE=0.0901\n",
      "          macro(ACC): ACC=1.000\n",
      "\n",
      "epoch 150 | train CE: 0.0077\n",
      "\n",
      "== epoch 150 validation ==\n",
      "       id_analogical: CE=0.0293 | PPL=1.030 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0059 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0078 | PPL=1.008 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0088 | PPL=1.009 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=0.6913 | PPL=1.996 | ACC=0.500 | N=2\n",
      "           macro(CE): CE=0.1486\n",
      "          macro(ACC): ACC=0.900\n",
      "\n",
      "epoch 160 | train CE: 0.0078\n",
      "\n",
      "== epoch 160 validation ==\n",
      "       id_analogical: CE=0.0402 | PPL=1.041 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0058 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0075 | PPL=1.008 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0086 | PPL=1.009 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=0.8632 | PPL=2.371 | ACC=0.500 | N=2\n",
      "           macro(CE): CE=0.1851\n",
      "          macro(ACC): ACC=0.900\n",
      "\n",
      "epoch 170 | train CE: 0.0074\n",
      "\n",
      "== epoch 170 validation ==\n",
      "       id_analogical: CE=0.0405 | PPL=1.041 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0058 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0073 | PPL=1.007 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0085 | PPL=1.008 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=1.1925 | PPL=3.295 | ACC=0.500 | N=2\n",
      "           macro(CE): CE=0.2509\n",
      "          macro(ACC): ACC=0.900\n",
      "\n",
      "epoch 180 | train CE: 0.0072\n",
      "\n",
      "== epoch 180 validation ==\n",
      "       id_analogical: CE=0.0254 | PPL=1.026 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0059 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0073 | PPL=1.007 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0081 | PPL=1.008 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=1.3162 | PPL=3.729 | ACC=0.500 | N=2\n",
      "           macro(CE): CE=0.2726\n",
      "          macro(ACC): ACC=0.900\n",
      "\n",
      "epoch 190 | train CE: 0.0074\n",
      "\n",
      "== epoch 190 validation ==\n",
      "       id_analogical: CE=0.0390 | PPL=1.040 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0060 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0070 | PPL=1.007 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0084 | PPL=1.008 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=1.5854 | PPL=4.881 | ACC=0.500 | N=2\n",
      "           macro(CE): CE=0.3292\n",
      "          macro(ACC): ACC=0.900\n",
      "\n",
      "epoch 200 | train CE: 0.0070\n",
      "\n",
      "== epoch 200 validation ==\n",
      "       id_analogical: CE=0.0254 | PPL=1.026 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0060 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0068 | PPL=1.007 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0079 | PPL=1.008 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=2.0673 | PPL=7.904 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.4227\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 210 | train CE: 0.0069\n",
      "\n",
      "== epoch 210 validation ==\n",
      "       id_analogical: CE=0.0211 | PPL=1.021 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0062 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0068 | PPL=1.007 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0080 | PPL=1.008 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=2.8096 | PPL=16.603 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.5703\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 220 | train CE: 0.0069\n",
      "\n",
      "== epoch 220 validation ==\n",
      "       id_analogical: CE=0.0168 | PPL=1.017 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0062 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0068 | PPL=1.007 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0077 | PPL=1.008 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.0671 | PPL=21.479 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.6209\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 230 | train CE: 0.4152\n",
      "\n",
      "== epoch 230 validation ==\n",
      "       id_analogical: CE=0.1950 | PPL=1.215 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.3281 | PPL=1.388 | ACC=0.850 | N=40\n",
      "    id_compositional: CE=0.3790 | PPL=1.461 | ACC=0.849 | N=106\n",
      "near_ood_compositional: CE=0.4657 | PPL=1.593 | ACC=0.857 | N=14\n",
      "      ood_analogical: CE=3.9293 | PPL=50.873 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=1.0594\n",
      "          macro(ACC): ACC=0.711\n",
      "\n",
      "epoch 240 | train CE: 0.0033\n",
      "\n",
      "== epoch 240 validation ==\n",
      "       id_analogical: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0015 | PPL=1.002 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0036 | PPL=1.004 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0074 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=0.0926 | PPL=1.097 | ACC=1.000 | N=2\n",
      "           macro(CE): CE=0.0222\n",
      "          macro(ACC): ACC=1.000\n",
      "\n",
      "epoch 250 | train CE: 0.0031\n",
      "\n",
      "== epoch 250 validation ==\n",
      "       id_analogical: CE=0.0094 | PPL=1.009 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0018 | PPL=1.002 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0033 | PPL=1.003 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0056 | PPL=1.006 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=0.2404 | PPL=1.272 | ACC=1.000 | N=2\n",
      "           macro(CE): CE=0.0521\n",
      "          macro(ACC): ACC=1.000\n",
      "\n",
      "epoch 260 | train CE: 0.0028\n",
      "\n",
      "== epoch 260 validation ==\n",
      "       id_analogical: CE=0.0182 | PPL=1.018 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0019 | PPL=1.002 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0026 | PPL=1.003 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0040 | PPL=1.004 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=0.4222 | PPL=1.525 | ACC=1.000 | N=2\n",
      "           macro(CE): CE=0.0898\n",
      "          macro(ACC): ACC=1.000\n",
      "\n",
      "epoch 270 | train CE: 0.0029\n",
      "\n",
      "== epoch 270 validation ==\n",
      "       id_analogical: CE=0.0232 | PPL=1.024 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0021 | PPL=1.002 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0026 | PPL=1.003 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0043 | PPL=1.004 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=0.6979 | PPL=2.010 | ACC=1.000 | N=2\n",
      "           macro(CE): CE=0.1460\n",
      "          macro(ACC): ACC=1.000\n",
      "\n",
      "epoch 280 | train CE: 0.0043\n",
      "\n",
      "== epoch 280 validation ==\n",
      "       id_analogical: CE=0.0447 | PPL=1.046 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0032 | PPL=1.003 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0037 | PPL=1.004 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0066 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=1.0877 | PPL=2.967 | ACC=0.500 | N=2\n",
      "           macro(CE): CE=0.2292\n",
      "          macro(ACC): ACC=0.900\n",
      "\n",
      "epoch 290 | train CE: 0.0064\n",
      "\n",
      "== epoch 290 validation ==\n",
      "       id_analogical: CE=0.0546 | PPL=1.056 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0050 | PPL=1.005 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0057 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0099 | PPL=1.010 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=1.6073 | PPL=4.989 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.3365\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 300 | train CE: 0.0071\n",
      "\n",
      "== epoch 300 validation ==\n",
      "       id_analogical: CE=0.0457 | PPL=1.047 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0057 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0064 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0111 | PPL=1.011 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=2.0364 | PPL=7.663 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.4210\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 310 | train CE: 0.0072\n",
      "\n",
      "== epoch 310 validation ==\n",
      "       id_analogical: CE=0.0429 | PPL=1.044 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0059 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0068 | PPL=1.007 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0114 | PPL=1.011 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=2.2671 | PPL=9.652 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.4668\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 320 | train CE: 0.0074\n",
      "\n",
      "== epoch 320 validation ==\n",
      "       id_analogical: CE=0.0485 | PPL=1.050 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0060 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0067 | PPL=1.007 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0093 | PPL=1.009 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=2.4804 | PPL=11.947 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.5102\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 330 | train CE: 0.0076\n",
      "\n",
      "== epoch 330 validation ==\n",
      "       id_analogical: CE=0.0507 | PPL=1.052 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0062 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0069 | PPL=1.007 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0085 | PPL=1.009 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=2.6449 | PPL=14.082 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.5435\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 340 | train CE: 0.0074\n",
      "\n",
      "== epoch 340 validation ==\n",
      "       id_analogical: CE=0.0364 | PPL=1.037 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0069 | PPL=1.007 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0086 | PPL=1.009 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.0061 | PPL=20.208 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.6128\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 350 | train CE: 0.0074\n",
      "\n",
      "== epoch 350 validation ==\n",
      "       id_analogical: CE=0.0355 | PPL=1.036 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0062 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0069 | PPL=1.007 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0084 | PPL=1.008 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.2292 | PPL=25.260 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.6572\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 360 | train CE: 0.0071\n",
      "\n",
      "== epoch 360 validation ==\n",
      "       id_analogical: CE=0.0294 | PPL=1.030 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0062 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0068 | PPL=1.007 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0074 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.5010 | PPL=33.148 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.7101\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 370 | train CE: 0.0071\n",
      "\n",
      "== epoch 370 validation ==\n",
      "       id_analogical: CE=0.0301 | PPL=1.031 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0062 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0068 | PPL=1.007 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0078 | PPL=1.008 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=4.1474 | PPL=63.270 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.8397\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 380 | train CE: 0.0070\n",
      "\n",
      "== epoch 380 validation ==\n",
      "       id_analogical: CE=0.0179 | PPL=1.018 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0064 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0070 | PPL=1.007 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0081 | PPL=1.008 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.8033 | PPL=44.850 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.7685\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 390 | train CE: 0.0069\n",
      "\n",
      "== epoch 390 validation ==\n",
      "       id_analogical: CE=0.0190 | PPL=1.019 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0063 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0067 | PPL=1.007 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0076 | PPL=1.008 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=4.1796 | PPL=65.340 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.8438\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 400 | train CE: 0.0100\n",
      "\n",
      "== epoch 400 validation ==\n",
      "       id_analogical: CE=0.1072 | PPL=1.113 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0079 | PPL=1.008 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0088 | PPL=1.009 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0107 | PPL=1.011 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=4.1929 | PPL=66.213 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.8655\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 410 | train CE: 0.0058\n",
      "\n",
      "== epoch 410 validation ==\n",
      "       id_analogical: CE=0.0141 | PPL=1.014 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0052 | PPL=1.005 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0058 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0062 | PPL=1.006 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=4.6695 | PPL=106.644 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.9402\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 420 | train CE: 0.0069\n",
      "\n",
      "== epoch 420 validation ==\n",
      "       id_analogical: CE=0.0269 | PPL=1.027 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0065 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0069 | PPL=1.007 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0075 | PPL=1.008 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=5.5084 | PPL=246.754 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=1.1112\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 430 | train CE: 0.0301\n",
      "\n",
      "== epoch 430 validation ==\n",
      "       id_analogical: CE=0.0244 | PPL=1.025 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0091 | PPL=1.009 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0113 | PPL=1.011 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0155 | PPL=1.016 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=0.5643 | PPL=1.758 | ACC=0.500 | N=2\n",
      "           macro(CE): CE=0.1249\n",
      "          macro(ACC): ACC=0.900\n",
      "\n",
      "epoch 440 | train CE: 0.0016\n",
      "\n",
      "== epoch 440 validation ==\n",
      "       id_analogical: CE=0.0188 | PPL=1.019 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0006 | PPL=1.001 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0014 | PPL=1.001 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0024 | PPL=1.002 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=1.5092 | PPL=4.523 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.3065\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 450 | train CE: 0.0019\n",
      "\n",
      "== epoch 450 validation ==\n",
      "       id_analogical: CE=0.0172 | PPL=1.017 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0014 | PPL=1.001 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0018 | PPL=1.002 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0036 | PPL=1.004 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=1.8522 | PPL=6.374 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.3753\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 460 | train CE: 0.0042\n",
      "\n",
      "== epoch 460 validation ==\n",
      "       id_analogical: CE=0.0319 | PPL=1.032 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0035 | PPL=1.003 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0040 | PPL=1.004 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0062 | PPL=1.006 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=1.8106 | PPL=6.114 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.3712\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 470 | train CE: 0.0065\n",
      "\n",
      "== epoch 470 validation ==\n",
      "       id_analogical: CE=0.0416 | PPL=1.042 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0053 | PPL=1.005 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0059 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0079 | PPL=1.008 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=1.9593 | PPL=7.094 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.4040\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 480 | train CE: 0.0065\n",
      "\n",
      "== epoch 480 validation ==\n",
      "       id_analogical: CE=0.0358 | PPL=1.036 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0055 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0062 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0072 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=2.7501 | PPL=15.645 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.5610\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 490 | train CE: 0.0066\n",
      "\n",
      "== epoch 490 validation ==\n",
      "       id_analogical: CE=0.0292 | PPL=1.030 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0056 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0062 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0073 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=2.7561 | PPL=15.738 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.5609\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 500 | train CE: 0.0065\n",
      "\n",
      "== epoch 500 validation ==\n",
      "       id_analogical: CE=0.0257 | PPL=1.026 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0058 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0063 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0081 | PPL=1.008 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=2.6426 | PPL=14.050 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.5377\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 510 | train CE: 0.0065\n",
      "\n",
      "== epoch 510 validation ==\n",
      "       id_analogical: CE=0.0252 | PPL=1.026 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0057 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0069 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.6849 | PPL=39.843 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.7458\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 520 | train CE: 0.0063\n",
      "\n",
      "== epoch 520 validation ==\n",
      "       id_analogical: CE=0.0152 | PPL=1.015 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0059 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0068 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.4625 | PPL=31.897 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.6993\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 530 | train CE: 0.0063\n",
      "\n",
      "== epoch 530 validation ==\n",
      "       id_analogical: CE=0.0180 | PPL=1.018 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0059 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0068 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.5634 | PPL=35.282 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.7200\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 540 | train CE: 0.0063\n",
      "\n",
      "== epoch 540 validation ==\n",
      "       id_analogical: CE=0.0239 | PPL=1.024 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0059 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0060 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0066 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=4.4997 | PPL=89.993 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.9084\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 550 | train CE: 0.0286\n",
      "\n",
      "== epoch 550 validation ==\n",
      "       id_analogical: CE=0.8081 | PPL=2.244 | ACC=0.667 | N=3\n",
      "           id_atomic: CE=0.0252 | PPL=1.025 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0385 | PPL=1.039 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.1251 | PPL=1.133 | ACC=0.929 | N=14\n",
      "      ood_analogical: CE=7.5620 | PPL=1923.785 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=1.7118\n",
      "          macro(ACC): ACC=0.719\n",
      "\n",
      "epoch 560 | train CE: 0.0114\n",
      "\n",
      "== epoch 560 validation ==\n",
      "       id_analogical: CE=0.0672 | PPL=1.070 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0035 | PPL=1.004 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0058 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0060 | PPL=1.006 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=2.9829 | PPL=19.744 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.6131\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 570 | train CE: 0.0042\n",
      "\n",
      "== epoch 570 validation ==\n",
      "       id_analogical: CE=0.0492 | PPL=1.050 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0030 | PPL=1.003 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0038 | PPL=1.004 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0045 | PPL=1.005 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=4.1951 | PPL=66.359 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.8511\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 580 | train CE: 0.0073\n",
      "\n",
      "== epoch 580 validation ==\n",
      "       id_analogical: CE=0.0735 | PPL=1.076 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0052 | PPL=1.005 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0070 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.8947 | PPL=49.140 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.7973\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 590 | train CE: 0.0067\n",
      "\n",
      "== epoch 590 validation ==\n",
      "       id_analogical: CE=0.0509 | PPL=1.052 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0054 | PPL=1.005 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0060 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0067 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=4.1225 | PPL=61.716 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.8383\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 600 | train CE: 0.0070\n",
      "\n",
      "== epoch 600 validation ==\n",
      "       id_analogical: CE=0.0569 | PPL=1.059 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0056 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0067 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=4.1138 | PPL=61.180 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.8378\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 610 | train CE: 0.0067\n",
      "\n",
      "== epoch 610 validation ==\n",
      "       id_analogical: CE=0.0436 | PPL=1.045 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0056 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0065 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=4.3392 | PPL=76.647 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.8802\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 620 | train CE: 0.0067\n",
      "\n",
      "== epoch 620 validation ==\n",
      "       id_analogical: CE=0.0397 | PPL=1.041 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0056 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0060 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0066 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=4.3919 | PPL=80.792 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.8900\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 630 | train CE: 0.0065\n",
      "\n",
      "== epoch 630 validation ==\n",
      "       id_analogical: CE=0.0293 | PPL=1.030 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0057 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0068 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=4.6651 | PPL=106.175 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.9426\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 640 | train CE: 0.0063\n",
      "\n",
      "== epoch 640 validation ==\n",
      "       id_analogical: CE=0.0207 | PPL=1.021 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0057 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0067 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=4.9289 | PPL=138.228 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.9936\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 650 | train CE: 0.0064\n",
      "\n",
      "== epoch 650 validation ==\n",
      "       id_analogical: CE=0.0231 | PPL=1.023 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0058 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0066 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=5.4121 | PPL=224.113 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=1.0907\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 660 | train CE: 0.0063\n",
      "\n",
      "== epoch 660 validation ==\n",
      "       id_analogical: CE=0.0187 | PPL=1.019 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0058 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0067 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=5.4483 | PPL=232.366 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=1.0971\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 670 | train CE: 0.0064\n",
      "\n",
      "== epoch 670 validation ==\n",
      "       id_analogical: CE=0.0178 | PPL=1.018 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0059 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0068 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=5.7864 | PPL=325.849 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=1.1646\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 680 | train CE: 0.0064\n",
      "\n",
      "== epoch 680 validation ==\n",
      "       id_analogical: CE=0.0283 | PPL=1.029 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0060 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0065 | PPL=1.006 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=7.0439 | PPL=1145.866 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=1.4182\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 690 | train CE: 0.0162\n",
      "\n",
      "== epoch 690 validation ==\n",
      "       id_analogical: CE=3.6305 | PPL=37.732 | ACC=0.333 | N=3\n",
      "           id_atomic: CE=0.0370 | PPL=1.038 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0580 | PPL=1.060 | ACC=0.981 | N=106\n",
      "near_ood_compositional: CE=0.0595 | PPL=1.061 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.9140 | PPL=50.100 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=1.5398\n",
      "          macro(ACC): ACC=0.663\n",
      "\n",
      "epoch 700 | train CE: 0.0031\n",
      "\n",
      "== epoch 700 validation ==\n",
      "       id_analogical: CE=0.0025 | PPL=1.002 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0039 | PPL=1.004 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0025 | PPL=1.002 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0571 | PPL=1.059 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.3735 | PPL=29.180 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.6879\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 710 | train CE: 0.0013\n",
      "\n",
      "== epoch 710 validation ==\n",
      "       id_analogical: CE=0.0054 | PPL=1.005 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0006 | PPL=1.001 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0013 | PPL=1.001 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0078 | PPL=1.008 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=2.8759 | PPL=17.742 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.5782\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 720 | train CE: 0.0019\n",
      "\n",
      "== epoch 720 validation ==\n",
      "       id_analogical: CE=0.0123 | PPL=1.012 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0012 | PPL=1.001 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0022 | PPL=1.002 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0081 | PPL=1.008 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.2321 | PPL=25.333 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.6512\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 730 | train CE: 0.0023\n",
      "\n",
      "== epoch 730 validation ==\n",
      "       id_analogical: CE=0.0139 | PPL=1.014 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0018 | PPL=1.002 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0021 | PPL=1.002 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0056 | PPL=1.006 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.0391 | PPL=20.886 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.6125\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 740 | train CE: 0.0025\n",
      "\n",
      "== epoch 740 validation ==\n",
      "       id_analogical: CE=0.0133 | PPL=1.013 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0026 | PPL=1.003 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0022 | PPL=1.002 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0046 | PPL=1.005 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.1136 | PPL=22.501 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.6273\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 750 | train CE: 0.0027\n",
      "\n",
      "== epoch 750 validation ==\n",
      "       id_analogical: CE=0.0160 | PPL=1.016 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0030 | PPL=1.003 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0023 | PPL=1.002 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0044 | PPL=1.004 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.3374 | PPL=28.145 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.6726\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 760 | train CE: 0.0032\n",
      "\n",
      "== epoch 760 validation ==\n",
      "       id_analogical: CE=0.0213 | PPL=1.022 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0036 | PPL=1.004 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0026 | PPL=1.003 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0050 | PPL=1.005 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.5440 | PPL=34.606 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.7153\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 770 | train CE: 0.0042\n",
      "\n",
      "== epoch 770 validation ==\n",
      "       id_analogical: CE=0.0319 | PPL=1.032 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0042 | PPL=1.004 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0035 | PPL=1.004 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0063 | PPL=1.006 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.4213 | PPL=30.610 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.6934\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 780 | train CE: 0.0054\n",
      "\n",
      "== epoch 780 validation ==\n",
      "       id_analogical: CE=0.0345 | PPL=1.035 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0051 | PPL=1.005 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0047 | PPL=1.005 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0076 | PPL=1.008 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.3216 | PPL=27.705 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.6747\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 790 | train CE: 0.0062\n",
      "\n",
      "== epoch 790 validation ==\n",
      "       id_analogical: CE=0.0447 | PPL=1.046 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0057 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0054 | PPL=1.005 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0077 | PPL=1.008 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.2305 | PPL=25.293 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.6588\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 800 | train CE: 0.0068\n",
      "\n",
      "== epoch 800 validation ==\n",
      "       id_analogical: CE=0.0482 | PPL=1.049 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0059 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0076 | PPL=1.008 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.4429 | PPL=31.276 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.7021\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 810 | train CE: 0.0069\n",
      "\n",
      "== epoch 810 validation ==\n",
      "       id_analogical: CE=0.0472 | PPL=1.048 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0062 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0060 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0071 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.4955 | PPL=32.965 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.7124\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 820 | train CE: 0.0069\n",
      "\n",
      "== epoch 820 validation ==\n",
      "       id_analogical: CE=0.0430 | PPL=1.044 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0062 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0076 | PPL=1.008 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.3299 | PPL=27.936 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.6786\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 830 | train CE: 0.0070\n",
      "\n",
      "== epoch 830 validation ==\n",
      "       id_analogical: CE=0.0450 | PPL=1.046 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0062 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0062 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0073 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.2682 | PPL=26.264 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.6666\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 840 | train CE: 0.0068\n",
      "\n",
      "== epoch 840 validation ==\n",
      "       id_analogical: CE=0.0371 | PPL=1.038 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0062 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0070 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.4982 | PPL=33.057 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.7109\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 850 | train CE: 0.0069\n",
      "\n",
      "== epoch 850 validation ==\n",
      "       id_analogical: CE=0.0365 | PPL=1.037 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0062 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0062 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0072 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.4992 | PPL=33.087 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.7111\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 860 | train CE: 0.0069\n",
      "\n",
      "== epoch 860 validation ==\n",
      "       id_analogical: CE=0.0383 | PPL=1.039 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0068 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.9765 | PPL=53.331 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.8068\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 870 | train CE: 0.0067\n",
      "\n",
      "== epoch 870 validation ==\n",
      "       id_analogical: CE=0.0279 | PPL=1.028 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0062 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0068 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.8149 | PPL=45.372 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.7724\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 880 | train CE: 0.0065\n",
      "\n",
      "== epoch 880 validation ==\n",
      "       id_analogical: CE=0.0251 | PPL=1.025 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0068 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.9225 | PPL=50.524 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.7933\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 890 | train CE: 0.0065\n",
      "\n",
      "== epoch 890 validation ==\n",
      "       id_analogical: CE=0.0223 | PPL=1.023 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0062 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0062 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0067 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=4.1666 | PPL=64.494 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.8416\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 900 | train CE: 0.0064\n",
      "\n",
      "== epoch 900 validation ==\n",
      "       id_analogical: CE=0.0224 | PPL=1.023 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0065 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=4.2617 | PPL=70.928 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.8606\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 910 | train CE: 0.0064\n",
      "\n",
      "== epoch 910 validation ==\n",
      "       id_analogical: CE=0.0211 | PPL=1.021 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0066 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=4.2360 | PPL=69.133 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.8552\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 920 | train CE: 0.0064\n",
      "\n",
      "== epoch 920 validation ==\n",
      "       id_analogical: CE=0.0204 | PPL=1.021 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0060 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0061 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0063 | PPL=1.006 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=4.5715 | PPL=96.685 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.9221\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 930 | train CE: 0.0063\n",
      "\n",
      "== epoch 930 validation ==\n",
      "       id_analogical: CE=0.0216 | PPL=1.022 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0060 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0060 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0065 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=4.4462 | PPL=85.298 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.8973\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 940 | train CE: 0.0064\n",
      "\n",
      "== epoch 940 validation ==\n",
      "       id_analogical: CE=0.0214 | PPL=1.022 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0060 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0060 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0065 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=4.7835 | PPL=119.518 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.9647\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 950 | train CE: 0.0065\n",
      "\n",
      "== epoch 950 validation ==\n",
      "       id_analogical: CE=0.0267 | PPL=1.027 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0060 | PPL=1.006 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0060 | PPL=1.006 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0066 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=4.9710 | PPL=144.165 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=1.0033\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 960 | train CE: 0.0100\n",
      "\n",
      "== epoch 960 validation ==\n",
      "       id_analogical: CE=0.0189 | PPL=1.019 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0088 | PPL=1.009 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0072 | PPL=1.007 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0026 | PPL=1.003 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=3.8350 | PPL=46.294 | ACC=0.500 | N=2\n",
      "           macro(CE): CE=0.7745\n",
      "          macro(ACC): ACC=0.900\n",
      "\n",
      "epoch 970 | train CE: 0.0015\n",
      "\n",
      "== epoch 970 validation ==\n",
      "       id_analogical: CE=0.0098 | PPL=1.010 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0011 | PPL=1.001 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0017 | PPL=1.002 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0026 | PPL=1.003 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=2.3446 | PPL=10.429 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.4719\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 980 | train CE: 0.0030\n",
      "\n",
      "== epoch 980 validation ==\n",
      "       id_analogical: CE=0.0261 | PPL=1.026 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0025 | PPL=1.002 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0026 | PPL=1.003 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0025 | PPL=1.003 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=2.0764 | PPL=7.976 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.4220\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 990 | train CE: 0.0033\n",
      "\n",
      "== epoch 990 validation ==\n",
      "       id_analogical: CE=0.0262 | PPL=1.027 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0030 | PPL=1.003 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0027 | PPL=1.003 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0041 | PPL=1.004 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=1.9503 | PPL=7.031 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.3973\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "epoch 1000 | train CE: 0.0035\n",
      "\n",
      "== epoch 1000 validation ==\n",
      "       id_analogical: CE=0.0338 | PPL=1.034 | ACC=1.000 | N=3\n",
      "           id_atomic: CE=0.0033 | PPL=1.003 | ACC=1.000 | N=40\n",
      "    id_compositional: CE=0.0028 | PPL=1.003 | ACC=1.000 | N=106\n",
      "near_ood_compositional: CE=0.0067 | PPL=1.007 | ACC=1.000 | N=14\n",
      "      ood_analogical: CE=2.1680 | PPL=8.741 | ACC=0.000 | N=2\n",
      "           macro(CE): CE=0.4429\n",
      "          macro(ACC): ACC=0.800\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# (optional) W&B init\n",
    "# ----------------------------\n",
    "wandb = None\n",
    "if use_wandb:\n",
    "    try:\n",
    "        import wandb as _wandb\n",
    "        wandb = _wandb\n",
    "        mode = \"online\" if os.environ.get(\"WANDB_API_KEY\") else \"disabled\"\n",
    "        wandb.init(\n",
    "            project=project,\n",
    "            name=run_name,\n",
    "            mode=mode,\n",
    "            config=dict(\n",
    "                data_dir=data_dir, max_len=max_len, batch_size=batch_size, epochs=epochs,\n",
    "                d_model=d_model, n_layer=n_layer, n_head=n_head, dropout=dropout,\n",
    "                lr=lr, weight_decay=weight_decay, warmup_steps=warmup_steps, use_amp=use_amp\n",
    "            ),\n",
    "        )\n",
    "        wandb.define_metric(\"global_step\")\n",
    "        wandb.define_metric(\"train/*\", step_metric=\"global_step\")\n",
    "        wandb.define_metric(\"lr\", step_metric=\"global_step\")\n",
    "    except Exception as e:\n",
    "        print(f\"[W&B] disabled: {e}\")\n",
    "        wandb = None\n",
    "\n",
    "# ----------------------------\n",
    "# Model setup\n",
    "# ----------------------------\n",
    "model = GPT2LikeEncoder(len(train_ds.vocab), d_model=d_model, n_layer=n_layer, n_head=n_head, dropout=dropout, max_len=max_len).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "sched = WarmupThenConstant(opt, warmup_steps=warmup_steps)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "# ----------------------------\n",
    "# Training helpers\n",
    "# ----------------------------\n",
    "def step_loss(model, batch):\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    target_ids = batch[\"target_ids\"].to(device)\n",
    "    loss_mask  = batch[\"loss_mask\"].to(device)\n",
    "    pad_mask   = batch.get(\"pad_mask\", None)\n",
    "    if pad_mask is not None:\n",
    "        pad_mask = pad_mask.to(device)\n",
    "\n",
    "    with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "        logits = model(input_ids, pad_mask=pad_mask)\n",
    "        B,L,V = logits.shape\n",
    "        logits_f, targets_f, mask_f = logits.view(B*L,V), target_ids.view(B*L), loss_mask.view(B*L)\n",
    "        sel_logits, sel_targets = logits_f[mask_f], targets_f[mask_f]\n",
    "        loss = F.cross_entropy(sel_logits, sel_targets)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = sel_logits.argmax(dim=-1)\n",
    "        acc  = (pred == sel_targets).float().mean().item()\n",
    "    return loss, acc, sel_logits.detach(), sel_targets.detach()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_loader(loader):\n",
    "    if loader is None: return None\n",
    "    model.eval()\n",
    "    sum_loss, sum_acc, n = 0.0, 0.0, 0\n",
    "    for batch in loader:\n",
    "        loss, acc, _, _ = step_loss(model, batch)\n",
    "        bs = batch[\"input_ids\"].size(0)\n",
    "        sum_loss += loss.item() * bs\n",
    "        sum_acc  += acc * bs\n",
    "        n += bs\n",
    "    mean_ce = sum_loss / max(1, n)\n",
    "    return {\"CE\": mean_ce, \"PPL\": math.exp(min(20.0, mean_ce)), \"ACC\": sum_acc/max(1,n), \"N\": n}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_split_by_type(model, loader):\n",
    "    model.eval()\n",
    "    sums = {}\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        target_ids = batch[\"target_ids\"].to(device)\n",
    "        loss_mask  = batch[\"loss_mask\"].to(device)\n",
    "        pad_mask   = batch[\"pad_mask\"].to(device)\n",
    "        types      = batch[\"type\"]\n",
    "\n",
    "        logits = model(input_ids, pad_mask=pad_mask)\n",
    "        B,L,V = logits.shape\n",
    "        logits_f, targets_f, mask_f = logits.view(B*L,V), target_ids.view(B*L), loss_mask.view(B*L)\n",
    "        sel_logits, sel_targets = logits_f[mask_f], targets_f[mask_f]\n",
    "        per_ce  = F.cross_entropy(sel_logits, sel_targets, reduction='none')\n",
    "        per_acc = (sel_logits.argmax(dim=-1) == sel_targets).float()\n",
    "        for t, ce, acc in zip(types, per_ce.tolist(), per_acc.tolist()):\n",
    "            d = sums.setdefault(t, {\"sum_ce\":0,\"sum_acc\":0,\"n\":0})\n",
    "            d[\"sum_ce\"]+=ce; d[\"sum_acc\"]+=acc; d[\"n\"]+=1\n",
    "    metrics={}\n",
    "    for t,d in sums.items():\n",
    "        mce=d[\"sum_ce\"]/max(1,d[\"n\"])\n",
    "        metrics[t]={\"CE\":mce,\"PPL\":math.exp(min(20.0,mce)),\"ACC\":d[\"sum_acc\"]/max(1,d[\"n\"]),\"N\":d[\"n\"]}\n",
    "    return metrics\n",
    "\n",
    "def flatten_metrics(prefix, mdict):\n",
    "    flat={}\n",
    "    for t,m in mdict.items():\n",
    "        # t is the type of data\n",
    "        for k,v in m.items():\n",
    "            # k is like acc, ppl,...\n",
    "            flat[f\"{prefix}_{k}/{t}\"]=v\n",
    "    return flat\n",
    "\n",
    "# ----------------------------\n",
    "# Training loop\n",
    "# ----------------------------\n",
    "best_val=float(\"inf\"); global_step=0; log_every=50\n",
    "if wandb: wandb.config.update({\"num_params\":sum(p.numel() for p in model.parameters())})\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train(); running,n_ex=0,0\n",
    "    for i,batch in enumerate(train_dl,1):\n",
    "        loss,acc,_,_=step_loss(model, batch)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward(); scaler.step(opt); scaler.update(); sched.step()\n",
    "        bs=batch[\"input_ids\"].size(0)\n",
    "        running+=loss.item()*bs; n_ex+=bs; global_step+=1\n",
    "        if wandb and (i%log_every==0 or i==1):\n",
    "            wandb.log({\"global_step\":global_step,\"train/CE_last\":loss.item(),\"train/ACC_last\":acc,\"lr\":opt.param_groups[0][\"lr\"]},step=global_step)\n",
    "\n",
    "    train_ce=running/max(1,n_ex)\n",
    "    if epoch % 10==0:\n",
    "      print(f\"epoch {epoch} | train CE: {train_ce:.4f}\")\n",
    "\n",
    "    metrics=evaluate_split_by_type(model, test_dl)\n",
    "    ce_macro=sum(m[\"CE\"] for m in metrics.values())/max(1,len(metrics))\n",
    "    acc_macro=sum(m[\"ACC\"] for m in metrics.values())/max(1,len(metrics))\n",
    "    if epoch % 10==0:\n",
    "      print(f\"\\n== epoch {epoch} validation ==\")\n",
    "      for k,v in sorted(metrics.items()):\n",
    "          print(f\"{k:>20}: CE={v['CE']:.4f} | PPL={v['PPL']:.3f} | ACC={v['ACC']:.3f} | N={v['N']}\")\n",
    "      print(f\"{'macro(CE)':>20}: CE={ce_macro:.4f}\")\n",
    "      print(f\"{'macro(ACC)':>20}: ACC={acc_macro:.3f}\\n\")\n",
    "\n",
    "    if wandb:\n",
    "        # table=wandb.Table(columns=[\"type\",\"CE\",\"PPL\",\"ACC\",\"N\"])\n",
    "        # for t,m in sorted(metrics.items()): table.add_data(t,m[\"CE\"],m[\"PPL\"],m[\"ACC\"],m[\"N\"])\n",
    "        log_payload={\"global_step\":global_step,\"epoch\":epoch,\"train/CE_epoch\":train_ce,\"val/macro/CE\":ce_macro,\"val/macro/ACC\":acc_macro}\n",
    "        log_payload.update(flatten_metrics(\"val\",metrics))\n",
    "        wandb.log(log_payload,step=global_step)\n",
    "\n",
    "    ckpt={\"model\":model.state_dict(),\n",
    "          \"config\":{\"d_model\":d_model,\"n_layer\":n_layer,\"n_head\":n_head,\"dropout\":dropout,\n",
    "                    \"max_len\":max_len,\"lr\":lr,\"weight_decay\":weight_decay,\"warmup_steps\":warmup_steps},\n",
    "          \"vocab\":train_ds.vocab}\n",
    "    torch.save(ckpt,os.path.join(save_dir,f\"epoch{epoch:03d}.pt\"))\n",
    "    if ce_macro<best_val:\n",
    "        best_val=ce_macro\n",
    "        best_path=os.path.join(save_dir,\"best.pt\")\n",
    "        torch.save(ckpt,best_path)\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 764
    },
    "id": "d5YdQUnAeuC8",
    "outputId": "9c323ec1-7adf-45ff-acb9-dd5b8733433b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇██████</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/ACC_last</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>train/CE_epoch</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/CE_last</td><td>▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/macro/ACC</td><td>▁▂▃▃▃▃████▅▅██▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>val/macro/CE</td><td>▇▅▄▅▂▂▁▁▁▁█▁▁▁▁▂▂▂▂▃▃▃▃▆▃▃▃▃▄▄▃▃▃▃▃▃▃▃▃▂</td></tr><tr><td>val_ACC/id_analogical</td><td>▁▃▁▃█▃████████████████████▆█████████████</td></tr><tr><td>val_ACC/id_atomic</td><td>█████████████████████████▁██████████████</td></tr><tr><td>+18</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1000</td></tr><tr><td>global_step</td><td>3000</td></tr><tr><td>lr</td><td>0.01</td></tr><tr><td>train/ACC_last</td><td>1</td></tr><tr><td>train/CE_epoch</td><td>0.00354</td></tr><tr><td>train/CE_last</td><td>0.00429</td></tr><tr><td>val/macro/ACC</td><td>0.8</td></tr><tr><td>val/macro/CE</td><td>0.44291</td></tr><tr><td>val_ACC/id_analogical</td><td>1</td></tr><tr><td>val_ACC/id_atomic</td><td>1</td></tr><tr><td>+18</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gpt2like_functor-composition_functor.10.10000.5_1768479094</strong> at: <a href='https://wandb.ai/gouki/emergent_analogy_functor_20260115/runs/h5wwjcw8' target=\"_blank\">https://wandb.ai/gouki/emergent_analogy_functor_20260115/runs/h5wwjcw8</a><br> View project at: <a href='https://wandb.ai/gouki/emergent_analogy_functor_20260115' target=\"_blank\">https://wandb.ai/gouki/emergent_analogy_functor_20260115</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260115_121136-h5wwjcw8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00218674721c4f2796b8a00964c5786c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e41a0d760264eec9ad28568d70905eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1fe50f0d192c41a78cae6235b1481028": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24d18c2452aa4fb5bd08d8f15209e559": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "31ec093ff2f84de7aa996349962a941b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6a175f3c87954291855e768a759d5488",
       "IPY_MODEL_41edb705cfee4fc7b504dbe765505311",
       "IPY_MODEL_7518919438f74527afc6e8b9ed5504f8"
      ],
      "layout": "IPY_MODEL_00218674721c4f2796b8a00964c5786c"
     }
    },
    "323afa4c7a6a4aeeaffe8384ece47370": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "41edb705cfee4fc7b504dbe765505311": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5afe77f841ca44afaefdac67da0d9f3a",
      "max": 2000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_323afa4c7a6a4aeeaffe8384ece47370",
      "value": 2000
     }
    },
    "487544ee16c6427a81be73f50690f9a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4c8710a736864be8821318d1b844a5e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56a7c956b3c8436a83f0119df5dfdd0e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5afe77f841ca44afaefdac67da0d9f3a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c4b337d19d348d2998be7c68ece723e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "605cf6073e304b36bb0c4ea1df747fd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6095ae7d99ef42a3985b28fe57edac77",
      "max": 2000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_24d18c2452aa4fb5bd08d8f15209e559",
      "value": 2000
     }
    },
    "6095ae7d99ef42a3985b28fe57edac77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64385b72913d4cefb67ced4296b6a948": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6a175f3c87954291855e768a759d5488": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed6f9e5104ea46b7b5a566d3cc830d29",
      "placeholder": "​",
      "style": "IPY_MODEL_fbd33915d45f4eb38f07462c5c17eb18",
      "value": "100%"
     }
    },
    "6d42fdb9ffd2406484e39223b9cc6a7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7c48de5f28cf41b383a57213158122a2",
       "IPY_MODEL_605cf6073e304b36bb0c4ea1df747fd3",
       "IPY_MODEL_84271db35276416097f9f7ed70b0d758"
      ],
      "layout": "IPY_MODEL_56a7c956b3c8436a83f0119df5dfdd0e"
     }
    },
    "7518919438f74527afc6e8b9ed5504f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2825c6af3484a8fbdc5abdc24ca4939",
      "placeholder": "​",
      "style": "IPY_MODEL_1e41a0d760264eec9ad28568d70905eb",
      "value": " 2000/2000 [00:00&lt;00:00, 9005.74it/s]"
     }
    },
    "7c48de5f28cf41b383a57213158122a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c4b337d19d348d2998be7c68ece723e",
      "placeholder": "​",
      "style": "IPY_MODEL_64385b72913d4cefb67ced4296b6a948",
      "value": "100%"
     }
    },
    "84271db35276416097f9f7ed70b0d758": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b08aa8a305dd4c5d9536302945b8f0a2",
      "placeholder": "​",
      "style": "IPY_MODEL_f59782f065d442158d1925495ae32b0d",
      "value": " 2000/2000 [00:02&lt;00:00, 749.37it/s]"
     }
    },
    "ab234546ea2a46a0b352d11c353d8fd7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b08aa8a305dd4c5d9536302945b8f0a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2825c6af3484a8fbdc5abdc24ca4939": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c471edc43a9d42b2af43792c67a56bb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f8cce61e763e4c74944ce19748e1ff15",
       "IPY_MODEL_f203832014364752ae089f58c2711249",
       "IPY_MODEL_dfca1c2552e447619274a3b4f02882d4"
      ],
      "layout": "IPY_MODEL_1fe50f0d192c41a78cae6235b1481028"
     }
    },
    "c53dfaab98334c0a9f2022ca2fd3bf16": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0b286ab73274c57ba2096a31e778d49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dfca1c2552e447619274a3b4f02882d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0b286ab73274c57ba2096a31e778d49",
      "placeholder": "​",
      "style": "IPY_MODEL_ab234546ea2a46a0b352d11c353d8fd7",
      "value": " 40000/40000 [00:00&lt;00:00, 48664.91it/s]"
     }
    },
    "ed6f9e5104ea46b7b5a566d3cc830d29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f203832014364752ae089f58c2711249": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c53dfaab98334c0a9f2022ca2fd3bf16",
      "max": 40000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_487544ee16c6427a81be73f50690f9a1",
      "value": 40000
     }
    },
    "f59782f065d442158d1925495ae32b0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f6c603ac98e240adbb92a37dff9cb38c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f8cce61e763e4c74944ce19748e1ff15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c8710a736864be8821318d1b844a5e0",
      "placeholder": "​",
      "style": "IPY_MODEL_f6c603ac98e240adbb92a37dff9cb38c",
      "value": "100%"
     }
    },
    "fbd33915d45f4eb38f07462c5c17eb18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}